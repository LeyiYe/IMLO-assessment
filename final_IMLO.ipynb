{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeyiYe/IMLO-assessment/blob/main/final_IMLO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import Flowers102\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define transforms for data augmentation and normalization\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the dataset with train/val/test splits\n",
        "train_dataset = Flowers102(root='./data', split='train', transform=train_transform, download=True)\n",
        "val_dataset = Flowers102(root='./data', split='val', transform=val_transform, download=True)\n",
        "\n",
        "test_dataset = Flowers102(root='./data', split='test', transform=val_transform, download=True)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "\n",
        "class CNN_NN(nn.Module):\n",
        "    def __init__(self, num_classes=102):\n",
        "        super(CNN_NN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv5= nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv6= nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "        #self.conv7= nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        #self.conv8= nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation =1, ceil_mode=False)\n",
        "\n",
        "        # Initialize the size of the fully connected layer based on the sample input\n",
        "        self._to_linear = None\n",
        "        self.convs = nn.Sequential(\n",
        "            self.conv1,\n",
        "            nn.ReLU(inplace=True),\n",
        "            #self.pool,\n",
        "            self.conv2,\n",
        "            nn.ReLU(inplace=True),\n",
        "            self.pool,\n",
        "            self.conv3,\n",
        "            nn.ReLU(inplace=True),\n",
        "            #self.pool,\n",
        "            self.conv4,\n",
        "            nn.ReLU(inplace=True),\n",
        "            self.pool,\n",
        "            self.conv5,\n",
        "            nn.ReLU(inplace=True),\n",
        "            #self.pool\n",
        "            #self.conv6,\n",
        "            #nn.ReLU(inplace=True),\n",
        "            self.conv6,\n",
        "            nn.ReLU(inplace=True),\n",
        "            self.pool,\n",
        "           # self.conv7,\n",
        "            #nn.ReLU(inplace=True),\n",
        "            #self.conv8,\n",
        "            #nn.ReLU(inplace=True),\n",
        "            #self.conv8,\n",
        "           # nn.ReLU(inplace=True),\n",
        "           # self.pool,\n",
        "            #self.conv8,\n",
        "           # nn.ReLU(inplace=True),\n",
        "           # self.conv8,\n",
        "           # nn.ReLU(inplace=True),\n",
        "           # self.conv8,\n",
        "           # nn.ReLU(inplace=True),\n",
        "           # self.pool,\n",
        "\n",
        "        )\n",
        "        self._initialize_linear_layer()\n",
        "\n",
        "        self.fc1 = nn.Linear(self._to_linear, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def _initialize_linear_layer(self):\n",
        "        # Create a dummy input tensor to calculate the output size after conv layers\n",
        "        x = torch.randn(1, 3, 224, 224)\n",
        "        x = self.convs(x)\n",
        "        self._to_linear = x.view(1, -1).shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convs(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor while preserving the batch size\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the device (GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "num_classes = 102  # Number of output classes\n",
        "learning_rate = 0.001\n",
        "num_epochs = 1200\n",
        "\n",
        "# Initialize the model\n",
        "model = CNN_NN(num_classes=num_classes).to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate average validation loss and accuracy\n",
        "    val_loss = val_loss / len(val_loader.dataset)\n",
        "    accuracy = correct / total\n",
        "\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "\n",
        "    with torch.no_grad():  # No need to compute gradients for evaluation\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item() * images.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate average test loss and accuracy\n",
        "    test_loss = test_loss / len(test_loader.dataset)\n",
        "    test_accuracy = test_correct / test_total\n",
        "\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "          f'Training Loss: {epoch_loss:.4f}, '\n",
        "          f'Validation Loss: {val_loss:.4f}, '\n",
        "          f'Validation Percentage:{accuracy*100:.4f}%,'\n",
        "          f'Test Loss: {test_loss:.4f}, '\n",
        "          f'Test Percentage:{test_accuracy*100:.4f}%'\n",
        "          )\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlydLSoXvHMa",
        "outputId": "abab71a2-af97-412d-8e58-8b58c952dd6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1200], Training Loss: 4.6455, Validation Loss: 4.6203, Validation Percentage:0.9804%,Test Loss: 4.6209, Test Percentage:0.7806%\n",
            "Epoch [2/1200], Training Loss: 4.6098, Validation Loss: 4.4841, Validation Percentage:1.5686%,Test Loss: 4.4700, Test Percentage:2.1955%\n",
            "Epoch [3/1200], Training Loss: 4.5115, Validation Loss: 4.4359, Validation Percentage:1.5686%,Test Loss: 4.4407, Test Percentage:2.0003%\n",
            "Epoch [4/1200], Training Loss: 4.4665, Validation Loss: 4.4457, Validation Percentage:1.6667%,Test Loss: 4.4664, Test Percentage:2.6508%\n",
            "Epoch [5/1200], Training Loss: 4.4443, Validation Loss: 4.3806, Validation Percentage:2.3529%,Test Loss: 4.4529, Test Percentage:1.4474%\n",
            "Epoch [6/1200], Training Loss: 4.3657, Validation Loss: 4.2995, Validation Percentage:2.8431%,Test Loss: 4.3489, Test Percentage:1.8052%\n",
            "Epoch [7/1200], Training Loss: 4.3145, Validation Loss: 4.2433, Validation Percentage:3.7255%,Test Loss: 4.3210, Test Percentage:2.2443%\n",
            "Epoch [8/1200], Training Loss: 4.2678, Validation Loss: 4.1885, Validation Percentage:3.6275%,Test Loss: 4.2619, Test Percentage:2.4394%\n",
            "Epoch [9/1200], Training Loss: 4.2396, Validation Loss: 4.2244, Validation Percentage:4.0196%,Test Loss: 4.3030, Test Percentage:2.6671%\n",
            "Epoch [10/1200], Training Loss: 4.2416, Validation Loss: 4.1882, Validation Percentage:4.3137%,Test Loss: 4.2728, Test Percentage:2.7484%\n",
            "Epoch [11/1200], Training Loss: 4.1868, Validation Loss: 4.2626, Validation Percentage:4.1176%,Test Loss: 4.3050, Test Percentage:3.8380%\n",
            "Epoch [12/1200], Training Loss: 4.1797, Validation Loss: 4.2002, Validation Percentage:4.4118%,Test Loss: 4.2823, Test Percentage:3.7242%\n",
            "Epoch [13/1200], Training Loss: 4.1457, Validation Loss: 4.1782, Validation Percentage:4.5098%,Test Loss: 4.2160, Test Percentage:3.8218%\n",
            "Epoch [14/1200], Training Loss: 4.1508, Validation Loss: 4.1228, Validation Percentage:6.8627%,Test Loss: 4.1752, Test Percentage:4.4235%\n",
            "Epoch [15/1200], Training Loss: 4.1062, Validation Loss: 4.0769, Validation Percentage:5.6863%,Test Loss: 4.1667, Test Percentage:4.8301%\n",
            "Epoch [16/1200], Training Loss: 4.0593, Validation Loss: 4.0511, Validation Percentage:6.2745%,Test Loss: 4.1171, Test Percentage:4.7813%\n",
            "Epoch [17/1200], Training Loss: 4.0782, Validation Loss: 4.0842, Validation Percentage:5.8824%,Test Loss: 4.1277, Test Percentage:4.5698%\n",
            "Epoch [18/1200], Training Loss: 4.0237, Validation Loss: 4.0571, Validation Percentage:5.7843%,Test Loss: 4.1272, Test Percentage:4.7650%\n",
            "Epoch [19/1200], Training Loss: 4.0159, Validation Loss: 4.0440, Validation Percentage:6.7647%,Test Loss: 4.1182, Test Percentage:5.9522%\n",
            "Epoch [20/1200], Training Loss: 3.9872, Validation Loss: 4.0325, Validation Percentage:7.1569%,Test Loss: 4.1068, Test Percentage:5.4480%\n",
            "Epoch [21/1200], Training Loss: 3.9737, Validation Loss: 3.9708, Validation Percentage:7.8431%,Test Loss: 4.0802, Test Percentage:5.7245%\n",
            "Epoch [22/1200], Training Loss: 4.0053, Validation Loss: 3.9836, Validation Percentage:7.2549%,Test Loss: 4.0471, Test Percentage:5.4643%\n",
            "Epoch [23/1200], Training Loss: 3.9730, Validation Loss: 3.9859, Validation Percentage:5.3922%,Test Loss: 4.0928, Test Percentage:5.3830%\n",
            "Epoch [24/1200], Training Loss: 3.9309, Validation Loss: 3.9233, Validation Percentage:8.2353%,Test Loss: 4.0294, Test Percentage:6.1311%\n",
            "Epoch [25/1200], Training Loss: 3.9286, Validation Loss: 3.9255, Validation Percentage:9.3137%,Test Loss: 4.0502, Test Percentage:5.7408%\n",
            "Epoch [26/1200], Training Loss: 3.8369, Validation Loss: 3.9143, Validation Percentage:7.5490%,Test Loss: 4.0190, Test Percentage:6.3425%\n",
            "Epoch [27/1200], Training Loss: 3.8370, Validation Loss: 3.8925, Validation Percentage:8.7255%,Test Loss: 3.9919, Test Percentage:6.3425%\n",
            "Epoch [28/1200], Training Loss: 3.8754, Validation Loss: 3.9321, Validation Percentage:8.3333%,Test Loss: 3.9974, Test Percentage:7.7899%\n",
            "Epoch [29/1200], Training Loss: 3.8099, Validation Loss: 3.8534, Validation Percentage:9.2157%,Test Loss: 3.9887, Test Percentage:6.7003%\n",
            "Epoch [30/1200], Training Loss: 3.7928, Validation Loss: 3.8036, Validation Percentage:8.9216%,Test Loss: 3.9201, Test Percentage:7.0418%\n",
            "Epoch [31/1200], Training Loss: 3.7398, Validation Loss: 3.8082, Validation Percentage:10.8824%,Test Loss: 3.9226, Test Percentage:7.5459%\n",
            "Epoch [32/1200], Training Loss: 3.7239, Validation Loss: 3.9122, Validation Percentage:9.4118%,Test Loss: 4.0126, Test Percentage:7.3996%\n",
            "Epoch [33/1200], Training Loss: 3.7350, Validation Loss: 3.8196, Validation Percentage:11.3725%,Test Loss: 3.9322, Test Percentage:7.6273%\n",
            "Epoch [34/1200], Training Loss: 3.6695, Validation Loss: 3.7662, Validation Percentage:10.9804%,Test Loss: 3.9036, Test Percentage:8.7169%\n",
            "Epoch [35/1200], Training Loss: 3.7357, Validation Loss: 3.7459, Validation Percentage:11.7647%,Test Loss: 3.8388, Test Percentage:9.1234%\n",
            "Epoch [36/1200], Training Loss: 3.6980, Validation Loss: 3.6913, Validation Percentage:12.8431%,Test Loss: 3.8397, Test Percentage:8.7494%\n",
            "Epoch [37/1200], Training Loss: 3.6226, Validation Loss: 3.7740, Validation Percentage:11.0784%,Test Loss: 3.8800, Test Percentage:8.8307%\n",
            "Epoch [38/1200], Training Loss: 3.6127, Validation Loss: 3.7105, Validation Percentage:10.2941%,Test Loss: 3.8282, Test Percentage:9.4162%\n",
            "Epoch [39/1200], Training Loss: 3.6241, Validation Loss: 3.6955, Validation Percentage:11.9608%,Test Loss: 3.8092, Test Percentage:8.9771%\n",
            "Epoch [40/1200], Training Loss: 3.5417, Validation Loss: 3.6175, Validation Percentage:13.6275%,Test Loss: 3.7904, Test Percentage:9.7577%\n",
            "Epoch [41/1200], Training Loss: 3.5537, Validation Loss: 3.6777, Validation Percentage:11.8627%,Test Loss: 3.8410, Test Percentage:9.4324%\n",
            "Epoch [42/1200], Training Loss: 3.5480, Validation Loss: 3.5890, Validation Percentage:11.9608%,Test Loss: 3.7630, Test Percentage:9.8553%\n",
            "Epoch [43/1200], Training Loss: 3.5481, Validation Loss: 3.6445, Validation Percentage:12.4510%,Test Loss: 3.8015, Test Percentage:10.8636%\n",
            "Epoch [44/1200], Training Loss: 3.5101, Validation Loss: 3.6869, Validation Percentage:12.6471%,Test Loss: 3.8288, Test Percentage:10.5708%\n",
            "Epoch [45/1200], Training Loss: 3.5555, Validation Loss: 3.7061, Validation Percentage:13.3333%,Test Loss: 3.8402, Test Percentage:11.1400%\n",
            "Epoch [46/1200], Training Loss: 3.4984, Validation Loss: 3.6488, Validation Percentage:13.2353%,Test Loss: 3.7816, Test Percentage:10.7009%\n",
            "Epoch [47/1200], Training Loss: 3.4535, Validation Loss: 3.6045, Validation Percentage:14.7059%,Test Loss: 3.7376, Test Percentage:11.1725%\n",
            "Epoch [48/1200], Training Loss: 3.4308, Validation Loss: 3.6307, Validation Percentage:12.6471%,Test Loss: 3.7205, Test Percentage:11.2701%\n",
            "Epoch [49/1200], Training Loss: 3.4280, Validation Loss: 3.5888, Validation Percentage:15.3922%,Test Loss: 3.6860, Test Percentage:12.4248%\n",
            "Epoch [50/1200], Training Loss: 3.3502, Validation Loss: 3.5942, Validation Percentage:13.5294%,Test Loss: 3.7280, Test Percentage:11.7417%\n",
            "Epoch [51/1200], Training Loss: 3.3980, Validation Loss: 3.5799, Validation Percentage:13.1373%,Test Loss: 3.7598, Test Percentage:11.7255%\n",
            "Epoch [52/1200], Training Loss: 3.4551, Validation Loss: 3.6059, Validation Percentage:12.7451%,Test Loss: 3.7128, Test Percentage:12.2622%\n",
            "Epoch [53/1200], Training Loss: 3.3387, Validation Loss: 3.5298, Validation Percentage:14.5098%,Test Loss: 3.6933, Test Percentage:12.3109%\n",
            "Epoch [54/1200], Training Loss: 3.3652, Validation Loss: 3.6011, Validation Percentage:13.4314%,Test Loss: 3.7811, Test Percentage:11.2376%\n",
            "Epoch [55/1200], Training Loss: 3.3471, Validation Loss: 3.5915, Validation Percentage:15.6863%,Test Loss: 3.7359, Test Percentage:12.4248%\n",
            "Epoch [56/1200], Training Loss: 3.2829, Validation Loss: 3.5187, Validation Percentage:14.8039%,Test Loss: 3.7088, Test Percentage:12.5386%\n",
            "Epoch [57/1200], Training Loss: 3.2687, Validation Loss: 3.5658, Validation Percentage:14.5098%,Test Loss: 3.7771, Test Percentage:12.6525%\n",
            "Epoch [58/1200], Training Loss: 3.2651, Validation Loss: 3.4672, Validation Percentage:17.0588%,Test Loss: 3.6327, Test Percentage:13.8071%\n",
            "Epoch [59/1200], Training Loss: 3.2571, Validation Loss: 3.4549, Validation Percentage:16.5686%,Test Loss: 3.6257, Test Percentage:14.1486%\n",
            "Epoch [60/1200], Training Loss: 3.2083, Validation Loss: 3.5241, Validation Percentage:14.9020%,Test Loss: 3.6613, Test Percentage:13.4656%\n",
            "Epoch [61/1200], Training Loss: 3.2676, Validation Loss: 3.4321, Validation Percentage:16.8627%,Test Loss: 3.5772, Test Percentage:14.5389%\n",
            "Epoch [62/1200], Training Loss: 3.2165, Validation Loss: 3.4717, Validation Percentage:16.2745%,Test Loss: 3.6929, Test Percentage:13.8071%\n",
            "Epoch [63/1200], Training Loss: 3.1884, Validation Loss: 3.4564, Validation Percentage:17.2549%,Test Loss: 3.6644, Test Percentage:14.3763%\n",
            "Epoch [64/1200], Training Loss: 3.1882, Validation Loss: 3.4270, Validation Percentage:17.2549%,Test Loss: 3.6840, Test Percentage:14.0348%\n",
            "Epoch [65/1200], Training Loss: 3.1717, Validation Loss: 3.4822, Validation Percentage:15.6863%,Test Loss: 3.6593, Test Percentage:15.1081%\n",
            "Epoch [66/1200], Training Loss: 3.1753, Validation Loss: 3.4146, Validation Percentage:17.8431%,Test Loss: 3.6341, Test Percentage:14.8479%\n",
            "Epoch [67/1200], Training Loss: 3.2166, Validation Loss: 3.5097, Validation Percentage:14.7059%,Test Loss: 3.6518, Test Percentage:15.0919%\n",
            "Epoch [68/1200], Training Loss: 3.1987, Validation Loss: 3.4079, Validation Percentage:16.9608%,Test Loss: 3.6045, Test Percentage:14.7504%\n",
            "Epoch [69/1200], Training Loss: 3.1742, Validation Loss: 3.5197, Validation Percentage:16.8627%,Test Loss: 3.6931, Test Percentage:14.7666%\n",
            "Epoch [70/1200], Training Loss: 3.2412, Validation Loss: 3.4229, Validation Percentage:18.2353%,Test Loss: 3.6117, Test Percentage:14.8317%\n",
            "Epoch [71/1200], Training Loss: 3.1312, Validation Loss: 3.4532, Validation Percentage:16.6667%,Test Loss: 3.6241, Test Percentage:15.3033%\n",
            "Epoch [72/1200], Training Loss: 3.0438, Validation Loss: 3.4610, Validation Percentage:18.0392%,Test Loss: 3.6325, Test Percentage:16.0839%\n",
            "Epoch [73/1200], Training Loss: 3.0875, Validation Loss: 3.4717, Validation Percentage:16.1765%,Test Loss: 3.6130, Test Percentage:15.8562%\n",
            "Epoch [74/1200], Training Loss: 3.0966, Validation Loss: 3.3889, Validation Percentage:18.4314%,Test Loss: 3.5716, Test Percentage:16.0839%\n",
            "Epoch [75/1200], Training Loss: 3.0042, Validation Loss: 3.4534, Validation Percentage:17.2549%,Test Loss: 3.6109, Test Percentage:16.4254%\n",
            "Epoch [76/1200], Training Loss: 3.0112, Validation Loss: 3.3934, Validation Percentage:19.3137%,Test Loss: 3.5591, Test Percentage:16.7507%\n",
            "Epoch [77/1200], Training Loss: 2.9688, Validation Loss: 3.4595, Validation Percentage:17.0588%,Test Loss: 3.5285, Test Percentage:17.0109%\n",
            "Epoch [78/1200], Training Loss: 2.9584, Validation Loss: 3.5411, Validation Percentage:17.3529%,Test Loss: 3.7648, Test Percentage:15.4985%\n",
            "Epoch [79/1200], Training Loss: 2.9709, Validation Loss: 3.4601, Validation Percentage:17.5490%,Test Loss: 3.6292, Test Percentage:16.4905%\n",
            "Epoch [80/1200], Training Loss: 2.9957, Validation Loss: 3.3268, Validation Percentage:21.3725%,Test Loss: 3.6092, Test Percentage:16.5881%\n",
            "Epoch [81/1200], Training Loss: 2.8562, Validation Loss: 3.4801, Validation Percentage:18.0392%,Test Loss: 3.5991, Test Percentage:16.7507%\n",
            "Epoch [82/1200], Training Loss: 2.8775, Validation Loss: 3.2829, Validation Percentage:21.6667%,Test Loss: 3.5736, Test Percentage:17.1410%\n",
            "Epoch [83/1200], Training Loss: 2.9481, Validation Loss: 3.3056, Validation Percentage:21.9608%,Test Loss: 3.4864, Test Percentage:18.2957%\n",
            "Epoch [84/1200], Training Loss: 2.9121, Validation Loss: 3.3752, Validation Percentage:19.7059%,Test Loss: 3.5745, Test Percentage:18.2631%\n",
            "Epoch [85/1200], Training Loss: 2.9058, Validation Loss: 3.3417, Validation Percentage:19.7059%,Test Loss: 3.5019, Test Percentage:18.0192%\n",
            "Epoch [86/1200], Training Loss: 2.8667, Validation Loss: 3.3504, Validation Percentage:19.2157%,Test Loss: 3.5422, Test Percentage:17.8566%\n",
            "Epoch [87/1200], Training Loss: 2.8513, Validation Loss: 3.3804, Validation Percentage:19.1176%,Test Loss: 3.5495, Test Percentage:18.9136%\n",
            "Epoch [88/1200], Training Loss: 2.8091, Validation Loss: 3.3072, Validation Percentage:20.0000%,Test Loss: 3.5152, Test Percentage:19.7430%\n",
            "Epoch [89/1200], Training Loss: 2.8278, Validation Loss: 3.2987, Validation Percentage:22.2549%,Test Loss: 3.4869, Test Percentage:19.1738%\n",
            "Epoch [90/1200], Training Loss: 2.9056, Validation Loss: 3.3457, Validation Percentage:19.6078%,Test Loss: 3.5710, Test Percentage:18.0029%\n",
            "Epoch [91/1200], Training Loss: 2.8181, Validation Loss: 3.3113, Validation Percentage:20.9804%,Test Loss: 3.4901, Test Percentage:19.1088%\n",
            "Epoch [92/1200], Training Loss: 2.7506, Validation Loss: 3.3072, Validation Percentage:21.6667%,Test Loss: 3.5159, Test Percentage:19.2389%\n",
            "Epoch [93/1200], Training Loss: 2.7672, Validation Loss: 3.3188, Validation Percentage:20.7843%,Test Loss: 3.4757, Test Percentage:19.1576%\n",
            "Epoch [94/1200], Training Loss: 2.7805, Validation Loss: 3.2540, Validation Percentage:23.7255%,Test Loss: 3.4453, Test Percentage:19.5316%\n",
            "Epoch [95/1200], Training Loss: 2.6998, Validation Loss: 3.3293, Validation Percentage:20.4902%,Test Loss: 3.4772, Test Percentage:18.7022%\n",
            "Epoch [96/1200], Training Loss: 2.7146, Validation Loss: 3.2466, Validation Percentage:22.6471%,Test Loss: 3.5001, Test Percentage:20.3285%\n",
            "Epoch [97/1200], Training Loss: 2.6976, Validation Loss: 3.4256, Validation Percentage:19.7059%,Test Loss: 3.5104, Test Percentage:20.1984%\n",
            "Epoch [98/1200], Training Loss: 2.7754, Validation Loss: 3.1792, Validation Percentage:23.8235%,Test Loss: 3.4408, Test Percentage:20.3285%\n",
            "Epoch [99/1200], Training Loss: 2.6423, Validation Loss: 3.3156, Validation Percentage:20.4902%,Test Loss: 3.5120, Test Percentage:20.5074%\n",
            "Epoch [100/1200], Training Loss: 2.6599, Validation Loss: 3.2731, Validation Percentage:23.5294%,Test Loss: 3.4907, Test Percentage:20.6212%\n",
            "Epoch [101/1200], Training Loss: 2.6778, Validation Loss: 3.2528, Validation Percentage:23.0392%,Test Loss: 3.4151, Test Percentage:21.2555%\n",
            "Epoch [102/1200], Training Loss: 2.6983, Validation Loss: 3.3078, Validation Percentage:20.9804%,Test Loss: 3.4613, Test Percentage:21.0766%\n",
            "Epoch [103/1200], Training Loss: 2.6221, Validation Loss: 3.2195, Validation Percentage:22.3529%,Test Loss: 3.4182, Test Percentage:20.9302%\n",
            "Epoch [104/1200], Training Loss: 2.6375, Validation Loss: 3.2035, Validation Percentage:22.7451%,Test Loss: 3.4547, Test Percentage:20.8977%\n",
            "Epoch [105/1200], Training Loss: 2.5963, Validation Loss: 3.1260, Validation Percentage:23.1373%,Test Loss: 3.4122, Test Percentage:20.6375%\n",
            "Epoch [106/1200], Training Loss: 2.6263, Validation Loss: 3.2084, Validation Percentage:23.3333%,Test Loss: 3.4385, Test Percentage:21.5320%\n",
            "Epoch [107/1200], Training Loss: 2.5967, Validation Loss: 3.1807, Validation Percentage:23.1373%,Test Loss: 3.4272, Test Percentage:21.8897%\n",
            "Epoch [108/1200], Training Loss: 2.6186, Validation Loss: 3.2995, Validation Percentage:22.4510%,Test Loss: 3.4418, Test Percentage:21.1416%\n",
            "Epoch [109/1200], Training Loss: 2.6556, Validation Loss: 3.3073, Validation Percentage:22.2549%,Test Loss: 3.4781, Test Percentage:22.4752%\n",
            "Epoch [110/1200], Training Loss: 2.5656, Validation Loss: 3.1956, Validation Percentage:24.8039%,Test Loss: 3.4132, Test Percentage:21.6946%\n",
            "Epoch [111/1200], Training Loss: 2.5608, Validation Loss: 3.1757, Validation Percentage:25.3922%,Test Loss: 3.4355, Test Percentage:21.3856%\n",
            "Epoch [112/1200], Training Loss: 2.6245, Validation Loss: 3.2478, Validation Percentage:24.0196%,Test Loss: 3.4209, Test Percentage:21.5970%\n",
            "Epoch [113/1200], Training Loss: 2.5817, Validation Loss: 3.2395, Validation Percentage:24.8039%,Test Loss: 3.4097, Test Percentage:21.6946%\n",
            "Epoch [114/1200], Training Loss: 2.5630, Validation Loss: 3.1738, Validation Percentage:23.7255%,Test Loss: 3.3384, Test Percentage:21.8572%\n",
            "Epoch [115/1200], Training Loss: 2.5680, Validation Loss: 3.1230, Validation Percentage:25.1961%,Test Loss: 3.2714, Test Percentage:23.5485%\n",
            "Epoch [116/1200], Training Loss: 2.5615, Validation Loss: 3.1599, Validation Percentage:26.2745%,Test Loss: 3.3726, Test Percentage:23.4672%\n",
            "Epoch [117/1200], Training Loss: 2.4150, Validation Loss: 3.2332, Validation Percentage:25.5882%,Test Loss: 3.5047, Test Percentage:22.5565%\n",
            "Epoch [118/1200], Training Loss: 2.4786, Validation Loss: 3.1090, Validation Percentage:26.3725%,Test Loss: 3.3922, Test Percentage:22.7517%\n",
            "Epoch [119/1200], Training Loss: 2.4642, Validation Loss: 3.2251, Validation Percentage:26.1765%,Test Loss: 3.3894, Test Percentage:22.0361%\n",
            "Epoch [120/1200], Training Loss: 2.4092, Validation Loss: 3.1898, Validation Percentage:26.0784%,Test Loss: 3.4277, Test Percentage:23.7437%\n",
            "Epoch [121/1200], Training Loss: 2.4297, Validation Loss: 3.1506, Validation Percentage:26.6667%,Test Loss: 3.4573, Test Percentage:22.0849%\n",
            "Epoch [122/1200], Training Loss: 2.4673, Validation Loss: 3.2352, Validation Percentage:23.7255%,Test Loss: 3.4784, Test Percentage:20.7676%\n",
            "Epoch [123/1200], Training Loss: 2.5643, Validation Loss: 3.0608, Validation Percentage:28.1373%,Test Loss: 3.2917, Test Percentage:23.7925%\n",
            "Epoch [124/1200], Training Loss: 2.5276, Validation Loss: 3.1535, Validation Percentage:24.7059%,Test Loss: 3.4017, Test Percentage:22.9306%\n",
            "Epoch [125/1200], Training Loss: 2.4077, Validation Loss: 3.1433, Validation Percentage:25.1961%,Test Loss: 3.3678, Test Percentage:24.2478%\n",
            "Epoch [126/1200], Training Loss: 2.4496, Validation Loss: 3.1902, Validation Percentage:26.0784%,Test Loss: 3.3691, Test Percentage:23.1420%\n",
            "Epoch [127/1200], Training Loss: 2.4574, Validation Loss: 3.1680, Validation Percentage:23.2353%,Test Loss: 3.3994, Test Percentage:23.2721%\n",
            "Epoch [128/1200], Training Loss: 2.4248, Validation Loss: 3.1599, Validation Percentage:27.1569%,Test Loss: 3.3661, Test Percentage:23.4998%\n",
            "Epoch [129/1200], Training Loss: 2.4134, Validation Loss: 3.1430, Validation Percentage:26.3725%,Test Loss: 3.3532, Test Percentage:23.6949%\n",
            "Epoch [130/1200], Training Loss: 2.3620, Validation Loss: 3.1010, Validation Percentage:28.6275%,Test Loss: 3.3749, Test Percentage:23.6461%\n",
            "Epoch [131/1200], Training Loss: 2.4090, Validation Loss: 3.1470, Validation Percentage:25.1961%,Test Loss: 3.3554, Test Percentage:23.0769%\n",
            "Epoch [132/1200], Training Loss: 2.3873, Validation Loss: 3.0917, Validation Percentage:26.5686%,Test Loss: 3.3504, Test Percentage:24.1991%\n",
            "Epoch [133/1200], Training Loss: 2.3921, Validation Loss: 3.2196, Validation Percentage:25.9804%,Test Loss: 3.4013, Test Percentage:23.4672%\n",
            "Epoch [134/1200], Training Loss: 2.3719, Validation Loss: 3.0743, Validation Percentage:29.2157%,Test Loss: 3.4095, Test Percentage:23.2883%\n",
            "Epoch [135/1200], Training Loss: 2.3609, Validation Loss: 3.1306, Validation Percentage:25.0980%,Test Loss: 3.3363, Test Percentage:23.7274%\n",
            "Epoch [136/1200], Training Loss: 2.3718, Validation Loss: 2.9584, Validation Percentage:29.3137%,Test Loss: 3.2538, Test Percentage:25.7115%\n",
            "Epoch [137/1200], Training Loss: 2.2095, Validation Loss: 3.0920, Validation Percentage:27.3529%,Test Loss: 3.4075, Test Percentage:24.3942%\n",
            "Epoch [138/1200], Training Loss: 2.3571, Validation Loss: 3.1408, Validation Percentage:29.0196%,Test Loss: 3.3162, Test Percentage:25.4676%\n",
            "Epoch [139/1200], Training Loss: 2.3026, Validation Loss: 3.0476, Validation Percentage:28.7255%,Test Loss: 3.3458, Test Percentage:25.0285%\n",
            "Epoch [140/1200], Training Loss: 2.3110, Validation Loss: 3.2276, Validation Percentage:25.3922%,Test Loss: 3.3624, Test Percentage:25.8416%\n",
            "Epoch [141/1200], Training Loss: 2.1945, Validation Loss: 3.0749, Validation Percentage:28.5294%,Test Loss: 3.4379, Test Percentage:23.7600%\n",
            "Epoch [142/1200], Training Loss: 2.2499, Validation Loss: 3.0715, Validation Percentage:29.2157%,Test Loss: 3.2798, Test Percentage:24.8984%\n",
            "Epoch [143/1200], Training Loss: 2.2835, Validation Loss: 3.1360, Validation Percentage:29.0196%,Test Loss: 3.3224, Test Percentage:26.3457%\n",
            "Epoch [144/1200], Training Loss: 2.2950, Validation Loss: 3.1482, Validation Percentage:24.5098%,Test Loss: 3.3529, Test Percentage:24.3942%\n",
            "Epoch [145/1200], Training Loss: 2.2636, Validation Loss: 3.0606, Validation Percentage:30.6863%,Test Loss: 3.2785, Test Percentage:25.8253%\n",
            "Epoch [146/1200], Training Loss: 2.1516, Validation Loss: 3.1565, Validation Percentage:26.8627%,Test Loss: 3.3579, Test Percentage:25.4025%\n",
            "Epoch [147/1200], Training Loss: 2.2282, Validation Loss: 3.1115, Validation Percentage:28.3333%,Test Loss: 3.3320, Test Percentage:25.8091%\n",
            "Epoch [148/1200], Training Loss: 2.3390, Validation Loss: 3.0801, Validation Percentage:28.7255%,Test Loss: 3.4261, Test Percentage:24.1015%\n",
            "Epoch [149/1200], Training Loss: 2.2945, Validation Loss: 3.0217, Validation Percentage:29.6078%,Test Loss: 3.3518, Test Percentage:24.3942%\n",
            "Epoch [150/1200], Training Loss: 2.1046, Validation Loss: 3.0857, Validation Percentage:29.5098%,Test Loss: 3.3805, Test Percentage:26.0693%\n",
            "Epoch [151/1200], Training Loss: 2.2506, Validation Loss: 3.0673, Validation Percentage:27.9412%,Test Loss: 3.2887, Test Percentage:25.6302%\n",
            "Epoch [152/1200], Training Loss: 2.1221, Validation Loss: 3.1260, Validation Percentage:28.3333%,Test Loss: 3.4870, Test Percentage:25.2074%\n",
            "Epoch [153/1200], Training Loss: 2.2163, Validation Loss: 3.0212, Validation Percentage:30.1961%,Test Loss: 3.2819, Test Percentage:26.5572%\n",
            "Epoch [154/1200], Training Loss: 2.1556, Validation Loss: 3.2153, Validation Percentage:27.6471%,Test Loss: 3.4223, Test Percentage:24.6382%\n",
            "Epoch [155/1200], Training Loss: 2.1402, Validation Loss: 3.0524, Validation Percentage:30.4902%,Test Loss: 3.3209, Test Percentage:26.1181%\n",
            "Epoch [156/1200], Training Loss: 2.1480, Validation Loss: 3.0110, Validation Percentage:29.5098%,Test Loss: 3.2886, Test Percentage:26.4596%\n",
            "Epoch [157/1200], Training Loss: 2.0995, Validation Loss: 2.9874, Validation Percentage:30.1961%,Test Loss: 3.3276, Test Percentage:26.5897%\n",
            "Epoch [158/1200], Training Loss: 2.1281, Validation Loss: 3.1275, Validation Percentage:27.4510%,Test Loss: 3.3445, Test Percentage:25.9880%\n",
            "Epoch [159/1200], Training Loss: 2.1293, Validation Loss: 3.0298, Validation Percentage:28.2353%,Test Loss: 3.2663, Test Percentage:26.7198%\n",
            "Epoch [160/1200], Training Loss: 2.2288, Validation Loss: 2.9545, Validation Percentage:30.8824%,Test Loss: 3.3028, Test Percentage:26.1181%\n",
            "Epoch [161/1200], Training Loss: 2.1298, Validation Loss: 3.1234, Validation Percentage:28.8235%,Test Loss: 3.3510, Test Percentage:26.6222%\n",
            "Epoch [162/1200], Training Loss: 2.1711, Validation Loss: 3.0778, Validation Percentage:29.2157%,Test Loss: 3.3183, Test Percentage:26.4596%\n",
            "Epoch [163/1200], Training Loss: 2.1093, Validation Loss: 2.9984, Validation Percentage:30.0980%,Test Loss: 3.3767, Test Percentage:26.4108%\n",
            "Epoch [164/1200], Training Loss: 2.0703, Validation Loss: 3.0928, Validation Percentage:29.4118%,Test Loss: 3.2845, Test Percentage:26.7848%\n",
            "Epoch [165/1200], Training Loss: 2.2295, Validation Loss: 3.0726, Validation Percentage:29.2157%,Test Loss: 3.3433, Test Percentage:26.7035%\n",
            "Epoch [166/1200], Training Loss: 2.1605, Validation Loss: 2.9367, Validation Percentage:30.0000%,Test Loss: 3.2284, Test Percentage:28.2973%\n",
            "Epoch [167/1200], Training Loss: 2.1250, Validation Loss: 2.9616, Validation Percentage:30.1961%,Test Loss: 3.2108, Test Percentage:26.6222%\n",
            "Epoch [168/1200], Training Loss: 2.0645, Validation Loss: 3.0841, Validation Percentage:29.8039%,Test Loss: 3.3891, Test Percentage:27.5492%\n",
            "Epoch [169/1200], Training Loss: 2.1096, Validation Loss: 3.0434, Validation Percentage:30.0000%,Test Loss: 3.3008, Test Percentage:26.7198%\n",
            "Epoch [170/1200], Training Loss: 2.0990, Validation Loss: 3.0406, Validation Percentage:29.4118%,Test Loss: 3.3238, Test Percentage:28.2973%\n",
            "Epoch [171/1200], Training Loss: 2.1303, Validation Loss: 3.0354, Validation Percentage:30.5882%,Test Loss: 3.2197, Test Percentage:28.0208%\n",
            "Epoch [172/1200], Training Loss: 2.1434, Validation Loss: 3.0655, Validation Percentage:30.6863%,Test Loss: 3.2948, Test Percentage:26.9637%\n",
            "Epoch [173/1200], Training Loss: 2.0439, Validation Loss: 3.0176, Validation Percentage:30.4902%,Test Loss: 3.2918, Test Percentage:28.2160%\n",
            "Epoch [174/1200], Training Loss: 2.0460, Validation Loss: 2.9625, Validation Percentage:33.6275%,Test Loss: 3.2806, Test Percentage:27.2402%\n",
            "Epoch [175/1200], Training Loss: 1.9835, Validation Loss: 3.0543, Validation Percentage:28.4314%,Test Loss: 3.3137, Test Percentage:26.3457%\n",
            "Epoch [176/1200], Training Loss: 2.0437, Validation Loss: 3.0799, Validation Percentage:29.4118%,Test Loss: 3.2762, Test Percentage:27.1589%\n",
            "Epoch [177/1200], Training Loss: 2.1492, Validation Loss: 2.9978, Validation Percentage:30.3922%,Test Loss: 3.2778, Test Percentage:27.2077%\n",
            "Epoch [178/1200], Training Loss: 2.0646, Validation Loss: 3.0141, Validation Percentage:29.1176%,Test Loss: 3.3116, Test Percentage:26.1343%\n",
            "Epoch [179/1200], Training Loss: 2.0168, Validation Loss: 3.1687, Validation Percentage:29.5098%,Test Loss: 3.4513, Test Percentage:25.9392%\n",
            "Epoch [180/1200], Training Loss: 1.9787, Validation Loss: 3.0260, Validation Percentage:31.1765%,Test Loss: 3.3739, Test Percentage:27.9558%\n",
            "Epoch [181/1200], Training Loss: 2.1007, Validation Loss: 2.9721, Validation Percentage:31.4706%,Test Loss: 3.2699, Test Percentage:27.6630%\n",
            "Epoch [182/1200], Training Loss: 2.0473, Validation Loss: 3.0507, Validation Percentage:29.3137%,Test Loss: 3.2999, Test Percentage:27.5980%\n",
            "Epoch [183/1200], Training Loss: 1.9402, Validation Loss: 3.0332, Validation Percentage:30.3922%,Test Loss: 3.2957, Test Percentage:27.7281%\n",
            "Epoch [184/1200], Training Loss: 1.9476, Validation Loss: 3.0318, Validation Percentage:31.2745%,Test Loss: 3.2944, Test Percentage:28.1834%\n",
            "Epoch [185/1200], Training Loss: 1.9492, Validation Loss: 3.0563, Validation Percentage:33.1373%,Test Loss: 3.3156, Test Percentage:27.4354%\n",
            "Epoch [186/1200], Training Loss: 1.9653, Validation Loss: 3.1266, Validation Percentage:32.1569%,Test Loss: 3.3540, Test Percentage:28.3461%\n",
            "Epoch [187/1200], Training Loss: 1.9936, Validation Loss: 3.0458, Validation Percentage:30.6863%,Test Loss: 3.1988, Test Percentage:29.5495%\n",
            "Epoch [188/1200], Training Loss: 1.9584, Validation Loss: 3.0299, Validation Percentage:32.1569%,Test Loss: 3.2789, Test Percentage:29.1267%\n",
            "Epoch [189/1200], Training Loss: 1.8820, Validation Loss: 3.0566, Validation Percentage:31.4706%,Test Loss: 3.3135, Test Percentage:28.3461%\n",
            "Epoch [190/1200], Training Loss: 1.8833, Validation Loss: 3.1635, Validation Percentage:31.6667%,Test Loss: 3.3868, Test Percentage:28.1021%\n",
            "Epoch [191/1200], Training Loss: 1.9425, Validation Loss: 2.9858, Validation Percentage:32.0588%,Test Loss: 3.2026, Test Percentage:29.6471%\n",
            "Epoch [192/1200], Training Loss: 1.9994, Validation Loss: 2.9639, Validation Percentage:32.1569%,Test Loss: 3.2725, Test Percentage:28.8014%\n",
            "Epoch [193/1200], Training Loss: 1.9248, Validation Loss: 3.0440, Validation Percentage:30.9804%,Test Loss: 3.3373, Test Percentage:28.1347%\n",
            "Epoch [194/1200], Training Loss: 2.0165, Validation Loss: 2.9387, Validation Percentage:30.2941%,Test Loss: 3.2789, Test Percentage:29.3056%\n",
            "Epoch [195/1200], Training Loss: 1.9587, Validation Loss: 3.0151, Validation Percentage:31.0784%,Test Loss: 3.2488, Test Percentage:27.0125%\n",
            "Epoch [196/1200], Training Loss: 1.8942, Validation Loss: 3.0017, Validation Percentage:32.0588%,Test Loss: 3.2512, Test Percentage:28.5412%\n",
            "Epoch [197/1200], Training Loss: 1.9083, Validation Loss: 3.0637, Validation Percentage:32.4510%,Test Loss: 3.3167, Test Percentage:28.0533%\n",
            "Epoch [198/1200], Training Loss: 1.8999, Validation Loss: 3.2361, Validation Percentage:30.0980%,Test Loss: 3.5142, Test Percentage:26.8824%\n",
            "Epoch [199/1200], Training Loss: 1.9119, Validation Loss: 3.0445, Validation Percentage:34.8039%,Test Loss: 3.3881, Test Percentage:28.0533%\n",
            "Epoch [200/1200], Training Loss: 1.9033, Validation Loss: 2.9225, Validation Percentage:31.3725%,Test Loss: 3.3003, Test Percentage:27.7769%\n",
            "Epoch [201/1200], Training Loss: 1.9212, Validation Loss: 3.0579, Validation Percentage:31.2745%,Test Loss: 3.3638, Test Percentage:29.4357%\n",
            "Epoch [202/1200], Training Loss: 1.8276, Validation Loss: 3.1119, Validation Percentage:33.1373%,Test Loss: 3.3186, Test Percentage:29.4032%\n",
            "Epoch [203/1200], Training Loss: 1.9574, Validation Loss: 3.0187, Validation Percentage:31.8627%,Test Loss: 3.3167, Test Percentage:28.7364%\n",
            "Epoch [204/1200], Training Loss: 1.9262, Validation Loss: 3.0855, Validation Percentage:31.8627%,Test Loss: 3.3494, Test Percentage:28.5575%\n",
            "Epoch [205/1200], Training Loss: 1.9303, Validation Loss: 3.0498, Validation Percentage:31.7647%,Test Loss: 3.2826, Test Percentage:29.1917%\n",
            "Epoch [206/1200], Training Loss: 1.8223, Validation Loss: 3.0982, Validation Percentage:31.0784%,Test Loss: 3.4137, Test Percentage:29.5007%\n",
            "Epoch [207/1200], Training Loss: 1.9283, Validation Loss: 2.8599, Validation Percentage:32.3529%,Test Loss: 3.0720, Test Percentage:30.2813%\n",
            "Epoch [208/1200], Training Loss: 1.9099, Validation Loss: 2.9929, Validation Percentage:33.5294%,Test Loss: 3.2859, Test Percentage:29.6308%\n",
            "Epoch [209/1200], Training Loss: 1.8752, Validation Loss: 2.9564, Validation Percentage:32.5490%,Test Loss: 3.2058, Test Percentage:29.0291%\n",
            "Epoch [210/1200], Training Loss: 1.8738, Validation Loss: 2.9990, Validation Percentage:33.3333%,Test Loss: 3.2748, Test Percentage:29.8097%\n",
            "Epoch [211/1200], Training Loss: 1.8702, Validation Loss: 3.0093, Validation Percentage:33.2353%,Test Loss: 3.2242, Test Percentage:30.0374%\n",
            "Epoch [212/1200], Training Loss: 1.8284, Validation Loss: 2.9572, Validation Percentage:32.1569%,Test Loss: 3.2740, Test Percentage:29.9398%\n",
            "Epoch [213/1200], Training Loss: 1.8125, Validation Loss: 3.1026, Validation Percentage:32.0588%,Test Loss: 3.3320, Test Percentage:30.5903%\n",
            "Epoch [214/1200], Training Loss: 1.8495, Validation Loss: 2.9759, Validation Percentage:33.7255%,Test Loss: 3.3008, Test Percentage:30.6717%\n",
            "Epoch [215/1200], Training Loss: 1.8557, Validation Loss: 3.1205, Validation Percentage:31.1765%,Test Loss: 3.3935, Test Percentage:29.8910%\n",
            "Epoch [216/1200], Training Loss: 1.8138, Validation Loss: 3.1769, Validation Percentage:31.5686%,Test Loss: 3.3843, Test Percentage:30.4765%\n",
            "Epoch [217/1200], Training Loss: 1.7260, Validation Loss: 3.0639, Validation Percentage:32.9412%,Test Loss: 3.3555, Test Percentage:29.2731%\n",
            "Epoch [218/1200], Training Loss: 1.8171, Validation Loss: 3.0942, Validation Percentage:30.0000%,Test Loss: 3.3882, Test Percentage:28.9641%\n",
            "Epoch [219/1200], Training Loss: 1.7723, Validation Loss: 2.9971, Validation Percentage:33.1373%,Test Loss: 3.2549, Test Percentage:30.0374%\n",
            "Epoch [220/1200], Training Loss: 1.7476, Validation Loss: 3.0941, Validation Percentage:34.5098%,Test Loss: 3.3795, Test Percentage:30.4602%\n",
            "Epoch [221/1200], Training Loss: 1.7834, Validation Loss: 2.9115, Validation Percentage:34.0196%,Test Loss: 3.2391, Test Percentage:30.6229%\n",
            "Epoch [222/1200], Training Loss: 1.8210, Validation Loss: 2.9246, Validation Percentage:35.0000%,Test Loss: 3.2214, Test Percentage:31.0457%\n",
            "Epoch [223/1200], Training Loss: 1.7267, Validation Loss: 3.0933, Validation Percentage:32.9412%,Test Loss: 3.3088, Test Percentage:30.6229%\n",
            "Epoch [224/1200], Training Loss: 1.8813, Validation Loss: 2.9326, Validation Percentage:31.5686%,Test Loss: 3.2439, Test Percentage:30.4114%\n",
            "Epoch [225/1200], Training Loss: 1.8328, Validation Loss: 2.9739, Validation Percentage:34.2157%,Test Loss: 3.2534, Test Percentage:30.5416%\n",
            "Epoch [226/1200], Training Loss: 1.8375, Validation Loss: 3.0276, Validation Percentage:30.9804%,Test Loss: 3.2321, Test Percentage:29.5495%\n",
            "Epoch [227/1200], Training Loss: 1.8253, Validation Loss: 2.9790, Validation Percentage:32.2549%,Test Loss: 3.2115, Test Percentage:30.0862%\n",
            "Epoch [228/1200], Training Loss: 1.7307, Validation Loss: 2.9200, Validation Percentage:33.3333%,Test Loss: 3.1828, Test Percentage:31.1758%\n",
            "Epoch [229/1200], Training Loss: 1.7179, Validation Loss: 2.9773, Validation Percentage:32.7451%,Test Loss: 3.2062, Test Percentage:32.2979%\n",
            "Epoch [230/1200], Training Loss: 1.7947, Validation Loss: 3.0715, Validation Percentage:33.8235%,Test Loss: 3.3659, Test Percentage:29.7609%\n",
            "Epoch [231/1200], Training Loss: 1.7337, Validation Loss: 2.9281, Validation Percentage:32.1569%,Test Loss: 3.1435, Test Percentage:29.8260%\n",
            "Epoch [232/1200], Training Loss: 1.7564, Validation Loss: 2.8715, Validation Percentage:33.5294%,Test Loss: 3.2176, Test Percentage:30.3789%\n",
            "Epoch [233/1200], Training Loss: 1.8243, Validation Loss: 2.9593, Validation Percentage:33.9216%,Test Loss: 3.2433, Test Percentage:29.2405%\n",
            "Epoch [234/1200], Training Loss: 1.7624, Validation Loss: 2.9728, Validation Percentage:32.2549%,Test Loss: 3.2179, Test Percentage:29.5495%\n",
            "Epoch [235/1200], Training Loss: 1.8096, Validation Loss: 2.8767, Validation Percentage:33.2353%,Test Loss: 3.1630, Test Percentage:30.2651%\n",
            "Epoch [236/1200], Training Loss: 1.7029, Validation Loss: 2.9552, Validation Percentage:33.0392%,Test Loss: 3.2340, Test Percentage:30.9969%\n",
            "Epoch [237/1200], Training Loss: 1.6796, Validation Loss: 2.8747, Validation Percentage:35.1961%,Test Loss: 3.2184, Test Percentage:30.8505%\n",
            "Epoch [238/1200], Training Loss: 1.8094, Validation Loss: 3.0171, Validation Percentage:33.6275%,Test Loss: 3.2094, Test Percentage:30.8993%\n",
            "Epoch [239/1200], Training Loss: 1.6488, Validation Loss: 2.9949, Validation Percentage:34.7059%,Test Loss: 3.2209, Test Percentage:31.3710%\n",
            "Epoch [240/1200], Training Loss: 1.8416, Validation Loss: 2.9202, Validation Percentage:35.0980%,Test Loss: 3.1855, Test Percentage:31.3222%\n",
            "Epoch [241/1200], Training Loss: 1.7385, Validation Loss: 3.0780, Validation Percentage:31.5686%,Test Loss: 3.2428, Test Percentage:30.6229%\n",
            "Epoch [242/1200], Training Loss: 1.6396, Validation Loss: 2.9189, Validation Percentage:35.4902%,Test Loss: 3.3695, Test Percentage:30.6391%\n",
            "Epoch [243/1200], Training Loss: 1.7114, Validation Loss: 2.9491, Validation Percentage:34.2157%,Test Loss: 3.2476, Test Percentage:31.0294%\n",
            "Epoch [244/1200], Training Loss: 1.6434, Validation Loss: 2.9810, Validation Percentage:35.9804%,Test Loss: 3.3432, Test Percentage:31.2571%\n",
            "Epoch [245/1200], Training Loss: 1.7957, Validation Loss: 2.7499, Validation Percentage:33.9216%,Test Loss: 3.1361, Test Percentage:30.5090%\n",
            "Epoch [246/1200], Training Loss: 1.7119, Validation Loss: 2.8690, Validation Percentage:34.1176%,Test Loss: 3.1423, Test Percentage:30.9806%\n",
            "Epoch [247/1200], Training Loss: 1.7377, Validation Loss: 2.9780, Validation Percentage:33.4314%,Test Loss: 3.1726, Test Percentage:32.1190%\n",
            "Epoch [248/1200], Training Loss: 1.6735, Validation Loss: 2.8563, Validation Percentage:35.8824%,Test Loss: 3.2287, Test Percentage:31.5336%\n",
            "Epoch [249/1200], Training Loss: 1.7823, Validation Loss: 3.0116, Validation Percentage:32.5490%,Test Loss: 3.2378, Test Percentage:31.1433%\n",
            "Epoch [250/1200], Training Loss: 1.6694, Validation Loss: 2.9422, Validation Percentage:33.9216%,Test Loss: 3.2279, Test Percentage:31.5661%\n",
            "Epoch [251/1200], Training Loss: 1.6872, Validation Loss: 2.9883, Validation Percentage:33.7255%,Test Loss: 3.1670, Test Percentage:31.3384%\n",
            "Epoch [252/1200], Training Loss: 1.6396, Validation Loss: 2.9088, Validation Percentage:36.3725%,Test Loss: 3.1610, Test Percentage:32.3142%\n",
            "Epoch [253/1200], Training Loss: 1.6136, Validation Loss: 3.0291, Validation Percentage:33.5294%,Test Loss: 3.2115, Test Percentage:32.1353%\n",
            "Epoch [254/1200], Training Loss: 1.5956, Validation Loss: 2.9551, Validation Percentage:35.4902%,Test Loss: 3.2147, Test Percentage:32.4280%\n",
            "Epoch [255/1200], Training Loss: 1.6172, Validation Loss: 3.0241, Validation Percentage:33.7255%,Test Loss: 3.2307, Test Percentage:32.6882%\n",
            "Epoch [256/1200], Training Loss: 1.6732, Validation Loss: 2.9920, Validation Percentage:33.5294%,Test Loss: 3.1829, Test Percentage:32.2491%\n",
            "Epoch [257/1200], Training Loss: 1.6250, Validation Loss: 3.0256, Validation Percentage:32.7451%,Test Loss: 3.2269, Test Percentage:30.3464%\n",
            "Epoch [258/1200], Training Loss: 1.5702, Validation Loss: 3.1043, Validation Percentage:32.8431%,Test Loss: 3.5468, Test Percentage:29.8097%\n",
            "Epoch [259/1200], Training Loss: 1.6381, Validation Loss: 2.9200, Validation Percentage:37.2549%,Test Loss: 3.1866, Test Percentage:33.1436%\n",
            "Epoch [260/1200], Training Loss: 1.7663, Validation Loss: 3.0105, Validation Percentage:33.7255%,Test Loss: 3.3557, Test Percentage:31.0782%\n",
            "Epoch [261/1200], Training Loss: 1.7113, Validation Loss: 2.9153, Validation Percentage:35.5882%,Test Loss: 3.2061, Test Percentage:31.8101%\n",
            "Epoch [262/1200], Training Loss: 1.6869, Validation Loss: 2.9311, Validation Percentage:35.8824%,Test Loss: 3.2381, Test Percentage:31.4685%\n",
            "Epoch [263/1200], Training Loss: 1.6021, Validation Loss: 2.9135, Validation Percentage:35.4902%,Test Loss: 3.2093, Test Percentage:31.9076%\n",
            "Epoch [264/1200], Training Loss: 1.6063, Validation Loss: 2.9072, Validation Percentage:35.0980%,Test Loss: 3.2185, Test Percentage:32.6882%\n",
            "Epoch [265/1200], Training Loss: 1.6885, Validation Loss: 2.9902, Validation Percentage:35.7843%,Test Loss: 3.3669, Test Percentage:31.9727%\n",
            "Epoch [266/1200], Training Loss: 1.5697, Validation Loss: 3.1229, Validation Percentage:33.0392%,Test Loss: 3.3780, Test Percentage:31.5824%\n",
            "Epoch [267/1200], Training Loss: 1.6230, Validation Loss: 3.0552, Validation Percentage:33.8235%,Test Loss: 3.3191, Test Percentage:30.9644%\n",
            "Epoch [268/1200], Training Loss: 1.4870, Validation Loss: 3.0115, Validation Percentage:35.5882%,Test Loss: 3.2383, Test Percentage:33.5502%\n",
            "Epoch [269/1200], Training Loss: 1.6886, Validation Loss: 2.9546, Validation Percentage:37.2549%,Test Loss: 3.1531, Test Percentage:34.3308%\n",
            "Epoch [270/1200], Training Loss: 1.5009, Validation Loss: 2.9219, Validation Percentage:35.0000%,Test Loss: 3.1748, Test Percentage:32.8183%\n",
            "Epoch [271/1200], Training Loss: 1.6746, Validation Loss: 2.9335, Validation Percentage:35.0000%,Test Loss: 3.2117, Test Percentage:32.6557%\n",
            "Epoch [272/1200], Training Loss: 1.5485, Validation Loss: 3.0224, Validation Percentage:35.0000%,Test Loss: 3.3030, Test Percentage:31.5011%\n",
            "Epoch [273/1200], Training Loss: 1.5085, Validation Loss: 2.9714, Validation Percentage:34.1176%,Test Loss: 3.1468, Test Percentage:32.1841%\n",
            "Epoch [274/1200], Training Loss: 1.6772, Validation Loss: 2.9233, Validation Percentage:35.0000%,Test Loss: 3.2144, Test Percentage:32.4931%\n",
            "Epoch [275/1200], Training Loss: 1.5902, Validation Loss: 2.9821, Validation Percentage:34.1176%,Test Loss: 3.3337, Test Percentage:30.6717%\n",
            "Epoch [276/1200], Training Loss: 1.6049, Validation Loss: 2.9323, Validation Percentage:34.5098%,Test Loss: 3.1819, Test Percentage:32.5094%\n",
            "Epoch [277/1200], Training Loss: 1.5167, Validation Loss: 2.9919, Validation Percentage:35.3922%,Test Loss: 3.3290, Test Percentage:33.0623%\n",
            "Epoch [278/1200], Training Loss: 1.7837, Validation Loss: 2.9658, Validation Percentage:36.9608%,Test Loss: 3.2788, Test Percentage:31.2734%\n",
            "Epoch [279/1200], Training Loss: 1.5986, Validation Loss: 2.8961, Validation Percentage:35.1961%,Test Loss: 3.0509, Test Percentage:33.6640%\n",
            "Epoch [280/1200], Training Loss: 1.5707, Validation Loss: 2.9280, Validation Percentage:35.2941%,Test Loss: 3.2253, Test Percentage:33.0298%\n",
            "Epoch [281/1200], Training Loss: 1.4878, Validation Loss: 2.9075, Validation Percentage:36.3725%,Test Loss: 3.2918, Test Percentage:32.6557%\n",
            "Epoch [282/1200], Training Loss: 1.5385, Validation Loss: 2.8605, Validation Percentage:37.4510%,Test Loss: 3.2260, Test Percentage:32.5256%\n",
            "Epoch [283/1200], Training Loss: 1.4876, Validation Loss: 3.0275, Validation Percentage:35.8824%,Test Loss: 3.3013, Test Percentage:32.0703%\n",
            "Epoch [284/1200], Training Loss: 1.6300, Validation Loss: 2.9161, Validation Percentage:35.6863%,Test Loss: 3.1147, Test Percentage:34.1031%\n",
            "Epoch [285/1200], Training Loss: 1.5353, Validation Loss: 2.9659, Validation Percentage:36.1765%,Test Loss: 3.3734, Test Percentage:32.6395%\n",
            "Epoch [286/1200], Training Loss: 1.4158, Validation Loss: 3.1105, Validation Percentage:36.7647%,Test Loss: 3.3942, Test Percentage:32.9972%\n",
            "Epoch [287/1200], Training Loss: 1.6166, Validation Loss: 2.9460, Validation Percentage:35.6863%,Test Loss: 3.1834, Test Percentage:31.1270%\n",
            "Epoch [288/1200], Training Loss: 1.6019, Validation Loss: 3.0560, Validation Percentage:36.3725%,Test Loss: 3.3377, Test Percentage:33.4851%\n",
            "Epoch [289/1200], Training Loss: 1.4576, Validation Loss: 2.9330, Validation Percentage:34.6078%,Test Loss: 3.2687, Test Percentage:32.3305%\n",
            "Epoch [290/1200], Training Loss: 1.5898, Validation Loss: 2.8763, Validation Percentage:35.4902%,Test Loss: 3.1516, Test Percentage:33.5827%\n",
            "Epoch [291/1200], Training Loss: 1.5114, Validation Loss: 2.9376, Validation Percentage:37.2549%,Test Loss: 3.2957, Test Percentage:33.7779%\n",
            "Epoch [292/1200], Training Loss: 1.5179, Validation Loss: 2.8977, Validation Percentage:36.1765%,Test Loss: 3.1137, Test Percentage:33.8104%\n",
            "Epoch [293/1200], Training Loss: 1.4978, Validation Loss: 2.9146, Validation Percentage:37.7451%,Test Loss: 3.1339, Test Percentage:33.3062%\n",
            "Epoch [294/1200], Training Loss: 1.5205, Validation Loss: 3.1275, Validation Percentage:33.7255%,Test Loss: 3.5372, Test Percentage:31.0620%\n",
            "Epoch [295/1200], Training Loss: 1.4784, Validation Loss: 2.9466, Validation Percentage:35.1961%,Test Loss: 3.1503, Test Percentage:32.8021%\n",
            "Epoch [296/1200], Training Loss: 1.4690, Validation Loss: 2.8850, Validation Percentage:35.7843%,Test Loss: 3.0959, Test Percentage:33.4201%\n",
            "Epoch [297/1200], Training Loss: 1.4929, Validation Loss: 2.8943, Validation Percentage:37.6471%,Test Loss: 3.2540, Test Percentage:33.2249%\n",
            "Epoch [298/1200], Training Loss: 1.5020, Validation Loss: 3.0845, Validation Percentage:33.1373%,Test Loss: 3.4351, Test Percentage:30.7530%\n",
            "Epoch [299/1200], Training Loss: 1.4891, Validation Loss: 2.9476, Validation Percentage:36.3725%,Test Loss: 3.1512, Test Percentage:33.3388%\n",
            "Epoch [300/1200], Training Loss: 1.4760, Validation Loss: 2.9062, Validation Percentage:34.5098%,Test Loss: 3.1970, Test Percentage:31.0620%\n",
            "Epoch [301/1200], Training Loss: 1.5243, Validation Loss: 3.0203, Validation Percentage:35.8824%,Test Loss: 3.2568, Test Percentage:32.5094%\n",
            "Epoch [302/1200], Training Loss: 1.5417, Validation Loss: 2.9661, Validation Percentage:35.5882%,Test Loss: 3.2613, Test Percentage:32.1353%\n",
            "Epoch [303/1200], Training Loss: 1.5774, Validation Loss: 3.0178, Validation Percentage:32.7451%,Test Loss: 3.2272, Test Percentage:31.9239%\n",
            "Epoch [304/1200], Training Loss: 1.5209, Validation Loss: 3.1744, Validation Percentage:33.5294%,Test Loss: 3.3232, Test Percentage:31.4848%\n",
            "Epoch [305/1200], Training Loss: 1.5665, Validation Loss: 2.9564, Validation Percentage:34.8039%,Test Loss: 3.2297, Test Percentage:33.3062%\n",
            "Epoch [306/1200], Training Loss: 1.5471, Validation Loss: 2.9416, Validation Percentage:35.0980%,Test Loss: 3.1450, Test Percentage:33.2737%\n",
            "Epoch [307/1200], Training Loss: 1.3467, Validation Loss: 2.9752, Validation Percentage:37.3529%,Test Loss: 3.2968, Test Percentage:33.0135%\n",
            "Epoch [308/1200], Training Loss: 1.5267, Validation Loss: 3.0733, Validation Percentage:37.7451%,Test Loss: 3.4870, Test Percentage:31.3547%\n",
            "Epoch [309/1200], Training Loss: 1.5622, Validation Loss: 3.0991, Validation Percentage:36.8627%,Test Loss: 3.4875, Test Percentage:32.0865%\n",
            "Epoch [310/1200], Training Loss: 1.4333, Validation Loss: 2.9413, Validation Percentage:34.5098%,Test Loss: 3.2431, Test Percentage:33.1599%\n",
            "Epoch [311/1200], Training Loss: 1.4949, Validation Loss: 2.8585, Validation Percentage:36.9608%,Test Loss: 3.2148, Test Percentage:32.9647%\n",
            "Epoch [312/1200], Training Loss: 1.5068, Validation Loss: 3.0457, Validation Percentage:36.5686%,Test Loss: 3.2161, Test Percentage:32.7370%\n",
            "Epoch [313/1200], Training Loss: 1.4260, Validation Loss: 2.9981, Validation Percentage:36.2745%,Test Loss: 3.2550, Test Percentage:34.0055%\n",
            "Epoch [314/1200], Training Loss: 1.4594, Validation Loss: 3.0488, Validation Percentage:35.9804%,Test Loss: 3.3096, Test Percentage:33.5176%\n",
            "Epoch [315/1200], Training Loss: 1.5977, Validation Loss: 2.7936, Validation Percentage:36.2745%,Test Loss: 3.0249, Test Percentage:33.7291%\n",
            "Epoch [316/1200], Training Loss: 1.4429, Validation Loss: 2.9530, Validation Percentage:36.8627%,Test Loss: 3.3047, Test Percentage:34.0218%\n",
            "Epoch [317/1200], Training Loss: 1.4964, Validation Loss: 2.9628, Validation Percentage:33.6275%,Test Loss: 3.1551, Test Percentage:33.9567%\n",
            "Epoch [318/1200], Training Loss: 1.4543, Validation Loss: 2.7802, Validation Percentage:37.9412%,Test Loss: 3.0803, Test Percentage:33.5339%\n",
            "Epoch [319/1200], Training Loss: 1.4239, Validation Loss: 2.9713, Validation Percentage:35.0980%,Test Loss: 3.2595, Test Percentage:32.5419%\n",
            "Epoch [320/1200], Training Loss: 1.5436, Validation Loss: 2.8893, Validation Percentage:34.3137%,Test Loss: 3.0841, Test Percentage:33.8917%\n",
            "Epoch [321/1200], Training Loss: 1.4319, Validation Loss: 2.8957, Validation Percentage:37.3529%,Test Loss: 3.1803, Test Percentage:34.1519%\n",
            "Epoch [322/1200], Training Loss: 1.5315, Validation Loss: 3.0586, Validation Percentage:34.7059%,Test Loss: 3.2586, Test Percentage:32.0703%\n",
            "Epoch [323/1200], Training Loss: 1.5109, Validation Loss: 2.8802, Validation Percentage:35.3922%,Test Loss: 3.1822, Test Percentage:32.1190%\n",
            "Epoch [324/1200], Training Loss: 1.4684, Validation Loss: 2.9543, Validation Percentage:36.5686%,Test Loss: 3.1350, Test Percentage:34.9813%\n",
            "Epoch [325/1200], Training Loss: 1.4326, Validation Loss: 3.0821, Validation Percentage:36.4706%,Test Loss: 3.3493, Test Percentage:32.5419%\n",
            "Epoch [326/1200], Training Loss: 1.4430, Validation Loss: 2.8489, Validation Percentage:39.9020%,Test Loss: 3.1441, Test Percentage:33.6477%\n",
            "Epoch [327/1200], Training Loss: 1.4390, Validation Loss: 2.8622, Validation Percentage:37.8431%,Test Loss: 3.1312, Test Percentage:34.9650%\n",
            "Epoch [328/1200], Training Loss: 1.3809, Validation Loss: 2.9639, Validation Percentage:37.1569%,Test Loss: 3.1303, Test Percentage:34.5910%\n",
            "Epoch [329/1200], Training Loss: 1.4611, Validation Loss: 3.1463, Validation Percentage:33.2353%,Test Loss: 3.4583, Test Percentage:31.8751%\n",
            "Epoch [330/1200], Training Loss: 1.4594, Validation Loss: 2.9342, Validation Percentage:36.8627%,Test Loss: 3.2619, Test Percentage:34.9650%\n",
            "Epoch [331/1200], Training Loss: 1.4117, Validation Loss: 2.9867, Validation Percentage:37.6471%,Test Loss: 3.3168, Test Percentage:33.8754%\n",
            "Epoch [332/1200], Training Loss: 1.5035, Validation Loss: 3.0751, Validation Percentage:33.9216%,Test Loss: 3.1504, Test Percentage:34.1031%\n",
            "Epoch [333/1200], Training Loss: 1.5045, Validation Loss: 2.8567, Validation Percentage:35.2941%,Test Loss: 3.1115, Test Percentage:34.3470%\n",
            "Epoch [334/1200], Training Loss: 1.4040, Validation Loss: 2.8205, Validation Percentage:37.9412%,Test Loss: 3.1613, Test Percentage:35.4529%\n",
            "Epoch [335/1200], Training Loss: 1.3967, Validation Loss: 2.8578, Validation Percentage:36.7647%,Test Loss: 3.0786, Test Percentage:34.7536%\n",
            "Epoch [336/1200], Training Loss: 1.4718, Validation Loss: 2.9422, Validation Percentage:36.4706%,Test Loss: 3.1823, Test Percentage:34.2332%\n",
            "Epoch [337/1200], Training Loss: 1.4560, Validation Loss: 3.0388, Validation Percentage:36.6667%,Test Loss: 3.3007, Test Percentage:33.4851%\n",
            "Epoch [338/1200], Training Loss: 1.3976, Validation Loss: 2.9717, Validation Percentage:35.7843%,Test Loss: 3.1589, Test Percentage:34.6235%\n",
            "Epoch [339/1200], Training Loss: 1.3418, Validation Loss: 2.9449, Validation Percentage:36.7647%,Test Loss: 3.2533, Test Percentage:34.0543%\n",
            "Epoch [340/1200], Training Loss: 1.4834, Validation Loss: 2.8933, Validation Percentage:37.8431%,Test Loss: 3.1531, Test Percentage:33.7779%\n",
            "Epoch [341/1200], Training Loss: 1.4437, Validation Loss: 2.9067, Validation Percentage:37.1569%,Test Loss: 3.1604, Test Percentage:33.2412%\n",
            "Epoch [342/1200], Training Loss: 1.4239, Validation Loss: 2.8275, Validation Percentage:35.8824%,Test Loss: 3.0965, Test Percentage:33.4851%\n",
            "Epoch [343/1200], Training Loss: 1.3939, Validation Loss: 2.8426, Validation Percentage:38.0392%,Test Loss: 3.1714, Test Percentage:35.1114%\n",
            "Epoch [344/1200], Training Loss: 1.3304, Validation Loss: 2.8614, Validation Percentage:39.3137%,Test Loss: 3.1777, Test Percentage:34.5259%\n",
            "Epoch [345/1200], Training Loss: 1.4561, Validation Loss: 3.0527, Validation Percentage:35.9804%,Test Loss: 3.2815, Test Percentage:34.6723%\n",
            "Epoch [346/1200], Training Loss: 1.3103, Validation Loss: 2.8334, Validation Percentage:37.4510%,Test Loss: 3.0971, Test Percentage:35.5180%\n",
            "Epoch [347/1200], Training Loss: 1.3025, Validation Loss: 2.9072, Validation Percentage:39.1176%,Test Loss: 3.2276, Test Percentage:35.0301%\n",
            "Epoch [348/1200], Training Loss: 1.4084, Validation Loss: 3.0281, Validation Percentage:38.0392%,Test Loss: 3.2738, Test Percentage:33.5664%\n",
            "Epoch [349/1200], Training Loss: 1.3163, Validation Loss: 2.8823, Validation Percentage:38.7255%,Test Loss: 3.1964, Test Percentage:34.1844%\n",
            "Epoch [350/1200], Training Loss: 1.3338, Validation Loss: 3.0927, Validation Percentage:37.6471%,Test Loss: 3.1747, Test Percentage:35.6481%\n",
            "Epoch [351/1200], Training Loss: 1.4403, Validation Loss: 2.9778, Validation Percentage:37.3529%,Test Loss: 3.2822, Test Percentage:34.2983%\n",
            "Epoch [352/1200], Training Loss: 1.3114, Validation Loss: 2.8979, Validation Percentage:38.7255%,Test Loss: 3.1495, Test Percentage:34.1519%\n",
            "Epoch [353/1200], Training Loss: 1.4518, Validation Loss: 3.0124, Validation Percentage:37.4510%,Test Loss: 3.2480, Test Percentage:34.6886%\n",
            "Epoch [354/1200], Training Loss: 1.4183, Validation Loss: 2.8228, Validation Percentage:38.6275%,Test Loss: 3.1132, Test Percentage:34.6560%\n",
            "Epoch [355/1200], Training Loss: 1.3870, Validation Loss: 2.9185, Validation Percentage:35.8824%,Test Loss: 3.1158, Test Percentage:34.5910%\n",
            "Epoch [356/1200], Training Loss: 1.3888, Validation Loss: 2.9651, Validation Percentage:36.7647%,Test Loss: 3.3542, Test Percentage:33.0135%\n",
            "Epoch [357/1200], Training Loss: 1.3911, Validation Loss: 2.9422, Validation Percentage:36.4706%,Test Loss: 3.1284, Test Percentage:34.9162%\n",
            "Epoch [358/1200], Training Loss: 1.4714, Validation Loss: 2.8660, Validation Percentage:37.3529%,Test Loss: 3.2233, Test Percentage:34.2332%\n",
            "Epoch [359/1200], Training Loss: 1.3536, Validation Loss: 2.8748, Validation Percentage:38.9216%,Test Loss: 3.2254, Test Percentage:34.8349%\n",
            "Epoch [360/1200], Training Loss: 1.4282, Validation Loss: 2.9098, Validation Percentage:36.8627%,Test Loss: 3.2018, Test Percentage:34.5097%\n",
            "Epoch [361/1200], Training Loss: 1.3987, Validation Loss: 2.8908, Validation Percentage:37.5490%,Test Loss: 3.2449, Test Percentage:34.1031%\n",
            "Epoch [362/1200], Training Loss: 1.3221, Validation Loss: 3.1816, Validation Percentage:37.0588%,Test Loss: 3.5674, Test Percentage:34.4934%\n",
            "Epoch [363/1200], Training Loss: 1.2787, Validation Loss: 3.0220, Validation Percentage:37.1569%,Test Loss: 3.2209, Test Percentage:35.3553%\n",
            "Epoch [364/1200], Training Loss: 1.3555, Validation Loss: 3.1791, Validation Percentage:36.6667%,Test Loss: 3.4690, Test Percentage:33.9893%\n",
            "Epoch [365/1200], Training Loss: 1.3838, Validation Loss: 2.9956, Validation Percentage:35.5882%,Test Loss: 3.1253, Test Percentage:36.1034%\n",
            "Epoch [366/1200], Training Loss: 1.3137, Validation Loss: 2.7966, Validation Percentage:38.0392%,Test Loss: 3.0296, Test Percentage:35.4692%\n",
            "Epoch [367/1200], Training Loss: 1.4258, Validation Loss: 2.8316, Validation Percentage:35.6863%,Test Loss: 3.0746, Test Percentage:35.4204%\n",
            "Epoch [368/1200], Training Loss: 1.3263, Validation Loss: 2.8274, Validation Percentage:37.6471%,Test Loss: 3.0897, Test Percentage:34.4934%\n",
            "Epoch [369/1200], Training Loss: 1.2942, Validation Loss: 3.0755, Validation Percentage:37.4510%,Test Loss: 3.2220, Test Percentage:35.2090%\n",
            "Epoch [370/1200], Training Loss: 1.3999, Validation Loss: 2.9176, Validation Percentage:39.1176%,Test Loss: 3.1807, Test Percentage:36.6076%\n",
            "Epoch [371/1200], Training Loss: 1.3497, Validation Loss: 2.9049, Validation Percentage:40.0980%,Test Loss: 3.1804, Test Percentage:36.1197%\n",
            "Epoch [372/1200], Training Loss: 1.3438, Validation Loss: 3.1220, Validation Percentage:38.6275%,Test Loss: 3.4358, Test Percentage:35.0463%\n",
            "Epoch [373/1200], Training Loss: 1.4096, Validation Loss: 2.8773, Validation Percentage:37.4510%,Test Loss: 3.1769, Test Percentage:35.5993%\n",
            "Epoch [374/1200], Training Loss: 1.3617, Validation Loss: 2.8178, Validation Percentage:39.3137%,Test Loss: 3.0946, Test Percentage:36.5263%\n",
            "Epoch [375/1200], Training Loss: 1.4214, Validation Loss: 2.9204, Validation Percentage:37.6471%,Test Loss: 3.1460, Test Percentage:36.5100%\n",
            "Epoch [376/1200], Training Loss: 1.3972, Validation Loss: 2.7994, Validation Percentage:39.4118%,Test Loss: 3.0502, Test Percentage:36.5263%\n",
            "Epoch [377/1200], Training Loss: 1.3200, Validation Loss: 2.8604, Validation Percentage:39.8039%,Test Loss: 3.1575, Test Percentage:35.7944%\n",
            "Epoch [378/1200], Training Loss: 1.2743, Validation Loss: 2.8460, Validation Percentage:39.3137%,Test Loss: 3.2229, Test Percentage:35.1114%\n",
            "Epoch [379/1200], Training Loss: 1.3451, Validation Loss: 2.9105, Validation Percentage:36.6667%,Test Loss: 3.2531, Test Percentage:35.3066%\n",
            "Epoch [380/1200], Training Loss: 1.3109, Validation Loss: 3.1013, Validation Percentage:38.8235%,Test Loss: 3.3940, Test Percentage:36.1360%\n",
            "Epoch [381/1200], Training Loss: 1.3493, Validation Loss: 2.9923, Validation Percentage:39.7059%,Test Loss: 3.2932, Test Percentage:35.2252%\n",
            "Epoch [382/1200], Training Loss: 1.3507, Validation Loss: 2.8674, Validation Percentage:39.4118%,Test Loss: 3.1284, Test Percentage:37.0955%\n",
            "Epoch [383/1200], Training Loss: 1.2906, Validation Loss: 2.8766, Validation Percentage:40.5882%,Test Loss: 3.2170, Test Percentage:34.4772%\n",
            "Epoch [384/1200], Training Loss: 1.3348, Validation Loss: 2.8356, Validation Percentage:40.0000%,Test Loss: 3.1470, Test Percentage:35.8920%\n",
            "Epoch [385/1200], Training Loss: 1.4789, Validation Loss: 2.8129, Validation Percentage:37.4510%,Test Loss: 3.0620, Test Percentage:35.7619%\n",
            "Epoch [386/1200], Training Loss: 1.3774, Validation Loss: 2.9329, Validation Percentage:38.6275%,Test Loss: 3.2256, Test Percentage:35.6969%\n",
            "Epoch [387/1200], Training Loss: 1.3474, Validation Loss: 2.9151, Validation Percentage:38.4314%,Test Loss: 3.1259, Test Percentage:36.2823%\n",
            "Epoch [388/1200], Training Loss: 1.3247, Validation Loss: 2.7586, Validation Percentage:41.9608%,Test Loss: 3.0439, Test Percentage:36.9654%\n",
            "Epoch [389/1200], Training Loss: 1.3482, Validation Loss: 2.9996, Validation Percentage:37.4510%,Test Loss: 3.2645, Test Percentage:36.2823%\n",
            "Epoch [390/1200], Training Loss: 1.1875, Validation Loss: 3.1366, Validation Percentage:39.8039%,Test Loss: 3.3902, Test Percentage:37.5508%\n",
            "Epoch [391/1200], Training Loss: 1.3605, Validation Loss: 2.9180, Validation Percentage:37.6471%,Test Loss: 3.0777, Test Percentage:36.8027%\n",
            "Epoch [392/1200], Training Loss: 1.3497, Validation Loss: 2.8203, Validation Percentage:38.3333%,Test Loss: 3.1207, Test Percentage:34.0381%\n",
            "Epoch [393/1200], Training Loss: 1.3540, Validation Loss: 2.8017, Validation Percentage:39.1176%,Test Loss: 3.0452, Test Percentage:36.5263%\n",
            "Epoch [394/1200], Training Loss: 1.2531, Validation Loss: 2.9083, Validation Percentage:39.0196%,Test Loss: 3.1684, Test Percentage:37.1117%\n",
            "Epoch [395/1200], Training Loss: 1.2166, Validation Loss: 2.9034, Validation Percentage:40.0980%,Test Loss: 3.2803, Test Percentage:34.8187%\n",
            "Epoch [396/1200], Training Loss: 1.2225, Validation Loss: 2.9027, Validation Percentage:41.3725%,Test Loss: 3.2183, Test Percentage:37.9411%\n",
            "Epoch [397/1200], Training Loss: 1.2400, Validation Loss: 2.8940, Validation Percentage:39.1176%,Test Loss: 3.1696, Test Percentage:37.5508%\n",
            "Epoch [398/1200], Training Loss: 1.2800, Validation Loss: 2.9454, Validation Percentage:36.8627%,Test Loss: 3.2331, Test Percentage:34.7536%\n",
            "Epoch [399/1200], Training Loss: 1.3880, Validation Loss: 2.9540, Validation Percentage:39.4118%,Test Loss: 3.2060, Test Percentage:35.5342%\n",
            "Epoch [400/1200], Training Loss: 1.3536, Validation Loss: 2.7073, Validation Percentage:42.1569%,Test Loss: 3.0761, Test Percentage:35.3716%\n",
            "Epoch [401/1200], Training Loss: 1.3987, Validation Loss: 2.8033, Validation Percentage:39.6078%,Test Loss: 3.1014, Test Percentage:35.3066%\n",
            "Epoch [402/1200], Training Loss: 1.4209, Validation Loss: 2.8426, Validation Percentage:39.0196%,Test Loss: 3.0610, Test Percentage:34.5910%\n",
            "Epoch [403/1200], Training Loss: 1.3333, Validation Loss: 2.9948, Validation Percentage:37.8431%,Test Loss: 3.2191, Test Percentage:35.5668%\n",
            "Epoch [404/1200], Training Loss: 1.3449, Validation Loss: 2.9059, Validation Percentage:40.0000%,Test Loss: 3.0748, Test Percentage:38.2826%\n",
            "Epoch [405/1200], Training Loss: 1.3045, Validation Loss: 2.8044, Validation Percentage:40.3922%,Test Loss: 3.1277, Test Percentage:35.0951%\n",
            "Epoch [406/1200], Training Loss: 1.3164, Validation Loss: 2.8238, Validation Percentage:38.9216%,Test Loss: 3.1607, Test Percentage:35.1927%\n",
            "Epoch [407/1200], Training Loss: 1.3583, Validation Loss: 2.8933, Validation Percentage:38.3333%,Test Loss: 3.1816, Test Percentage:35.2415%\n",
            "Epoch [408/1200], Training Loss: 1.3048, Validation Loss: 2.6146, Validation Percentage:40.4902%,Test Loss: 2.9502, Test Percentage:37.4695%\n",
            "Epoch [409/1200], Training Loss: 1.2610, Validation Loss: 2.9118, Validation Percentage:36.7647%,Test Loss: 3.2165, Test Percentage:34.2983%\n",
            "Epoch [410/1200], Training Loss: 1.2847, Validation Loss: 2.8690, Validation Percentage:40.3922%,Test Loss: 3.2546, Test Percentage:35.8432%\n",
            "Epoch [411/1200], Training Loss: 1.3610, Validation Loss: 2.7839, Validation Percentage:38.9216%,Test Loss: 3.0495, Test Percentage:37.4370%\n",
            "Epoch [412/1200], Training Loss: 1.2993, Validation Loss: 2.9805, Validation Percentage:37.4510%,Test Loss: 3.2185, Test Percentage:36.0059%\n",
            "Epoch [413/1200], Training Loss: 1.2892, Validation Loss: 2.9957, Validation Percentage:40.8824%,Test Loss: 3.3533, Test Percentage:35.8270%\n",
            "Epoch [414/1200], Training Loss: 1.3125, Validation Loss: 3.0878, Validation Percentage:39.6078%,Test Loss: 3.3141, Test Percentage:36.0059%\n",
            "Epoch [415/1200], Training Loss: 1.3051, Validation Loss: 2.7910, Validation Percentage:40.9804%,Test Loss: 3.2757, Test Percentage:36.6889%\n",
            "Epoch [416/1200], Training Loss: 1.2130, Validation Loss: 2.8017, Validation Percentage:40.3922%,Test Loss: 3.1303, Test Percentage:37.7134%\n",
            "Epoch [417/1200], Training Loss: 1.2562, Validation Loss: 2.9186, Validation Percentage:39.9020%,Test Loss: 3.2702, Test Percentage:38.2501%\n",
            "Epoch [418/1200], Training Loss: 1.2724, Validation Loss: 2.9429, Validation Percentage:40.8824%,Test Loss: 3.3256, Test Percentage:36.6564%\n",
            "Epoch [419/1200], Training Loss: 1.2340, Validation Loss: 2.8733, Validation Percentage:37.8431%,Test Loss: 3.1243, Test Percentage:37.2093%\n",
            "Epoch [420/1200], Training Loss: 1.2127, Validation Loss: 2.8679, Validation Percentage:38.7255%,Test Loss: 3.1434, Test Percentage:37.0304%\n",
            "Epoch [421/1200], Training Loss: 1.3189, Validation Loss: 2.9174, Validation Percentage:38.5294%,Test Loss: 3.1001, Test Percentage:35.2740%\n",
            "Epoch [422/1200], Training Loss: 1.2771, Validation Loss: 2.9003, Validation Percentage:39.8039%,Test Loss: 3.1692, Test Percentage:35.5342%\n",
            "Epoch [423/1200], Training Loss: 1.2358, Validation Loss: 2.9102, Validation Percentage:36.4706%,Test Loss: 3.2080, Test Percentage:35.3228%\n",
            "Epoch [424/1200], Training Loss: 1.3444, Validation Loss: 2.7923, Validation Percentage:40.0000%,Test Loss: 3.0889, Test Percentage:36.2010%\n",
            "Epoch [425/1200], Training Loss: 1.2363, Validation Loss: 2.9701, Validation Percentage:39.1176%,Test Loss: 3.2474, Test Percentage:35.9083%\n",
            "Epoch [426/1200], Training Loss: 1.3537, Validation Loss: 3.1619, Validation Percentage:39.4118%,Test Loss: 3.4200, Test Percentage:36.2173%\n",
            "Epoch [427/1200], Training Loss: 1.1833, Validation Loss: 2.8814, Validation Percentage:42.0588%,Test Loss: 3.2126, Test Percentage:36.7539%\n",
            "Epoch [428/1200], Training Loss: 1.2000, Validation Loss: 2.8879, Validation Percentage:38.8235%,Test Loss: 3.1278, Test Percentage:37.8923%\n",
            "Epoch [429/1200], Training Loss: 1.1238, Validation Loss: 2.9963, Validation Percentage:38.2353%,Test Loss: 3.1947, Test Percentage:35.6806%\n",
            "Epoch [430/1200], Training Loss: 1.3619, Validation Loss: 2.9326, Validation Percentage:39.5098%,Test Loss: 3.1043, Test Percentage:37.8110%\n",
            "Epoch [431/1200], Training Loss: 1.1898, Validation Loss: 2.9749, Validation Percentage:38.1373%,Test Loss: 3.2485, Test Percentage:36.9979%\n",
            "Epoch [432/1200], Training Loss: 1.2292, Validation Loss: 3.1010, Validation Percentage:38.9216%,Test Loss: 3.3968, Test Percentage:35.7131%\n",
            "Epoch [433/1200], Training Loss: 1.1938, Validation Loss: 2.8067, Validation Percentage:40.2941%,Test Loss: 3.2420, Test Percentage:36.2173%\n",
            "Epoch [434/1200], Training Loss: 1.2557, Validation Loss: 3.2003, Validation Percentage:38.8235%,Test Loss: 3.4116, Test Percentage:37.0304%\n",
            "Epoch [435/1200], Training Loss: 1.2571, Validation Loss: 2.8375, Validation Percentage:38.0392%,Test Loss: 3.1952, Test Percentage:35.3879%\n",
            "Epoch [436/1200], Training Loss: 1.2553, Validation Loss: 2.9964, Validation Percentage:39.0196%,Test Loss: 3.3522, Test Percentage:35.0951%\n",
            "Epoch [437/1200], Training Loss: 1.2534, Validation Loss: 2.9889, Validation Percentage:38.4314%,Test Loss: 3.3033, Test Percentage:37.6484%\n",
            "Epoch [438/1200], Training Loss: 1.2787, Validation Loss: 2.9288, Validation Percentage:37.8431%,Test Loss: 3.2183, Test Percentage:37.2093%\n",
            "Epoch [439/1200], Training Loss: 1.2197, Validation Loss: 2.9042, Validation Percentage:40.3922%,Test Loss: 3.2385, Test Percentage:36.0059%\n",
            "Epoch [440/1200], Training Loss: 1.2396, Validation Loss: 2.9934, Validation Percentage:40.4902%,Test Loss: 3.3143, Test Percentage:36.6889%\n",
            "Epoch [441/1200], Training Loss: 1.2104, Validation Loss: 2.7975, Validation Percentage:39.2157%,Test Loss: 3.0797, Test Percentage:37.5508%\n",
            "Epoch [442/1200], Training Loss: 1.1900, Validation Loss: 2.9612, Validation Percentage:37.9412%,Test Loss: 3.2486, Test Percentage:37.6647%\n",
            "Epoch [443/1200], Training Loss: 1.2765, Validation Loss: 2.9186, Validation Percentage:35.9804%,Test Loss: 3.1600, Test Percentage:36.0384%\n",
            "Epoch [444/1200], Training Loss: 1.2498, Validation Loss: 2.8702, Validation Percentage:39.7059%,Test Loss: 3.1546, Test Percentage:36.7052%\n",
            "Epoch [445/1200], Training Loss: 1.2580, Validation Loss: 3.1181, Validation Percentage:41.2745%,Test Loss: 3.3533, Test Percentage:37.5996%\n",
            "Epoch [446/1200], Training Loss: 1.2287, Validation Loss: 2.9096, Validation Percentage:39.7059%,Test Loss: 3.1654, Test Percentage:36.5751%\n",
            "Epoch [447/1200], Training Loss: 1.1837, Validation Loss: 2.8444, Validation Percentage:42.4510%,Test Loss: 3.2587, Test Percentage:36.5263%\n",
            "Epoch [448/1200], Training Loss: 1.2416, Validation Loss: 2.9759, Validation Percentage:39.3137%,Test Loss: 3.2218, Test Percentage:37.2418%\n",
            "Epoch [449/1200], Training Loss: 1.1561, Validation Loss: 3.1154, Validation Percentage:40.0980%,Test Loss: 3.2673, Test Percentage:38.3314%\n",
            "Epoch [450/1200], Training Loss: 1.2273, Validation Loss: 2.9133, Validation Percentage:40.2941%,Test Loss: 3.1746, Test Percentage:37.4370%\n",
            "Epoch [451/1200], Training Loss: 1.2523, Validation Loss: 2.9116, Validation Percentage:39.4118%,Test Loss: 3.0663, Test Percentage:38.1688%\n",
            "Epoch [452/1200], Training Loss: 1.3016, Validation Loss: 2.8665, Validation Percentage:40.2941%,Test Loss: 3.1640, Test Percentage:36.9816%\n",
            "Epoch [453/1200], Training Loss: 1.1605, Validation Loss: 2.8988, Validation Percentage:40.3922%,Test Loss: 3.1436, Test Percentage:35.1277%\n",
            "Epoch [454/1200], Training Loss: 1.2795, Validation Loss: 2.9501, Validation Percentage:39.9020%,Test Loss: 3.1311, Test Percentage:36.5588%\n",
            "Epoch [455/1200], Training Loss: 1.2023, Validation Loss: 3.0263, Validation Percentage:40.8824%,Test Loss: 3.3624, Test Percentage:37.2418%\n",
            "Epoch [456/1200], Training Loss: 1.1858, Validation Loss: 3.0592, Validation Percentage:37.0588%,Test Loss: 3.2231, Test Percentage:36.9491%\n",
            "Epoch [457/1200], Training Loss: 1.3341, Validation Loss: 3.1559, Validation Percentage:38.3333%,Test Loss: 3.4408, Test Percentage:35.8432%\n",
            "Epoch [458/1200], Training Loss: 1.2680, Validation Loss: 2.9493, Validation Percentage:40.2941%,Test Loss: 3.2290, Test Percentage:37.2418%\n",
            "Epoch [459/1200], Training Loss: 1.2387, Validation Loss: 2.7819, Validation Percentage:40.0000%,Test Loss: 3.0658, Test Percentage:37.4207%\n",
            "Epoch [460/1200], Training Loss: 1.1700, Validation Loss: 3.0773, Validation Percentage:39.8039%,Test Loss: 3.3131, Test Percentage:37.5508%\n",
            "Epoch [461/1200], Training Loss: 1.2637, Validation Loss: 2.7992, Validation Percentage:41.5686%,Test Loss: 3.1676, Test Percentage:35.9733%\n",
            "Epoch [462/1200], Training Loss: 1.1472, Validation Loss: 2.9595, Validation Percentage:41.3725%,Test Loss: 3.3497, Test Percentage:38.1851%\n",
            "Epoch [463/1200], Training Loss: 1.2127, Validation Loss: 2.9094, Validation Percentage:41.5686%,Test Loss: 3.2128, Test Percentage:37.6484%\n",
            "Epoch [464/1200], Training Loss: 1.2027, Validation Loss: 2.8401, Validation Percentage:40.8824%,Test Loss: 3.1890, Test Percentage:36.3799%\n",
            "Epoch [465/1200], Training Loss: 1.1451, Validation Loss: 3.0657, Validation Percentage:38.2353%,Test Loss: 3.3604, Test Percentage:37.9411%\n",
            "Epoch [466/1200], Training Loss: 1.1976, Validation Loss: 2.8458, Validation Percentage:40.0980%,Test Loss: 3.1413, Test Percentage:36.5913%\n",
            "Epoch [467/1200], Training Loss: 1.1864, Validation Loss: 2.7738, Validation Percentage:40.9804%,Test Loss: 3.2114, Test Percentage:35.5830%\n",
            "Epoch [468/1200], Training Loss: 1.1915, Validation Loss: 2.9216, Validation Percentage:40.3922%,Test Loss: 3.1608, Test Percentage:37.1117%\n",
            "Epoch [469/1200], Training Loss: 1.2751, Validation Loss: 2.9279, Validation Percentage:41.8627%,Test Loss: 3.3396, Test Percentage:35.7294%\n",
            "Epoch [470/1200], Training Loss: 1.1185, Validation Loss: 2.8240, Validation Percentage:41.2745%,Test Loss: 3.1421, Test Percentage:37.3394%\n",
            "Epoch [471/1200], Training Loss: 1.2260, Validation Loss: 2.7391, Validation Percentage:41.8627%,Test Loss: 3.0863, Test Percentage:37.8436%\n",
            "Epoch [472/1200], Training Loss: 1.3264, Validation Loss: 2.8894, Validation Percentage:40.7843%,Test Loss: 3.2177, Test Percentage:35.6643%\n",
            "Epoch [473/1200], Training Loss: 1.2290, Validation Loss: 2.9283, Validation Percentage:40.9804%,Test Loss: 3.3216, Test Percentage:35.2252%\n",
            "Epoch [474/1200], Training Loss: 1.1784, Validation Loss: 2.9621, Validation Percentage:39.8039%,Test Loss: 3.2920, Test Percentage:37.1605%\n",
            "Epoch [475/1200], Training Loss: 1.2530, Validation Loss: 2.8945, Validation Percentage:40.4902%,Test Loss: 3.2769, Test Percentage:37.1443%\n",
            "Epoch [476/1200], Training Loss: 1.1627, Validation Loss: 2.8534, Validation Percentage:39.5098%,Test Loss: 3.2669, Test Percentage:36.2173%\n",
            "Epoch [477/1200], Training Loss: 1.1142, Validation Loss: 3.0744, Validation Percentage:40.0980%,Test Loss: 3.3884, Test Percentage:37.3882%\n",
            "Epoch [478/1200], Training Loss: 1.1611, Validation Loss: 3.2052, Validation Percentage:38.4314%,Test Loss: 3.5655, Test Percentage:34.8512%\n",
            "Epoch [479/1200], Training Loss: 1.2255, Validation Loss: 2.9176, Validation Percentage:40.1961%,Test Loss: 3.2981, Test Percentage:36.9491%\n",
            "Epoch [480/1200], Training Loss: 1.1365, Validation Loss: 2.8039, Validation Percentage:42.8431%,Test Loss: 3.1039, Test Percentage:37.9737%\n",
            "Epoch [481/1200], Training Loss: 1.1060, Validation Loss: 3.0032, Validation Percentage:37.9412%,Test Loss: 3.3131, Test Percentage:35.9896%\n",
            "Epoch [482/1200], Training Loss: 1.1928, Validation Loss: 2.9008, Validation Percentage:40.4902%,Test Loss: 3.2304, Test Percentage:35.9733%\n",
            "Epoch [483/1200], Training Loss: 1.1388, Validation Loss: 2.8848, Validation Percentage:41.3725%,Test Loss: 3.2177, Test Percentage:38.7868%\n",
            "Epoch [484/1200], Training Loss: 1.0767, Validation Loss: 2.9313, Validation Percentage:40.9804%,Test Loss: 3.2550, Test Percentage:38.0712%\n",
            "Epoch [485/1200], Training Loss: 1.1312, Validation Loss: 3.0728, Validation Percentage:41.3725%,Test Loss: 3.2684, Test Percentage:38.1200%\n",
            "Epoch [486/1200], Training Loss: 1.1573, Validation Loss: 2.9745, Validation Percentage:41.3725%,Test Loss: 3.2800, Test Percentage:37.5508%\n",
            "Epoch [487/1200], Training Loss: 1.1092, Validation Loss: 2.8650, Validation Percentage:39.7059%,Test Loss: 3.2110, Test Percentage:37.7297%\n",
            "Epoch [488/1200], Training Loss: 1.2063, Validation Loss: 2.7880, Validation Percentage:42.3529%,Test Loss: 3.0177, Test Percentage:37.9411%\n",
            "Epoch [489/1200], Training Loss: 1.1182, Validation Loss: 3.0148, Validation Percentage:40.7843%,Test Loss: 3.3546, Test Percentage:37.8761%\n",
            "Epoch [490/1200], Training Loss: 1.2878, Validation Loss: 2.9119, Validation Percentage:39.9020%,Test Loss: 3.1088, Test Percentage:37.6809%\n",
            "Epoch [491/1200], Training Loss: 1.1459, Validation Loss: 2.9111, Validation Percentage:40.8824%,Test Loss: 3.2575, Test Percentage:37.9086%\n",
            "Epoch [492/1200], Training Loss: 1.2759, Validation Loss: 2.9102, Validation Percentage:38.6275%,Test Loss: 3.3211, Test Percentage:36.2335%\n",
            "Epoch [493/1200], Training Loss: 1.2890, Validation Loss: 2.8801, Validation Percentage:39.7059%,Test Loss: 3.2531, Test Percentage:36.6076%\n",
            "Epoch [494/1200], Training Loss: 1.2860, Validation Loss: 2.9551, Validation Percentage:36.6667%,Test Loss: 3.0835, Test Percentage:36.6076%\n",
            "Epoch [495/1200], Training Loss: 1.1866, Validation Loss: 3.1394, Validation Percentage:35.9804%,Test Loss: 3.4921, Test Percentage:34.6886%\n",
            "Epoch [496/1200], Training Loss: 1.1456, Validation Loss: 2.8996, Validation Percentage:38.3333%,Test Loss: 3.1409, Test Percentage:36.5751%\n",
            "Epoch [497/1200], Training Loss: 1.2249, Validation Loss: 2.9727, Validation Percentage:38.1373%,Test Loss: 3.2455, Test Percentage:38.0224%\n",
            "Epoch [498/1200], Training Loss: 1.2002, Validation Loss: 2.7725, Validation Percentage:43.3333%,Test Loss: 3.2082, Test Percentage:38.0875%\n",
            "Epoch [499/1200], Training Loss: 1.1776, Validation Loss: 2.9075, Validation Percentage:42.7451%,Test Loss: 3.2643, Test Percentage:37.7622%\n",
            "Epoch [500/1200], Training Loss: 1.2266, Validation Loss: 2.9141, Validation Percentage:38.5294%,Test Loss: 3.1362, Test Percentage:37.9086%\n",
            "Epoch [501/1200], Training Loss: 1.1637, Validation Loss: 3.0721, Validation Percentage:39.6078%,Test Loss: 3.2942, Test Percentage:36.9491%\n",
            "Epoch [502/1200], Training Loss: 1.1040, Validation Loss: 2.9728, Validation Percentage:40.2941%,Test Loss: 3.1717, Test Percentage:38.4128%\n",
            "Epoch [503/1200], Training Loss: 1.2013, Validation Loss: 2.9099, Validation Percentage:39.8039%,Test Loss: 3.2028, Test Percentage:38.4778%\n",
            "Epoch [504/1200], Training Loss: 1.1426, Validation Loss: 2.9559, Validation Percentage:41.7647%,Test Loss: 3.3790, Test Percentage:37.6159%\n",
            "Epoch [505/1200], Training Loss: 1.1965, Validation Loss: 2.7268, Validation Percentage:40.3922%,Test Loss: 2.9667, Test Percentage:37.2906%\n",
            "Epoch [506/1200], Training Loss: 1.1261, Validation Loss: 2.8033, Validation Percentage:40.8824%,Test Loss: 3.2168, Test Percentage:37.6321%\n",
            "Epoch [507/1200], Training Loss: 1.0519, Validation Loss: 2.9684, Validation Percentage:40.7843%,Test Loss: 3.2785, Test Percentage:38.5429%\n",
            "Epoch [508/1200], Training Loss: 1.1221, Validation Loss: 3.0997, Validation Percentage:40.2941%,Test Loss: 3.3445, Test Percentage:37.2418%\n",
            "Epoch [509/1200], Training Loss: 1.1221, Validation Loss: 3.0585, Validation Percentage:41.5686%,Test Loss: 3.3839, Test Percentage:37.6647%\n",
            "Epoch [510/1200], Training Loss: 1.0921, Validation Loss: 2.9659, Validation Percentage:39.9020%,Test Loss: 3.3220, Test Percentage:37.5508%\n",
            "Epoch [511/1200], Training Loss: 1.1045, Validation Loss: 2.8525, Validation Percentage:41.0784%,Test Loss: 3.1127, Test Percentage:39.3885%\n",
            "Epoch [512/1200], Training Loss: 1.1522, Validation Loss: 2.8364, Validation Percentage:40.1961%,Test Loss: 3.0991, Test Percentage:38.5916%\n",
            "Epoch [513/1200], Training Loss: 1.0537, Validation Loss: 2.9230, Validation Percentage:40.2941%,Test Loss: 3.2441, Test Percentage:38.6892%\n",
            "Epoch [514/1200], Training Loss: 1.0813, Validation Loss: 2.9500, Validation Percentage:42.5490%,Test Loss: 3.2196, Test Percentage:38.6892%\n",
            "Epoch [515/1200], Training Loss: 1.1698, Validation Loss: 2.9942, Validation Percentage:39.1176%,Test Loss: 3.2163, Test Percentage:36.9328%\n",
            "Epoch [516/1200], Training Loss: 1.1117, Validation Loss: 3.1048, Validation Percentage:40.2941%,Test Loss: 3.3863, Test Percentage:38.0550%\n",
            "Epoch [517/1200], Training Loss: 1.1411, Validation Loss: 2.8765, Validation Percentage:40.8824%,Test Loss: 3.1709, Test Percentage:36.9003%\n",
            "Epoch [518/1200], Training Loss: 1.0944, Validation Loss: 2.8637, Validation Percentage:43.2353%,Test Loss: 3.3710, Test Percentage:38.3802%\n",
            "Epoch [519/1200], Training Loss: 1.1425, Validation Loss: 3.0892, Validation Percentage:42.1569%,Test Loss: 3.2912, Test Percentage:37.9737%\n",
            "Epoch [520/1200], Training Loss: 1.1166, Validation Loss: 2.8437, Validation Percentage:40.0980%,Test Loss: 3.0701, Test Percentage:39.0470%\n",
            "Epoch [521/1200], Training Loss: 1.0553, Validation Loss: 2.8677, Validation Percentage:40.6863%,Test Loss: 3.0739, Test Percentage:38.9982%\n",
            "Epoch [522/1200], Training Loss: 1.0087, Validation Loss: 3.0267, Validation Percentage:41.0784%,Test Loss: 3.2514, Test Percentage:37.3231%\n",
            "Epoch [523/1200], Training Loss: 1.1498, Validation Loss: 2.9553, Validation Percentage:40.5882%,Test Loss: 3.2279, Test Percentage:37.8110%\n",
            "Epoch [524/1200], Training Loss: 1.1209, Validation Loss: 2.9502, Validation Percentage:38.8235%,Test Loss: 3.0658, Test Percentage:38.1525%\n",
            "Epoch [525/1200], Training Loss: 1.0995, Validation Loss: 2.7818, Validation Percentage:42.0588%,Test Loss: 3.2741, Test Percentage:37.1768%\n",
            "Epoch [526/1200], Training Loss: 1.0068, Validation Loss: 2.8950, Validation Percentage:40.8824%,Test Loss: 3.1166, Test Percentage:36.8027%\n",
            "Epoch [527/1200], Training Loss: 1.2129, Validation Loss: 3.0774, Validation Percentage:39.3137%,Test Loss: 3.5318, Test Percentage:35.6969%\n",
            "Epoch [528/1200], Training Loss: 1.0073, Validation Loss: 3.1680, Validation Percentage:38.7255%,Test Loss: 3.3756, Test Percentage:36.8353%\n",
            "Epoch [529/1200], Training Loss: 1.1888, Validation Loss: 2.7430, Validation Percentage:40.1961%,Test Loss: 2.9511, Test Percentage:38.7380%\n",
            "Epoch [530/1200], Training Loss: 1.1987, Validation Loss: 2.8343, Validation Percentage:40.2941%,Test Loss: 3.1071, Test Percentage:38.5754%\n",
            "Epoch [531/1200], Training Loss: 1.1161, Validation Loss: 2.8176, Validation Percentage:40.9804%,Test Loss: 3.1090, Test Percentage:38.2664%\n",
            "Epoch [532/1200], Training Loss: 1.0620, Validation Loss: 2.9372, Validation Percentage:40.6863%,Test Loss: 3.1931, Test Percentage:38.9494%\n",
            "Epoch [533/1200], Training Loss: 1.0876, Validation Loss: 3.2279, Validation Percentage:40.4902%,Test Loss: 3.5602, Test Percentage:36.7702%\n",
            "Epoch [534/1200], Training Loss: 1.0268, Validation Loss: 3.2590, Validation Percentage:39.4118%,Test Loss: 3.5535, Test Percentage:38.1525%\n",
            "Epoch [535/1200], Training Loss: 1.0889, Validation Loss: 2.9286, Validation Percentage:41.3725%,Test Loss: 3.2286, Test Percentage:36.9166%\n",
            "Epoch [536/1200], Training Loss: 1.1502, Validation Loss: 3.0298, Validation Percentage:40.5882%,Test Loss: 3.2238, Test Percentage:37.7622%\n",
            "Epoch [537/1200], Training Loss: 1.1385, Validation Loss: 2.8657, Validation Percentage:40.1961%,Test Loss: 3.1883, Test Percentage:37.6159%\n",
            "Epoch [538/1200], Training Loss: 1.0953, Validation Loss: 2.7977, Validation Percentage:42.3529%,Test Loss: 3.0982, Test Percentage:39.4210%\n",
            "Epoch [539/1200], Training Loss: 1.1447, Validation Loss: 2.8078, Validation Percentage:42.7451%,Test Loss: 3.0897, Test Percentage:38.6730%\n",
            "Epoch [540/1200], Training Loss: 1.0707, Validation Loss: 2.9517, Validation Percentage:41.1765%,Test Loss: 3.3504, Test Percentage:39.1121%\n",
            "Epoch [541/1200], Training Loss: 1.1485, Validation Loss: 2.9940, Validation Percentage:39.1176%,Test Loss: 3.2954, Test Percentage:36.7214%\n",
            "Epoch [542/1200], Training Loss: 1.1533, Validation Loss: 3.0280, Validation Percentage:39.8039%,Test Loss: 3.1356, Test Percentage:37.0792%\n",
            "Epoch [543/1200], Training Loss: 1.1886, Validation Loss: 2.9646, Validation Percentage:39.3137%,Test Loss: 3.3033, Test Percentage:36.7702%\n",
            "Epoch [544/1200], Training Loss: 1.1270, Validation Loss: 2.8254, Validation Percentage:41.4706%,Test Loss: 3.1921, Test Percentage:38.2989%\n",
            "Epoch [545/1200], Training Loss: 1.2209, Validation Loss: 2.8640, Validation Percentage:39.0196%,Test Loss: 3.0539, Test Percentage:38.1363%\n",
            "Epoch [546/1200], Training Loss: 1.1986, Validation Loss: 2.8950, Validation Percentage:41.1765%,Test Loss: 3.0434, Test Percentage:38.4290%\n",
            "Epoch [547/1200], Training Loss: 1.1180, Validation Loss: 2.9869, Validation Percentage:41.5686%,Test Loss: 3.2940, Test Percentage:38.2176%\n",
            "Epoch [548/1200], Training Loss: 1.0908, Validation Loss: 3.0277, Validation Percentage:40.3922%,Test Loss: 3.2060, Test Percentage:37.9574%\n",
            "Epoch [549/1200], Training Loss: 1.0467, Validation Loss: 2.9380, Validation Percentage:42.2549%,Test Loss: 3.1401, Test Percentage:38.6404%\n",
            "Epoch [550/1200], Training Loss: 1.0365, Validation Loss: 2.8436, Validation Percentage:41.4706%,Test Loss: 3.1274, Test Percentage:38.5754%\n",
            "Epoch [551/1200], Training Loss: 1.2007, Validation Loss: 2.7756, Validation Percentage:42.4510%,Test Loss: 3.0837, Test Percentage:39.7626%\n",
            "Epoch [552/1200], Training Loss: 1.1284, Validation Loss: 2.8063, Validation Percentage:42.9412%,Test Loss: 3.1222, Test Percentage:38.4128%\n",
            "Epoch [553/1200], Training Loss: 1.1028, Validation Loss: 2.7715, Validation Percentage:41.1765%,Test Loss: 3.1084, Test Percentage:37.1443%\n",
            "Epoch [554/1200], Training Loss: 1.0586, Validation Loss: 2.9257, Validation Percentage:42.2549%,Test Loss: 3.2275, Test Percentage:39.3560%\n",
            "Epoch [555/1200], Training Loss: 1.0315, Validation Loss: 2.7301, Validation Percentage:41.2745%,Test Loss: 3.0367, Test Percentage:38.4128%\n",
            "Epoch [556/1200], Training Loss: 1.1267, Validation Loss: 2.8896, Validation Percentage:40.6863%,Test Loss: 3.0687, Test Percentage:38.8031%\n",
            "Epoch [557/1200], Training Loss: 1.0779, Validation Loss: 2.9200, Validation Percentage:42.1569%,Test Loss: 3.2259, Test Percentage:38.4453%\n",
            "Epoch [558/1200], Training Loss: 1.0284, Validation Loss: 2.9643, Validation Percentage:41.9608%,Test Loss: 3.2857, Test Percentage:38.8681%\n",
            "Epoch [559/1200], Training Loss: 1.1124, Validation Loss: 2.9073, Validation Percentage:41.7647%,Test Loss: 3.1311, Test Percentage:39.3723%\n",
            "Epoch [560/1200], Training Loss: 1.0687, Validation Loss: 2.7997, Validation Percentage:40.5882%,Test Loss: 3.0842, Test Percentage:38.5916%\n",
            "Epoch [561/1200], Training Loss: 1.0145, Validation Loss: 2.9284, Validation Percentage:41.0784%,Test Loss: 3.1710, Test Percentage:39.1446%\n",
            "Epoch [562/1200], Training Loss: 1.0915, Validation Loss: 2.8586, Validation Percentage:40.0980%,Test Loss: 3.3442, Test Percentage:38.0062%\n",
            "Epoch [563/1200], Training Loss: 1.1891, Validation Loss: 3.0435, Validation Percentage:40.9804%,Test Loss: 3.2094, Test Percentage:38.5916%\n",
            "Epoch [564/1200], Training Loss: 1.2218, Validation Loss: 3.0156, Validation Percentage:41.2745%,Test Loss: 3.3946, Test Percentage:38.3477%\n",
            "Epoch [565/1200], Training Loss: 1.1303, Validation Loss: 2.9437, Validation Percentage:42.1569%,Test Loss: 3.2552, Test Percentage:36.9166%\n",
            "Epoch [566/1200], Training Loss: 1.0083, Validation Loss: 2.8991, Validation Percentage:42.8431%,Test Loss: 3.4221, Test Percentage:38.0550%\n",
            "Epoch [567/1200], Training Loss: 1.0528, Validation Loss: 3.0153, Validation Percentage:39.8039%,Test Loss: 3.2472, Test Percentage:37.9086%\n",
            "Epoch [568/1200], Training Loss: 1.0930, Validation Loss: 3.1172, Validation Percentage:37.7451%,Test Loss: 3.3645, Test Percentage:38.0550%\n",
            "Epoch [569/1200], Training Loss: 1.1697, Validation Loss: 2.9353, Validation Percentage:40.8824%,Test Loss: 3.0908, Test Percentage:38.7543%\n",
            "Epoch [570/1200], Training Loss: 1.2084, Validation Loss: 2.7776, Validation Percentage:43.9216%,Test Loss: 3.2405, Test Percentage:38.8681%\n",
            "Epoch [571/1200], Training Loss: 1.0887, Validation Loss: 2.9937, Validation Percentage:41.7647%,Test Loss: 3.2707, Test Percentage:38.9332%\n",
            "Epoch [572/1200], Training Loss: 1.1265, Validation Loss: 2.8751, Validation Percentage:42.5490%,Test Loss: 3.1499, Test Percentage:39.3397%\n",
            "Epoch [573/1200], Training Loss: 1.1004, Validation Loss: 2.8667, Validation Percentage:43.2353%,Test Loss: 3.1146, Test Percentage:39.9740%\n",
            "Epoch [574/1200], Training Loss: 1.0568, Validation Loss: 2.8330, Validation Percentage:40.4902%,Test Loss: 3.1910, Test Percentage:38.5103%\n",
            "Epoch [575/1200], Training Loss: 1.0306, Validation Loss: 2.9351, Validation Percentage:41.3725%,Test Loss: 3.0480, Test Percentage:40.4131%\n",
            "Epoch [576/1200], Training Loss: 1.0198, Validation Loss: 2.8721, Validation Percentage:42.0588%,Test Loss: 3.2021, Test Percentage:40.1854%\n",
            "Epoch [577/1200], Training Loss: 1.0688, Validation Loss: 2.9267, Validation Percentage:43.7255%,Test Loss: 3.2802, Test Percentage:39.2259%\n",
            "Epoch [578/1200], Training Loss: 1.1180, Validation Loss: 3.0059, Validation Percentage:40.8824%,Test Loss: 3.4517, Test Percentage:37.1605%\n",
            "Epoch [579/1200], Training Loss: 1.1210, Validation Loss: 2.9149, Validation Percentage:39.5098%,Test Loss: 3.2409, Test Percentage:38.2826%\n",
            "Epoch [580/1200], Training Loss: 1.0605, Validation Loss: 2.8773, Validation Percentage:40.9804%,Test Loss: 3.2046, Test Percentage:38.4615%\n",
            "Epoch [581/1200], Training Loss: 1.0890, Validation Loss: 2.7040, Validation Percentage:43.1373%,Test Loss: 2.9821, Test Percentage:40.2667%\n",
            "Epoch [582/1200], Training Loss: 1.1039, Validation Loss: 2.7492, Validation Percentage:44.3137%,Test Loss: 3.1925, Test Percentage:37.8923%\n",
            "Epoch [583/1200], Training Loss: 1.0823, Validation Loss: 3.0122, Validation Percentage:40.6863%,Test Loss: 3.3419, Test Percentage:39.0958%\n",
            "Epoch [584/1200], Training Loss: 1.1404, Validation Loss: 2.9625, Validation Percentage:39.6078%,Test Loss: 3.2854, Test Percentage:39.4373%\n",
            "Epoch [585/1200], Training Loss: 1.0558, Validation Loss: 2.8504, Validation Percentage:39.7059%,Test Loss: 3.1904, Test Percentage:39.4698%\n",
            "Epoch [586/1200], Training Loss: 1.0692, Validation Loss: 3.0181, Validation Percentage:40.8824%,Test Loss: 3.4292, Test Percentage:38.8518%\n",
            "Epoch [587/1200], Training Loss: 1.0991, Validation Loss: 2.9703, Validation Percentage:42.6471%,Test Loss: 3.3052, Test Percentage:38.5429%\n",
            "Epoch [588/1200], Training Loss: 1.0948, Validation Loss: 2.9585, Validation Percentage:40.6863%,Test Loss: 3.1736, Test Percentage:38.7705%\n",
            "Epoch [589/1200], Training Loss: 1.0631, Validation Loss: 2.9270, Validation Percentage:39.6078%,Test Loss: 3.2292, Test Percentage:37.9411%\n",
            "Epoch [590/1200], Training Loss: 1.0594, Validation Loss: 2.9595, Validation Percentage:43.4314%,Test Loss: 3.3836, Test Percentage:38.9006%\n",
            "Epoch [591/1200], Training Loss: 1.0661, Validation Loss: 3.0465, Validation Percentage:41.5686%,Test Loss: 3.3724, Test Percentage:38.5754%\n",
            "Epoch [592/1200], Training Loss: 1.1240, Validation Loss: 2.8706, Validation Percentage:41.5686%,Test Loss: 3.2399, Test Percentage:38.8193%\n",
            "Epoch [593/1200], Training Loss: 1.1950, Validation Loss: 3.2706, Validation Percentage:39.3137%,Test Loss: 3.6086, Test Percentage:37.1280%\n",
            "Epoch [594/1200], Training Loss: 1.0862, Validation Loss: 2.8574, Validation Percentage:40.1961%,Test Loss: 3.2498, Test Percentage:38.2013%\n",
            "Epoch [595/1200], Training Loss: 1.0531, Validation Loss: 3.1223, Validation Percentage:41.4706%,Test Loss: 3.3755, Test Percentage:39.7300%\n",
            "Epoch [596/1200], Training Loss: 1.0743, Validation Loss: 2.9424, Validation Percentage:43.1373%,Test Loss: 3.2032, Test Percentage:39.4048%\n",
            "Epoch [597/1200], Training Loss: 1.1634, Validation Loss: 3.0779, Validation Percentage:40.9804%,Test Loss: 3.3827, Test Percentage:36.8515%\n",
            "Epoch [598/1200], Training Loss: 1.0781, Validation Loss: 2.7945, Validation Percentage:41.3725%,Test Loss: 3.0565, Test Percentage:40.0390%\n",
            "Epoch [599/1200], Training Loss: 1.0005, Validation Loss: 2.8200, Validation Percentage:42.3529%,Test Loss: 3.1626, Test Percentage:39.2422%\n",
            "Epoch [600/1200], Training Loss: 1.0693, Validation Loss: 3.0823, Validation Percentage:38.6275%,Test Loss: 3.2159, Test Percentage:37.5346%\n",
            "Epoch [601/1200], Training Loss: 0.9638, Validation Loss: 3.0841, Validation Percentage:39.9020%,Test Loss: 3.2973, Test Percentage:36.7702%\n",
            "Epoch [602/1200], Training Loss: 1.0400, Validation Loss: 2.9005, Validation Percentage:40.5882%,Test Loss: 3.2652, Test Percentage:38.2664%\n",
            "Epoch [603/1200], Training Loss: 0.9718, Validation Loss: 2.9298, Validation Percentage:41.7647%,Test Loss: 3.2450, Test Percentage:38.5591%\n",
            "Epoch [604/1200], Training Loss: 1.0673, Validation Loss: 2.9080, Validation Percentage:40.5882%,Test Loss: 3.2713, Test Percentage:38.5103%\n",
            "Epoch [605/1200], Training Loss: 0.9901, Validation Loss: 2.8495, Validation Percentage:40.8824%,Test Loss: 3.0749, Test Percentage:38.9982%\n",
            "Epoch [606/1200], Training Loss: 1.0461, Validation Loss: 2.8068, Validation Percentage:42.6471%,Test Loss: 3.1044, Test Percentage:37.8436%\n",
            "Epoch [607/1200], Training Loss: 1.0980, Validation Loss: 3.0986, Validation Percentage:41.5686%,Test Loss: 3.3407, Test Percentage:39.4210%\n",
            "Epoch [608/1200], Training Loss: 1.1049, Validation Loss: 2.9122, Validation Percentage:40.0980%,Test Loss: 3.2126, Test Percentage:38.3640%\n",
            "Epoch [609/1200], Training Loss: 0.9705, Validation Loss: 2.8968, Validation Percentage:41.9608%,Test Loss: 3.1817, Test Percentage:38.6567%\n",
            "Epoch [610/1200], Training Loss: 0.9841, Validation Loss: 3.0974, Validation Percentage:43.2353%,Test Loss: 3.5171, Test Percentage:39.1608%\n",
            "Epoch [611/1200], Training Loss: 1.1007, Validation Loss: 3.0297, Validation Percentage:40.7843%,Test Loss: 3.3205, Test Percentage:38.6242%\n",
            "Epoch [612/1200], Training Loss: 1.0964, Validation Loss: 2.9390, Validation Percentage:42.8431%,Test Loss: 3.1733, Test Percentage:37.7622%\n",
            "Epoch [613/1200], Training Loss: 1.0025, Validation Loss: 2.9229, Validation Percentage:43.3333%,Test Loss: 3.1581, Test Percentage:39.5186%\n",
            "Epoch [614/1200], Training Loss: 1.0672, Validation Loss: 3.1709, Validation Percentage:39.7059%,Test Loss: 3.3010, Test Percentage:39.3235%\n",
            "Epoch [615/1200], Training Loss: 1.0465, Validation Loss: 3.0336, Validation Percentage:39.9020%,Test Loss: 3.3112, Test Percentage:39.0633%\n",
            "Epoch [616/1200], Training Loss: 1.0549, Validation Loss: 2.7789, Validation Percentage:42.9412%,Test Loss: 3.1063, Test Percentage:39.9415%\n",
            "Epoch [617/1200], Training Loss: 1.1404, Validation Loss: 2.7901, Validation Percentage:42.6471%,Test Loss: 3.0854, Test Percentage:39.8927%\n",
            "Epoch [618/1200], Training Loss: 1.0270, Validation Loss: 3.0714, Validation Percentage:39.8039%,Test Loss: 3.3255, Test Percentage:39.1283%\n",
            "Epoch [619/1200], Training Loss: 1.0451, Validation Loss: 2.8275, Validation Percentage:42.8431%,Test Loss: 3.1217, Test Percentage:39.2096%\n",
            "Epoch [620/1200], Training Loss: 1.0836, Validation Loss: 3.2232, Validation Percentage:39.2157%,Test Loss: 3.3649, Test Percentage:40.6570%\n",
            "Epoch [621/1200], Training Loss: 1.0359, Validation Loss: 2.8718, Validation Percentage:42.1569%,Test Loss: 3.3041, Test Percentage:38.5591%\n",
            "Epoch [622/1200], Training Loss: 1.1487, Validation Loss: 2.9980, Validation Percentage:40.0980%,Test Loss: 3.2131, Test Percentage:38.2501%\n",
            "Epoch [623/1200], Training Loss: 1.0583, Validation Loss: 2.8739, Validation Percentage:43.7255%,Test Loss: 3.2726, Test Percentage:38.6567%\n",
            "Epoch [624/1200], Training Loss: 1.0456, Validation Loss: 3.1303, Validation Percentage:41.9608%,Test Loss: 3.4401, Test Percentage:39.7138%\n",
            "Epoch [625/1200], Training Loss: 1.0405, Validation Loss: 3.0622, Validation Percentage:40.3922%,Test Loss: 3.4174, Test Percentage:38.2339%\n",
            "Epoch [626/1200], Training Loss: 0.9876, Validation Loss: 3.0574, Validation Percentage:41.9608%,Test Loss: 3.4278, Test Percentage:40.1203%\n",
            "Epoch [627/1200], Training Loss: 1.0952, Validation Loss: 2.8856, Validation Percentage:44.3137%,Test Loss: 3.1665, Test Percentage:39.5024%\n",
            "Epoch [628/1200], Training Loss: 1.0554, Validation Loss: 2.9085, Validation Percentage:42.1569%,Test Loss: 3.1189, Test Percentage:40.0228%\n",
            "Epoch [629/1200], Training Loss: 0.9841, Validation Loss: 3.0713, Validation Percentage:39.7059%,Test Loss: 3.3972, Test Percentage:37.9249%\n",
            "Epoch [630/1200], Training Loss: 1.1010, Validation Loss: 2.8589, Validation Percentage:43.9216%,Test Loss: 3.1435, Test Percentage:39.4861%\n",
            "Epoch [631/1200], Training Loss: 1.0669, Validation Loss: 2.7012, Validation Percentage:43.5294%,Test Loss: 3.0801, Test Percentage:40.9010%\n",
            "Epoch [632/1200], Training Loss: 1.0531, Validation Loss: 2.9124, Validation Percentage:39.4118%,Test Loss: 3.2640, Test Percentage:37.7134%\n",
            "Epoch [633/1200], Training Loss: 1.0270, Validation Loss: 2.8078, Validation Percentage:43.2353%,Test Loss: 3.1491, Test Percentage:38.2664%\n",
            "Epoch [634/1200], Training Loss: 1.0858, Validation Loss: 2.9297, Validation Percentage:41.5686%,Test Loss: 3.0817, Test Percentage:39.6650%\n",
            "Epoch [635/1200], Training Loss: 1.0471, Validation Loss: 2.8165, Validation Percentage:42.1569%,Test Loss: 3.2778, Test Percentage:39.2584%\n",
            "Epoch [636/1200], Training Loss: 1.1178, Validation Loss: 3.1475, Validation Percentage:40.9804%,Test Loss: 3.5314, Test Percentage:36.9003%\n",
            "Epoch [637/1200], Training Loss: 1.2077, Validation Loss: 2.9423, Validation Percentage:41.7647%,Test Loss: 3.1183, Test Percentage:39.3723%\n",
            "Epoch [638/1200], Training Loss: 1.0632, Validation Loss: 2.9679, Validation Percentage:41.4706%,Test Loss: 3.3542, Test Percentage:39.1283%\n",
            "Epoch [639/1200], Training Loss: 1.0499, Validation Loss: 2.7366, Validation Percentage:41.8627%,Test Loss: 3.0188, Test Percentage:41.6490%\n",
            "Epoch [640/1200], Training Loss: 1.0468, Validation Loss: 2.9479, Validation Percentage:44.8039%,Test Loss: 3.2103, Test Percentage:40.8359%\n",
            "Epoch [641/1200], Training Loss: 1.0559, Validation Loss: 2.9752, Validation Percentage:41.9608%,Test Loss: 3.2236, Test Percentage:38.8518%\n",
            "Epoch [642/1200], Training Loss: 1.0374, Validation Loss: 2.7809, Validation Percentage:43.4314%,Test Loss: 3.0852, Test Percentage:40.1203%\n",
            "Epoch [643/1200], Training Loss: 1.0726, Validation Loss: 3.2010, Validation Percentage:45.5882%,Test Loss: 3.5753, Test Percentage:39.9577%\n",
            "Epoch [644/1200], Training Loss: 1.0259, Validation Loss: 2.8627, Validation Percentage:44.9020%,Test Loss: 3.1847, Test Percentage:39.8601%\n",
            "Epoch [645/1200], Training Loss: 1.0340, Validation Loss: 2.8930, Validation Percentage:40.9804%,Test Loss: 3.1576, Test Percentage:40.0228%\n",
            "Epoch [646/1200], Training Loss: 1.0992, Validation Loss: 2.9407, Validation Percentage:44.0196%,Test Loss: 3.3847, Test Percentage:39.6812%\n",
            "Epoch [647/1200], Training Loss: 1.1330, Validation Loss: 3.1744, Validation Percentage:40.0980%,Test Loss: 3.4732, Test Percentage:37.8436%\n",
            "Epoch [648/1200], Training Loss: 1.0791, Validation Loss: 2.7237, Validation Percentage:42.1569%,Test Loss: 3.0653, Test Percentage:39.8927%\n",
            "Epoch [649/1200], Training Loss: 1.0839, Validation Loss: 2.7870, Validation Percentage:41.3725%,Test Loss: 3.2783, Test Percentage:39.5999%\n",
            "Epoch [650/1200], Training Loss: 0.9828, Validation Loss: 2.8832, Validation Percentage:41.6667%,Test Loss: 3.1311, Test Percentage:39.8927%\n",
            "Epoch [651/1200], Training Loss: 0.9023, Validation Loss: 3.1190, Validation Percentage:42.7451%,Test Loss: 3.4660, Test Percentage:40.1691%\n",
            "Epoch [652/1200], Training Loss: 1.0074, Validation Loss: 3.1426, Validation Percentage:43.0392%,Test Loss: 3.4840, Test Percentage:38.7217%\n",
            "Epoch [653/1200], Training Loss: 1.0122, Validation Loss: 2.9676, Validation Percentage:40.4902%,Test Loss: 3.3663, Test Percentage:37.9899%\n",
            "Epoch [654/1200], Training Loss: 1.0125, Validation Loss: 3.1985, Validation Percentage:41.5686%,Test Loss: 3.5533, Test Percentage:40.2342%\n",
            "Epoch [655/1200], Training Loss: 1.0831, Validation Loss: 3.0302, Validation Percentage:40.1961%,Test Loss: 3.2890, Test Percentage:39.4210%\n",
            "Epoch [656/1200], Training Loss: 1.1337, Validation Loss: 2.8542, Validation Percentage:42.1569%,Test Loss: 3.1305, Test Percentage:39.2259%\n",
            "Epoch [657/1200], Training Loss: 0.8865, Validation Loss: 2.9125, Validation Percentage:44.1176%,Test Loss: 3.2245, Test Percentage:40.7058%\n",
            "Epoch [658/1200], Training Loss: 1.0703, Validation Loss: 2.7372, Validation Percentage:41.1765%,Test Loss: 3.1323, Test Percentage:38.0550%\n",
            "Epoch [659/1200], Training Loss: 1.0312, Validation Loss: 2.9543, Validation Percentage:41.8627%,Test Loss: 3.3949, Test Percentage:39.0633%\n",
            "Epoch [660/1200], Training Loss: 1.0795, Validation Loss: 3.0082, Validation Percentage:42.4510%,Test Loss: 3.2360, Test Percentage:39.5349%\n",
            "Epoch [661/1200], Training Loss: 1.0913, Validation Loss: 2.9571, Validation Percentage:42.6471%,Test Loss: 3.3689, Test Percentage:37.5671%\n",
            "Epoch [662/1200], Training Loss: 1.1282, Validation Loss: 3.0090, Validation Percentage:41.8627%,Test Loss: 3.2623, Test Percentage:39.6487%\n",
            "Epoch [663/1200], Training Loss: 0.9893, Validation Loss: 2.7290, Validation Percentage:43.6275%,Test Loss: 3.0614, Test Percentage:38.5591%\n",
            "Epoch [664/1200], Training Loss: 0.9432, Validation Loss: 3.1775, Validation Percentage:43.3333%,Test Loss: 3.4618, Test Percentage:39.2747%\n",
            "Epoch [665/1200], Training Loss: 0.9832, Validation Loss: 2.9630, Validation Percentage:41.0784%,Test Loss: 3.2119, Test Percentage:40.0878%\n",
            "Epoch [666/1200], Training Loss: 0.9467, Validation Loss: 2.8351, Validation Percentage:41.5686%,Test Loss: 3.1347, Test Percentage:41.8117%\n",
            "Epoch [667/1200], Training Loss: 0.9515, Validation Loss: 2.9345, Validation Percentage:42.3529%,Test Loss: 3.1282, Test Percentage:41.2262%\n",
            "Epoch [668/1200], Training Loss: 1.0732, Validation Loss: 2.8973, Validation Percentage:42.0588%,Test Loss: 3.1409, Test Percentage:40.5757%\n",
            "Epoch [669/1200], Training Loss: 0.9466, Validation Loss: 3.0369, Validation Percentage:42.9412%,Test Loss: 3.4175, Test Percentage:39.9415%\n",
            "Epoch [670/1200], Training Loss: 0.9215, Validation Loss: 3.0476, Validation Percentage:41.1765%,Test Loss: 3.3014, Test Percentage:38.7868%\n",
            "Epoch [671/1200], Training Loss: 1.0893, Validation Loss: 2.5956, Validation Percentage:42.8431%,Test Loss: 2.9781, Test Percentage:38.8518%\n",
            "Epoch [672/1200], Training Loss: 0.9923, Validation Loss: 2.8614, Validation Percentage:43.5294%,Test Loss: 3.1358, Test Percentage:39.8114%\n",
            "Epoch [673/1200], Training Loss: 0.9371, Validation Loss: 2.8866, Validation Percentage:41.7647%,Test Loss: 3.3566, Test Percentage:38.9332%\n",
            "Epoch [674/1200], Training Loss: 0.9452, Validation Loss: 2.8996, Validation Percentage:41.2745%,Test Loss: 3.1858, Test Percentage:39.5349%\n",
            "Epoch [675/1200], Training Loss: 1.0067, Validation Loss: 3.2376, Validation Percentage:39.9020%,Test Loss: 3.5654, Test Percentage:39.0307%\n",
            "Epoch [676/1200], Training Loss: 0.9521, Validation Loss: 2.8850, Validation Percentage:41.3725%,Test Loss: 3.1927, Test Percentage:39.9577%\n",
            "Epoch [677/1200], Training Loss: 0.9889, Validation Loss: 2.8092, Validation Percentage:43.8235%,Test Loss: 3.2372, Test Percentage:40.3155%\n",
            "Epoch [678/1200], Training Loss: 0.9781, Validation Loss: 2.9931, Validation Percentage:42.4510%,Test Loss: 3.2372, Test Percentage:39.1608%\n",
            "Epoch [679/1200], Training Loss: 0.9985, Validation Loss: 2.8835, Validation Percentage:40.0000%,Test Loss: 3.1158, Test Percentage:39.7951%\n",
            "Epoch [680/1200], Training Loss: 1.0283, Validation Loss: 3.1112, Validation Percentage:41.7647%,Test Loss: 3.3953, Test Percentage:39.7788%\n",
            "Epoch [681/1200], Training Loss: 1.0156, Validation Loss: 2.8488, Validation Percentage:40.1961%,Test Loss: 3.0588, Test Percentage:41.6490%\n",
            "Epoch [682/1200], Training Loss: 1.0658, Validation Loss: 2.8878, Validation Percentage:42.6471%,Test Loss: 3.1959, Test Percentage:39.9577%\n",
            "Epoch [683/1200], Training Loss: 1.0804, Validation Loss: 3.0213, Validation Percentage:39.8039%,Test Loss: 3.0734, Test Percentage:39.7138%\n",
            "Epoch [684/1200], Training Loss: 1.0674, Validation Loss: 3.0335, Validation Percentage:40.8824%,Test Loss: 3.2772, Test Percentage:37.7622%\n",
            "Epoch [685/1200], Training Loss: 0.9792, Validation Loss: 3.0959, Validation Percentage:40.9804%,Test Loss: 3.2034, Test Percentage:40.2342%\n",
            "Epoch [686/1200], Training Loss: 1.0065, Validation Loss: 2.7431, Validation Percentage:43.9216%,Test Loss: 3.2076, Test Percentage:40.6408%\n",
            "Epoch [687/1200], Training Loss: 0.8682, Validation Loss: 3.3183, Validation Percentage:42.1569%,Test Loss: 3.4775, Test Percentage:40.4131%\n",
            "Epoch [688/1200], Training Loss: 1.0361, Validation Loss: 2.9786, Validation Percentage:42.1569%,Test Loss: 3.1535, Test Percentage:40.4293%\n",
            "Epoch [689/1200], Training Loss: 1.0729, Validation Loss: 3.0886, Validation Percentage:43.6275%,Test Loss: 3.3500, Test Percentage:39.6650%\n",
            "Epoch [690/1200], Training Loss: 1.0050, Validation Loss: 2.8665, Validation Percentage:41.1765%,Test Loss: 3.1050, Test Percentage:40.2992%\n",
            "Epoch [691/1200], Training Loss: 0.9495, Validation Loss: 3.0207, Validation Percentage:42.5490%,Test Loss: 3.3202, Test Percentage:40.0553%\n",
            "Epoch [692/1200], Training Loss: 0.9321, Validation Loss: 2.8663, Validation Percentage:43.5294%,Test Loss: 3.2263, Test Percentage:39.3072%\n",
            "Epoch [693/1200], Training Loss: 0.9594, Validation Loss: 3.1029, Validation Percentage:41.6667%,Test Loss: 3.4919, Test Percentage:38.9982%\n",
            "Epoch [694/1200], Training Loss: 1.0149, Validation Loss: 3.2213, Validation Percentage:40.5882%,Test Loss: 3.5578, Test Percentage:39.7788%\n",
            "Epoch [695/1200], Training Loss: 1.0498, Validation Loss: 3.0734, Validation Percentage:40.1961%,Test Loss: 3.3003, Test Percentage:39.3885%\n",
            "Epoch [696/1200], Training Loss: 1.0831, Validation Loss: 2.8742, Validation Percentage:43.1373%,Test Loss: 3.1713, Test Percentage:40.4619%\n",
            "Epoch [697/1200], Training Loss: 0.9884, Validation Loss: 3.0047, Validation Percentage:41.3725%,Test Loss: 3.2253, Test Percentage:39.7626%\n",
            "Epoch [698/1200], Training Loss: 0.9000, Validation Loss: 2.8647, Validation Percentage:44.2157%,Test Loss: 3.2120, Test Percentage:40.0553%\n",
            "Epoch [699/1200], Training Loss: 0.9578, Validation Loss: 2.9349, Validation Percentage:41.7647%,Test Loss: 3.3547, Test Percentage:39.3072%\n",
            "Epoch [700/1200], Training Loss: 0.9846, Validation Loss: 3.0060, Validation Percentage:43.7255%,Test Loss: 3.2350, Test Percentage:41.8930%\n",
            "Epoch [701/1200], Training Loss: 1.0357, Validation Loss: 3.0059, Validation Percentage:39.7059%,Test Loss: 3.2009, Test Percentage:39.9902%\n",
            "Epoch [702/1200], Training Loss: 0.9530, Validation Loss: 3.0116, Validation Percentage:42.1569%,Test Loss: 3.3410, Test Percentage:40.5594%\n",
            "Epoch [703/1200], Training Loss: 0.9684, Validation Loss: 3.1143, Validation Percentage:43.9216%,Test Loss: 3.3639, Test Percentage:40.8196%\n",
            "Epoch [704/1200], Training Loss: 1.0444, Validation Loss: 3.2665, Validation Percentage:40.9804%,Test Loss: 3.4487, Test Percentage:39.7300%\n",
            "Epoch [705/1200], Training Loss: 1.0321, Validation Loss: 2.7404, Validation Percentage:42.5490%,Test Loss: 3.1076, Test Percentage:39.4536%\n",
            "Epoch [706/1200], Training Loss: 1.0519, Validation Loss: 2.9207, Validation Percentage:44.0196%,Test Loss: 3.2644, Test Percentage:40.4619%\n",
            "Epoch [707/1200], Training Loss: 0.9463, Validation Loss: 3.1521, Validation Percentage:42.2549%,Test Loss: 3.4878, Test Percentage:40.2504%\n",
            "Epoch [708/1200], Training Loss: 0.9875, Validation Loss: 3.1820, Validation Percentage:40.2941%,Test Loss: 3.4782, Test Percentage:41.4376%\n",
            "Epoch [709/1200], Training Loss: 1.0901, Validation Loss: 3.0162, Validation Percentage:40.7843%,Test Loss: 3.2293, Test Percentage:39.0145%\n",
            "Epoch [710/1200], Training Loss: 1.1083, Validation Loss: 2.7728, Validation Percentage:42.8431%,Test Loss: 3.2758, Test Percentage:39.1934%\n",
            "Epoch [711/1200], Training Loss: 0.9713, Validation Loss: 2.8901, Validation Percentage:41.1765%,Test Loss: 3.2462, Test Percentage:39.5349%\n",
            "Epoch [712/1200], Training Loss: 0.9879, Validation Loss: 3.0705, Validation Percentage:41.8627%,Test Loss: 3.3788, Test Percentage:38.9006%\n",
            "Epoch [713/1200], Training Loss: 1.0370, Validation Loss: 2.8451, Validation Percentage:44.1176%,Test Loss: 3.2246, Test Percentage:39.8114%\n",
            "Epoch [714/1200], Training Loss: 0.9347, Validation Loss: 3.1144, Validation Percentage:41.8627%,Test Loss: 3.2850, Test Percentage:41.1612%\n",
            "Epoch [715/1200], Training Loss: 0.9743, Validation Loss: 2.9455, Validation Percentage:42.0588%,Test Loss: 3.2676, Test Percentage:39.2096%\n",
            "Epoch [716/1200], Training Loss: 0.8974, Validation Loss: 2.9226, Validation Percentage:44.6078%,Test Loss: 3.2532, Test Percentage:40.4293%\n",
            "Epoch [717/1200], Training Loss: 0.9950, Validation Loss: 2.8618, Validation Percentage:41.4706%,Test Loss: 3.2297, Test Percentage:40.1203%\n",
            "Epoch [718/1200], Training Loss: 0.9717, Validation Loss: 3.0814, Validation Percentage:41.2745%,Test Loss: 3.3240, Test Percentage:41.4702%\n",
            "Epoch [719/1200], Training Loss: 1.0534, Validation Loss: 2.8913, Validation Percentage:45.0980%,Test Loss: 3.2679, Test Percentage:40.6082%\n",
            "Epoch [720/1200], Training Loss: 0.9684, Validation Loss: 3.0039, Validation Percentage:40.5882%,Test Loss: 3.2596, Test Percentage:39.2909%\n",
            "Epoch [721/1200], Training Loss: 1.0676, Validation Loss: 2.8383, Validation Percentage:43.0392%,Test Loss: 3.1495, Test Percentage:40.5269%\n",
            "Epoch [722/1200], Training Loss: 0.9742, Validation Loss: 2.7577, Validation Percentage:43.2353%,Test Loss: 3.1605, Test Percentage:40.9010%\n",
            "Epoch [723/1200], Training Loss: 1.0602, Validation Loss: 2.9783, Validation Percentage:41.7647%,Test Loss: 3.3226, Test Percentage:39.3072%\n",
            "Epoch [724/1200], Training Loss: 0.8987, Validation Loss: 2.9544, Validation Percentage:42.7451%,Test Loss: 3.2147, Test Percentage:40.7546%\n",
            "Epoch [725/1200], Training Loss: 0.9029, Validation Loss: 2.9598, Validation Percentage:40.3922%,Test Loss: 3.2192, Test Percentage:41.4214%\n",
            "Epoch [726/1200], Training Loss: 0.9516, Validation Loss: 3.0496, Validation Percentage:41.8627%,Test Loss: 3.2532, Test Percentage:40.2992%\n",
            "Epoch [727/1200], Training Loss: 0.9254, Validation Loss: 2.8969, Validation Percentage:42.6471%,Test Loss: 3.2441, Test Percentage:41.2262%\n",
            "Epoch [728/1200], Training Loss: 1.0598, Validation Loss: 3.0566, Validation Percentage:39.8039%,Test Loss: 3.2063, Test Percentage:40.4293%\n",
            "Epoch [729/1200], Training Loss: 0.9712, Validation Loss: 2.8609, Validation Percentage:45.0980%,Test Loss: 3.2001, Test Percentage:41.2100%\n",
            "Epoch [730/1200], Training Loss: 0.9871, Validation Loss: 3.0374, Validation Percentage:42.1569%,Test Loss: 3.4031, Test Percentage:40.2017%\n",
            "Epoch [731/1200], Training Loss: 1.0014, Validation Loss: 2.8989, Validation Percentage:43.1373%,Test Loss: 3.2392, Test Percentage:40.1366%\n",
            "Epoch [732/1200], Training Loss: 0.9346, Validation Loss: 2.9631, Validation Percentage:42.2549%,Test Loss: 3.1471, Test Percentage:42.4297%\n",
            "Epoch [733/1200], Training Loss: 0.9495, Validation Loss: 2.9816, Validation Percentage:41.6667%,Test Loss: 3.4103, Test Percentage:39.7300%\n",
            "Epoch [734/1200], Training Loss: 1.0383, Validation Loss: 2.9170, Validation Percentage:42.9412%,Test Loss: 3.3621, Test Percentage:39.6487%\n",
            "Epoch [735/1200], Training Loss: 0.9664, Validation Loss: 2.8375, Validation Percentage:44.8039%,Test Loss: 3.3047, Test Percentage:40.4456%\n",
            "Epoch [736/1200], Training Loss: 1.0420, Validation Loss: 2.8206, Validation Percentage:41.9608%,Test Loss: 3.0657, Test Percentage:40.8522%\n",
            "Epoch [737/1200], Training Loss: 0.9686, Validation Loss: 2.8364, Validation Percentage:41.4706%,Test Loss: 3.2691, Test Percentage:39.3723%\n",
            "Epoch [738/1200], Training Loss: 1.0461, Validation Loss: 3.1475, Validation Percentage:40.3922%,Test Loss: 3.3968, Test Percentage:39.8927%\n",
            "Epoch [739/1200], Training Loss: 1.0266, Validation Loss: 3.2508, Validation Percentage:42.1569%,Test Loss: 3.5508, Test Percentage:40.3643%\n",
            "Epoch [740/1200], Training Loss: 1.0875, Validation Loss: 3.0533, Validation Percentage:41.3725%,Test Loss: 3.3118, Test Percentage:39.9740%\n",
            "Epoch [741/1200], Training Loss: 0.9644, Validation Loss: 2.7350, Validation Percentage:43.3333%,Test Loss: 3.1439, Test Percentage:41.4539%\n",
            "Epoch [742/1200], Training Loss: 0.9801, Validation Loss: 3.1165, Validation Percentage:43.0392%,Test Loss: 3.4993, Test Percentage:40.4131%\n",
            "Epoch [743/1200], Training Loss: 1.0298, Validation Loss: 2.9449, Validation Percentage:42.3529%,Test Loss: 3.3309, Test Percentage:39.7138%\n",
            "Epoch [744/1200], Training Loss: 1.0200, Validation Loss: 3.2348, Validation Percentage:40.8824%,Test Loss: 3.6662, Test Percentage:39.2259%\n",
            "Epoch [745/1200], Training Loss: 0.9158, Validation Loss: 3.1861, Validation Percentage:41.0784%,Test Loss: 3.4350, Test Percentage:39.4373%\n",
            "Epoch [746/1200], Training Loss: 0.9575, Validation Loss: 3.0342, Validation Percentage:41.6667%,Test Loss: 3.2375, Test Percentage:40.7058%\n",
            "Epoch [747/1200], Training Loss: 1.0282, Validation Loss: 2.9958, Validation Percentage:41.8627%,Test Loss: 3.3274, Test Percentage:41.0799%\n",
            "Epoch [748/1200], Training Loss: 0.9781, Validation Loss: 3.0109, Validation Percentage:42.9412%,Test Loss: 3.2310, Test Percentage:41.0961%\n",
            "Epoch [749/1200], Training Loss: 1.0124, Validation Loss: 2.9610, Validation Percentage:42.7451%,Test Loss: 3.3341, Test Percentage:40.8196%\n",
            "Epoch [750/1200], Training Loss: 1.0072, Validation Loss: 3.0290, Validation Percentage:40.2941%,Test Loss: 3.1927, Test Percentage:40.7058%\n",
            "Epoch [751/1200], Training Loss: 0.9967, Validation Loss: 3.0416, Validation Percentage:42.1569%,Test Loss: 3.3236, Test Percentage:41.2913%\n",
            "Epoch [752/1200], Training Loss: 0.9545, Validation Loss: 2.9024, Validation Percentage:43.8235%,Test Loss: 3.2666, Test Percentage:38.7055%\n",
            "Epoch [753/1200], Training Loss: 1.0097, Validation Loss: 3.1182, Validation Percentage:42.7451%,Test Loss: 3.4710, Test Percentage:39.4210%\n",
            "Epoch [754/1200], Training Loss: 0.9529, Validation Loss: 2.8439, Validation Percentage:41.8627%,Test Loss: 3.2725, Test Percentage:41.8605%\n",
            "Epoch [755/1200], Training Loss: 1.0436, Validation Loss: 2.8235, Validation Percentage:43.6275%,Test Loss: 3.3198, Test Percentage:37.6809%\n",
            "Epoch [756/1200], Training Loss: 0.9435, Validation Loss: 3.1337, Validation Percentage:42.3529%,Test Loss: 3.4865, Test Percentage:41.0636%\n",
            "Epoch [757/1200], Training Loss: 1.0144, Validation Loss: 2.9656, Validation Percentage:43.4314%,Test Loss: 3.2827, Test Percentage:38.7380%\n",
            "Epoch [758/1200], Training Loss: 0.9702, Validation Loss: 3.2171, Validation Percentage:41.5686%,Test Loss: 3.3748, Test Percentage:40.5107%\n",
            "Epoch [759/1200], Training Loss: 0.9807, Validation Loss: 2.9932, Validation Percentage:43.0392%,Test Loss: 3.2603, Test Percentage:41.1449%\n",
            "Epoch [760/1200], Training Loss: 0.9134, Validation Loss: 2.9125, Validation Percentage:43.6275%,Test Loss: 3.2319, Test Percentage:40.5920%\n",
            "Epoch [761/1200], Training Loss: 0.9087, Validation Loss: 3.0191, Validation Percentage:42.0588%,Test Loss: 3.3410, Test Percentage:40.2667%\n",
            "Epoch [762/1200], Training Loss: 1.0086, Validation Loss: 3.0586, Validation Percentage:40.5882%,Test Loss: 3.2672, Test Percentage:39.6812%\n",
            "Epoch [763/1200], Training Loss: 1.0105, Validation Loss: 3.0100, Validation Percentage:44.7059%,Test Loss: 3.3435, Test Percentage:39.4536%\n",
            "Epoch [764/1200], Training Loss: 0.9383, Validation Loss: 2.8961, Validation Percentage:44.1176%,Test Loss: 3.2390, Test Percentage:41.1612%\n",
            "Epoch [765/1200], Training Loss: 0.8891, Validation Loss: 3.1630, Validation Percentage:44.1176%,Test Loss: 3.5659, Test Percentage:39.9577%\n",
            "Epoch [766/1200], Training Loss: 1.0218, Validation Loss: 2.9855, Validation Percentage:43.4314%,Test Loss: 3.4646, Test Percentage:39.6487%\n",
            "Epoch [767/1200], Training Loss: 1.0100, Validation Loss: 3.0454, Validation Percentage:42.7451%,Test Loss: 3.4503, Test Percentage:39.9252%\n",
            "Epoch [768/1200], Training Loss: 0.9097, Validation Loss: 2.9481, Validation Percentage:44.3137%,Test Loss: 3.3693, Test Percentage:39.8114%\n",
            "Epoch [769/1200], Training Loss: 1.0498, Validation Loss: 2.8863, Validation Percentage:44.1176%,Test Loss: 3.3314, Test Percentage:40.2992%\n",
            "Epoch [770/1200], Training Loss: 1.0229, Validation Loss: 2.7698, Validation Percentage:43.9216%,Test Loss: 3.1163, Test Percentage:40.4619%\n",
            "Epoch [771/1200], Training Loss: 1.0297, Validation Loss: 3.0233, Validation Percentage:39.9020%,Test Loss: 3.2426, Test Percentage:38.6404%\n",
            "Epoch [772/1200], Training Loss: 0.8926, Validation Loss: 2.8877, Validation Percentage:42.8431%,Test Loss: 3.2180, Test Percentage:40.4944%\n",
            "Epoch [773/1200], Training Loss: 1.0684, Validation Loss: 2.8409, Validation Percentage:44.1176%,Test Loss: 3.2478, Test Percentage:41.1612%\n",
            "Epoch [774/1200], Training Loss: 0.9438, Validation Loss: 2.9303, Validation Percentage:44.7059%,Test Loss: 3.2564, Test Percentage:39.8601%\n",
            "Epoch [775/1200], Training Loss: 0.9587, Validation Loss: 2.8721, Validation Percentage:42.1569%,Test Loss: 3.1504, Test Percentage:41.6978%\n",
            "Epoch [776/1200], Training Loss: 0.8971, Validation Loss: 2.9001, Validation Percentage:45.0980%,Test Loss: 3.2553, Test Percentage:41.7466%\n",
            "Epoch [777/1200], Training Loss: 0.9424, Validation Loss: 3.0503, Validation Percentage:42.9412%,Test Loss: 3.2484, Test Percentage:41.2587%\n",
            "Epoch [778/1200], Training Loss: 0.9198, Validation Loss: 3.0672, Validation Percentage:44.4118%,Test Loss: 3.4539, Test Percentage:39.8276%\n",
            "Epoch [779/1200], Training Loss: 0.8902, Validation Loss: 2.8326, Validation Percentage:43.8235%,Test Loss: 3.2906, Test Percentage:39.6975%\n",
            "Epoch [780/1200], Training Loss: 0.9069, Validation Loss: 3.0435, Validation Percentage:45.7843%,Test Loss: 3.3443, Test Percentage:41.5027%\n",
            "Epoch [781/1200], Training Loss: 0.9691, Validation Loss: 3.0998, Validation Percentage:40.8824%,Test Loss: 3.4603, Test Percentage:38.9657%\n",
            "Epoch [782/1200], Training Loss: 1.0535, Validation Loss: 2.8968, Validation Percentage:43.7255%,Test Loss: 3.1675, Test Percentage:40.0716%\n",
            "Epoch [783/1200], Training Loss: 0.8839, Validation Loss: 3.0359, Validation Percentage:43.3333%,Test Loss: 3.2508, Test Percentage:40.9985%\n",
            "Epoch [784/1200], Training Loss: 0.8377, Validation Loss: 3.0635, Validation Percentage:42.5490%,Test Loss: 3.3719, Test Percentage:40.3318%\n",
            "Epoch [785/1200], Training Loss: 0.9604, Validation Loss: 2.8239, Validation Percentage:42.0588%,Test Loss: 3.2394, Test Percentage:39.9902%\n",
            "Epoch [786/1200], Training Loss: 1.0533, Validation Loss: 2.7550, Validation Percentage:43.3333%,Test Loss: 3.2303, Test Percentage:41.7954%\n",
            "Epoch [787/1200], Training Loss: 0.9167, Validation Loss: 2.9056, Validation Percentage:45.3922%,Test Loss: 3.2929, Test Percentage:39.0795%\n",
            "Epoch [788/1200], Training Loss: 1.1107, Validation Loss: 2.9847, Validation Percentage:41.9608%,Test Loss: 3.2798, Test Percentage:41.4702%\n",
            "Epoch [789/1200], Training Loss: 0.8955, Validation Loss: 3.0318, Validation Percentage:42.8431%,Test Loss: 3.3459, Test Percentage:40.9497%\n",
            "Epoch [790/1200], Training Loss: 0.8946, Validation Loss: 3.1821, Validation Percentage:44.9020%,Test Loss: 3.4993, Test Percentage:41.4702%\n",
            "Epoch [791/1200], Training Loss: 0.9949, Validation Loss: 2.9051, Validation Percentage:43.6275%,Test Loss: 3.1638, Test Percentage:41.1774%\n",
            "Epoch [792/1200], Training Loss: 1.0581, Validation Loss: 2.9190, Validation Percentage:41.2745%,Test Loss: 3.1943, Test Percentage:40.4456%\n",
            "Epoch [793/1200], Training Loss: 1.0110, Validation Loss: 2.7440, Validation Percentage:45.9804%,Test Loss: 3.1472, Test Percentage:40.6895%\n",
            "Epoch [794/1200], Training Loss: 0.9615, Validation Loss: 2.7636, Validation Percentage:43.4314%,Test Loss: 2.9368, Test Percentage:41.2587%\n",
            "Epoch [795/1200], Training Loss: 0.8342, Validation Loss: 2.9506, Validation Percentage:46.2745%,Test Loss: 3.4653, Test Percentage:41.5189%\n",
            "Epoch [796/1200], Training Loss: 0.9613, Validation Loss: 3.1461, Validation Percentage:41.8627%,Test Loss: 3.4463, Test Percentage:41.4539%\n",
            "Epoch [797/1200], Training Loss: 0.9470, Validation Loss: 2.9074, Validation Percentage:45.9804%,Test Loss: 3.3021, Test Percentage:42.2182%\n",
            "Epoch [798/1200], Training Loss: 0.9359, Validation Loss: 3.0372, Validation Percentage:42.0588%,Test Loss: 3.3868, Test Percentage:41.6978%\n",
            "Epoch [799/1200], Training Loss: 1.0061, Validation Loss: 3.0170, Validation Percentage:43.4314%,Test Loss: 3.4023, Test Percentage:41.6816%\n",
            "Epoch [800/1200], Training Loss: 0.9250, Validation Loss: 2.9580, Validation Percentage:44.5098%,Test Loss: 3.3108, Test Percentage:40.7546%\n",
            "Epoch [801/1200], Training Loss: 1.0192, Validation Loss: 2.8556, Validation Percentage:41.7647%,Test Loss: 3.1595, Test Percentage:42.4297%\n",
            "Epoch [802/1200], Training Loss: 0.9321, Validation Loss: 2.7638, Validation Percentage:42.0588%,Test Loss: 3.0452, Test Percentage:39.7951%\n",
            "Epoch [803/1200], Training Loss: 0.9445, Validation Loss: 3.1331, Validation Percentage:43.5294%,Test Loss: 3.4334, Test Percentage:40.2342%\n",
            "Epoch [804/1200], Training Loss: 0.9949, Validation Loss: 3.0057, Validation Percentage:41.7647%,Test Loss: 3.3148, Test Percentage:38.6892%\n",
            "Epoch [805/1200], Training Loss: 0.9753, Validation Loss: 2.9791, Validation Percentage:41.6667%,Test Loss: 3.2380, Test Percentage:39.7138%\n",
            "Epoch [806/1200], Training Loss: 0.9021, Validation Loss: 3.1266, Validation Percentage:40.8824%,Test Loss: 3.3176, Test Percentage:40.9823%\n",
            "Epoch [807/1200], Training Loss: 0.9325, Validation Loss: 3.2436, Validation Percentage:41.8627%,Test Loss: 3.5659, Test Percentage:39.0795%\n",
            "Epoch [808/1200], Training Loss: 0.8751, Validation Loss: 3.1710, Validation Percentage:42.5490%,Test Loss: 3.5346, Test Percentage:38.4941%\n",
            "Epoch [809/1200], Training Loss: 1.0152, Validation Loss: 2.9837, Validation Percentage:44.8039%,Test Loss: 3.3553, Test Percentage:41.1286%\n",
            "Epoch [810/1200], Training Loss: 0.9867, Validation Loss: 2.8152, Validation Percentage:41.2745%,Test Loss: 3.1554, Test Percentage:38.2339%\n",
            "Epoch [811/1200], Training Loss: 1.0048, Validation Loss: 3.0789, Validation Percentage:43.0392%,Test Loss: 3.3492, Test Percentage:41.1124%\n",
            "Epoch [812/1200], Training Loss: 0.8742, Validation Loss: 3.0739, Validation Percentage:43.9216%,Test Loss: 3.2448, Test Percentage:41.4702%\n",
            "Epoch [813/1200], Training Loss: 0.9662, Validation Loss: 2.8911, Validation Percentage:43.5294%,Test Loss: 3.0972, Test Percentage:41.1937%\n",
            "Epoch [814/1200], Training Loss: 1.0339, Validation Loss: 2.7638, Validation Percentage:44.6078%,Test Loss: 2.9093, Test Percentage:41.9743%\n",
            "Epoch [815/1200], Training Loss: 0.9780, Validation Loss: 3.1232, Validation Percentage:41.8627%,Test Loss: 3.4790, Test Percentage:39.9577%\n",
            "Epoch [816/1200], Training Loss: 0.9272, Validation Loss: 2.9980, Validation Percentage:43.8235%,Test Loss: 3.3892, Test Percentage:39.7788%\n",
            "Epoch [817/1200], Training Loss: 0.9379, Validation Loss: 2.8500, Validation Percentage:43.0392%,Test Loss: 3.2513, Test Percentage:41.0799%\n",
            "Epoch [818/1200], Training Loss: 0.8514, Validation Loss: 3.1086, Validation Percentage:43.1373%,Test Loss: 3.3292, Test Percentage:41.8279%\n",
            "Epoch [819/1200], Training Loss: 0.8807, Validation Loss: 2.8824, Validation Percentage:44.2157%,Test Loss: 3.2444, Test Percentage:42.3809%\n",
            "Epoch [820/1200], Training Loss: 0.8915, Validation Loss: 3.1794, Validation Percentage:42.7451%,Test Loss: 3.5875, Test Percentage:41.2425%\n",
            "Epoch [821/1200], Training Loss: 1.0307, Validation Loss: 3.1714, Validation Percentage:41.2745%,Test Loss: 3.4435, Test Percentage:40.4456%\n",
            "Epoch [822/1200], Training Loss: 0.9553, Validation Loss: 2.9267, Validation Percentage:43.2353%,Test Loss: 3.3281, Test Percentage:39.7951%\n",
            "Epoch [823/1200], Training Loss: 0.9591, Validation Loss: 3.0049, Validation Percentage:44.7059%,Test Loss: 3.4257, Test Percentage:40.7383%\n",
            "Epoch [824/1200], Training Loss: 0.9539, Validation Loss: 2.7301, Validation Percentage:43.3333%,Test Loss: 3.1769, Test Percentage:40.8034%\n",
            "Epoch [825/1200], Training Loss: 0.9510, Validation Loss: 2.9265, Validation Percentage:47.3529%,Test Loss: 3.3758, Test Percentage:41.4864%\n",
            "Epoch [826/1200], Training Loss: 0.9462, Validation Loss: 2.8061, Validation Percentage:42.7451%,Test Loss: 3.3132, Test Percentage:38.9819%\n",
            "Epoch [827/1200], Training Loss: 0.9138, Validation Loss: 2.8747, Validation Percentage:43.0392%,Test Loss: 3.2163, Test Percentage:39.7951%\n",
            "Epoch [828/1200], Training Loss: 0.9926, Validation Loss: 2.8037, Validation Percentage:43.4314%,Test Loss: 3.1285, Test Percentage:40.2830%\n",
            "Epoch [829/1200], Training Loss: 0.8687, Validation Loss: 2.9574, Validation Percentage:43.8235%,Test Loss: 3.2918, Test Percentage:41.5027%\n",
            "Epoch [830/1200], Training Loss: 0.9402, Validation Loss: 2.9430, Validation Percentage:42.0588%,Test Loss: 3.2027, Test Percentage:41.8279%\n",
            "Epoch [831/1200], Training Loss: 0.8291, Validation Loss: 3.1248, Validation Percentage:43.0392%,Test Loss: 3.4275, Test Percentage:40.4456%\n",
            "Epoch [832/1200], Training Loss: 0.9446, Validation Loss: 3.1989, Validation Percentage:42.1569%,Test Loss: 3.4006, Test Percentage:42.0394%\n",
            "Epoch [833/1200], Training Loss: 0.9518, Validation Loss: 2.8170, Validation Percentage:43.6275%,Test Loss: 3.0813, Test Percentage:41.6003%\n",
            "Epoch [834/1200], Training Loss: 0.9352, Validation Loss: 2.8389, Validation Percentage:44.4118%,Test Loss: 3.2058, Test Percentage:41.4864%\n",
            "Epoch [835/1200], Training Loss: 0.9972, Validation Loss: 3.0786, Validation Percentage:44.4118%,Test Loss: 3.2566, Test Percentage:41.1449%\n",
            "Epoch [836/1200], Training Loss: 0.9011, Validation Loss: 3.0352, Validation Percentage:44.3137%,Test Loss: 3.2529, Test Percentage:41.9093%\n",
            "Epoch [837/1200], Training Loss: 0.8867, Validation Loss: 3.1430, Validation Percentage:45.0000%,Test Loss: 3.5840, Test Percentage:40.8684%\n",
            "Epoch [838/1200], Training Loss: 0.9902, Validation Loss: 3.1173, Validation Percentage:43.0392%,Test Loss: 3.4432, Test Percentage:41.2262%\n",
            "Epoch [839/1200], Training Loss: 0.8961, Validation Loss: 2.8629, Validation Percentage:43.3333%,Test Loss: 3.2462, Test Percentage:40.5757%\n",
            "Epoch [840/1200], Training Loss: 0.8410, Validation Loss: 2.9738, Validation Percentage:42.2549%,Test Loss: 3.3009, Test Percentage:41.5515%\n",
            "Epoch [841/1200], Training Loss: 1.0039, Validation Loss: 3.2796, Validation Percentage:43.3333%,Test Loss: 3.5466, Test Percentage:39.7138%\n",
            "Epoch [842/1200], Training Loss: 0.9036, Validation Loss: 2.8903, Validation Percentage:43.2353%,Test Loss: 3.3183, Test Percentage:40.9985%\n",
            "Epoch [843/1200], Training Loss: 0.9839, Validation Loss: 3.1131, Validation Percentage:43.5294%,Test Loss: 3.4763, Test Percentage:40.5920%\n",
            "Epoch [844/1200], Training Loss: 0.8876, Validation Loss: 2.9092, Validation Percentage:42.1569%,Test Loss: 3.2708, Test Percentage:40.1041%\n",
            "Epoch [845/1200], Training Loss: 0.9605, Validation Loss: 3.1029, Validation Percentage:41.4706%,Test Loss: 3.3690, Test Percentage:39.4373%\n",
            "Epoch [846/1200], Training Loss: 1.0731, Validation Loss: 2.7577, Validation Percentage:45.3922%,Test Loss: 3.1290, Test Percentage:40.7546%\n",
            "Epoch [847/1200], Training Loss: 0.8486, Validation Loss: 3.0934, Validation Percentage:43.3333%,Test Loss: 3.5114, Test Percentage:41.2100%\n",
            "Epoch [848/1200], Training Loss: 0.8727, Validation Loss: 3.2753, Validation Percentage:44.5098%,Test Loss: 3.6407, Test Percentage:41.2587%\n",
            "Epoch [849/1200], Training Loss: 0.9050, Validation Loss: 3.1025, Validation Percentage:44.2157%,Test Loss: 3.4652, Test Percentage:40.3480%\n",
            "Epoch [850/1200], Training Loss: 0.9825, Validation Loss: 2.7799, Validation Percentage:45.2941%,Test Loss: 3.2475, Test Percentage:41.0636%\n",
            "Epoch [851/1200], Training Loss: 1.0305, Validation Loss: 2.8860, Validation Percentage:41.0784%,Test Loss: 2.9906, Test Percentage:39.8601%\n",
            "Epoch [852/1200], Training Loss: 1.0142, Validation Loss: 2.7987, Validation Percentage:44.7059%,Test Loss: 3.1926, Test Percentage:40.0553%\n",
            "Epoch [853/1200], Training Loss: 0.9303, Validation Loss: 3.0310, Validation Percentage:44.0196%,Test Loss: 3.3211, Test Percentage:41.9093%\n",
            "Epoch [854/1200], Training Loss: 0.9265, Validation Loss: 3.0816, Validation Percentage:43.0392%,Test Loss: 3.2578, Test Percentage:41.6003%\n",
            "Epoch [855/1200], Training Loss: 0.9649, Validation Loss: 3.1166, Validation Percentage:43.6275%,Test Loss: 3.3807, Test Percentage:40.2992%\n",
            "Epoch [856/1200], Training Loss: 0.9627, Validation Loss: 2.9479, Validation Percentage:42.1569%,Test Loss: 3.2370, Test Percentage:40.2504%\n",
            "Epoch [857/1200], Training Loss: 0.8457, Validation Loss: 2.9188, Validation Percentage:44.6078%,Test Loss: 3.3084, Test Percentage:41.6816%\n",
            "Epoch [858/1200], Training Loss: 0.8727, Validation Loss: 3.1775, Validation Percentage:44.7059%,Test Loss: 3.4984, Test Percentage:41.6165%\n",
            "Epoch [859/1200], Training Loss: 0.9412, Validation Loss: 2.9616, Validation Percentage:41.2745%,Test Loss: 3.3211, Test Percentage:39.2747%\n",
            "Epoch [860/1200], Training Loss: 0.8455, Validation Loss: 3.1182, Validation Percentage:43.1373%,Test Loss: 3.3976, Test Percentage:38.5266%\n",
            "Epoch [861/1200], Training Loss: 1.0198, Validation Loss: 3.2112, Validation Percentage:41.9608%,Test Loss: 3.5077, Test Percentage:40.4293%\n",
            "Epoch [862/1200], Training Loss: 0.9936, Validation Loss: 3.1137, Validation Percentage:42.7451%,Test Loss: 3.4241, Test Percentage:39.9902%\n",
            "Epoch [863/1200], Training Loss: 0.8906, Validation Loss: 2.9207, Validation Percentage:41.8627%,Test Loss: 3.2324, Test Percentage:40.3968%\n",
            "Epoch [864/1200], Training Loss: 0.9192, Validation Loss: 3.0528, Validation Percentage:41.4706%,Test Loss: 3.2980, Test Percentage:40.2830%\n",
            "Epoch [865/1200], Training Loss: 0.9118, Validation Loss: 3.0749, Validation Percentage:44.1176%,Test Loss: 3.3284, Test Percentage:41.9418%\n",
            "Epoch [866/1200], Training Loss: 0.9376, Validation Loss: 2.9386, Validation Percentage:41.8627%,Test Loss: 3.3091, Test Percentage:39.3072%\n",
            "Epoch [867/1200], Training Loss: 0.9556, Validation Loss: 3.2322, Validation Percentage:42.7451%,Test Loss: 3.5519, Test Percentage:40.2179%\n",
            "Epoch [868/1200], Training Loss: 0.9203, Validation Loss: 3.3436, Validation Percentage:42.1569%,Test Loss: 3.4882, Test Percentage:41.7629%\n",
            "Epoch [869/1200], Training Loss: 0.8853, Validation Loss: 3.0604, Validation Percentage:42.6471%,Test Loss: 3.2592, Test Percentage:41.5840%\n",
            "Epoch [870/1200], Training Loss: 0.8829, Validation Loss: 3.1222, Validation Percentage:42.8431%,Test Loss: 3.3700, Test Percentage:40.9660%\n",
            "Epoch [871/1200], Training Loss: 0.9688, Validation Loss: 2.8847, Validation Percentage:43.5294%,Test Loss: 3.2359, Test Percentage:40.8847%\n",
            "Epoch [872/1200], Training Loss: 0.9249, Validation Loss: 2.7190, Validation Percentage:46.7647%,Test Loss: 3.1121, Test Percentage:42.2182%\n",
            "Epoch [873/1200], Training Loss: 0.8475, Validation Loss: 3.0122, Validation Percentage:44.6078%,Test Loss: 3.2728, Test Percentage:41.4051%\n",
            "Epoch [874/1200], Training Loss: 0.7898, Validation Loss: 2.7596, Validation Percentage:44.3137%,Test Loss: 3.1873, Test Percentage:42.3321%\n",
            "Epoch [875/1200], Training Loss: 0.8456, Validation Loss: 2.8714, Validation Percentage:46.3725%,Test Loss: 3.1781, Test Percentage:42.4297%\n",
            "Epoch [876/1200], Training Loss: 1.0185, Validation Loss: 3.0511, Validation Percentage:42.8431%,Test Loss: 3.3409, Test Percentage:40.6733%\n",
            "Epoch [877/1200], Training Loss: 0.8805, Validation Loss: 3.2406, Validation Percentage:42.6471%,Test Loss: 3.5094, Test Percentage:41.5352%\n",
            "Epoch [878/1200], Training Loss: 0.9130, Validation Loss: 2.8363, Validation Percentage:42.9412%,Test Loss: 3.1236, Test Percentage:42.1369%\n",
            "Epoch [879/1200], Training Loss: 0.9370, Validation Loss: 2.8989, Validation Percentage:43.6275%,Test Loss: 3.1761, Test Percentage:42.2996%\n",
            "Epoch [880/1200], Training Loss: 0.9592, Validation Loss: 2.8106, Validation Percentage:45.0980%,Test Loss: 3.1360, Test Percentage:42.1369%\n",
            "Epoch [881/1200], Training Loss: 0.9453, Validation Loss: 2.9452, Validation Percentage:44.8039%,Test Loss: 3.2719, Test Percentage:41.2100%\n",
            "Epoch [882/1200], Training Loss: 0.9170, Validation Loss: 2.9471, Validation Percentage:44.9020%,Test Loss: 3.3077, Test Percentage:42.4947%\n",
            "Epoch [883/1200], Training Loss: 0.9292, Validation Loss: 3.0269, Validation Percentage:44.7059%,Test Loss: 3.2642, Test Percentage:41.4539%\n",
            "Epoch [884/1200], Training Loss: 0.7328, Validation Loss: 3.2023, Validation Percentage:42.3529%,Test Loss: 3.5166, Test Percentage:42.4459%\n",
            "Epoch [885/1200], Training Loss: 0.9124, Validation Loss: 2.9441, Validation Percentage:45.8824%,Test Loss: 3.2280, Test Percentage:42.3483%\n",
            "Epoch [886/1200], Training Loss: 0.9279, Validation Loss: 2.9603, Validation Percentage:44.7059%,Test Loss: 3.1937, Test Percentage:41.2750%\n",
            "Epoch [887/1200], Training Loss: 0.8107, Validation Loss: 3.2539, Validation Percentage:44.9020%,Test Loss: 3.6548, Test Percentage:43.2265%\n",
            "Epoch [888/1200], Training Loss: 0.8912, Validation Loss: 3.0022, Validation Percentage:43.6275%,Test Loss: 3.3136, Test Percentage:40.7058%\n",
            "Epoch [889/1200], Training Loss: 0.9967, Validation Loss: 2.9910, Validation Percentage:43.2353%,Test Loss: 3.2883, Test Percentage:41.0473%\n",
            "Epoch [890/1200], Training Loss: 0.9020, Validation Loss: 2.9896, Validation Percentage:43.4314%,Test Loss: 3.2539, Test Percentage:41.2425%\n",
            "Epoch [891/1200], Training Loss: 0.9932, Validation Loss: 2.9895, Validation Percentage:45.0000%,Test Loss: 3.3279, Test Percentage:41.4539%\n",
            "Epoch [892/1200], Training Loss: 1.0087, Validation Loss: 3.0749, Validation Percentage:43.2353%,Test Loss: 3.3266, Test Percentage:41.3238%\n",
            "Epoch [893/1200], Training Loss: 0.9355, Validation Loss: 2.8130, Validation Percentage:45.5882%,Test Loss: 2.9806, Test Percentage:43.3892%\n",
            "Epoch [894/1200], Training Loss: 1.0374, Validation Loss: 3.0962, Validation Percentage:43.5294%,Test Loss: 3.3308, Test Percentage:40.3480%\n",
            "Epoch [895/1200], Training Loss: 0.9341, Validation Loss: 2.9442, Validation Percentage:43.9216%,Test Loss: 3.2114, Test Percentage:41.5189%\n",
            "Epoch [896/1200], Training Loss: 0.9557, Validation Loss: 2.8141, Validation Percentage:44.3137%,Test Loss: 3.1299, Test Percentage:41.7629%\n",
            "Epoch [897/1200], Training Loss: 0.8918, Validation Loss: 2.8908, Validation Percentage:43.6275%,Test Loss: 3.1230, Test Percentage:41.3075%\n",
            "Epoch [898/1200], Training Loss: 0.8908, Validation Loss: 2.8762, Validation Percentage:44.9020%,Test Loss: 3.1582, Test Percentage:42.1857%\n",
            "Epoch [899/1200], Training Loss: 0.9088, Validation Loss: 3.1648, Validation Percentage:44.3137%,Test Loss: 3.4237, Test Percentage:41.5677%\n",
            "Epoch [900/1200], Training Loss: 0.8857, Validation Loss: 2.8308, Validation Percentage:46.0784%,Test Loss: 3.1577, Test Percentage:42.2508%\n",
            "Epoch [901/1200], Training Loss: 0.9205, Validation Loss: 2.8805, Validation Percentage:46.5686%,Test Loss: 3.3193, Test Percentage:41.9743%\n",
            "Epoch [902/1200], Training Loss: 0.9500, Validation Loss: 2.9174, Validation Percentage:41.9608%,Test Loss: 3.1773, Test Percentage:40.9823%\n",
            "Epoch [903/1200], Training Loss: 0.9906, Validation Loss: 2.7150, Validation Percentage:45.9804%,Test Loss: 3.0463, Test Percentage:42.2670%\n",
            "Epoch [904/1200], Training Loss: 0.9245, Validation Loss: 2.8719, Validation Percentage:42.4510%,Test Loss: 3.0470, Test Percentage:43.6656%\n",
            "Epoch [905/1200], Training Loss: 0.9267, Validation Loss: 2.8328, Validation Percentage:45.7843%,Test Loss: 3.1637, Test Percentage:41.3238%\n",
            "Epoch [906/1200], Training Loss: 0.8869, Validation Loss: 2.9435, Validation Percentage:43.7255%,Test Loss: 3.2316, Test Percentage:42.7387%\n",
            "Epoch [907/1200], Training Loss: 0.8297, Validation Loss: 3.2037, Validation Percentage:45.0000%,Test Loss: 3.3941, Test Percentage:41.5840%\n",
            "Epoch [908/1200], Training Loss: 0.9578, Validation Loss: 3.3951, Validation Percentage:44.2157%,Test Loss: 3.6628, Test Percentage:41.6165%\n",
            "Epoch [909/1200], Training Loss: 0.9748, Validation Loss: 2.8491, Validation Percentage:43.8235%,Test Loss: 3.2101, Test Percentage:40.2504%\n",
            "Epoch [910/1200], Training Loss: 0.8950, Validation Loss: 2.9540, Validation Percentage:42.9412%,Test Loss: 3.1809, Test Percentage:42.3483%\n",
            "Epoch [911/1200], Training Loss: 0.9056, Validation Loss: 2.8510, Validation Percentage:42.8431%,Test Loss: 3.1906, Test Percentage:41.2913%\n",
            "Epoch [912/1200], Training Loss: 0.9194, Validation Loss: 3.0306, Validation Percentage:44.4118%,Test Loss: 3.2543, Test Percentage:40.5107%\n",
            "Epoch [913/1200], Training Loss: 0.8873, Validation Loss: 2.9549, Validation Percentage:43.2353%,Test Loss: 3.2550, Test Percentage:41.0148%\n",
            "Epoch [914/1200], Training Loss: 0.9106, Validation Loss: 2.8769, Validation Percentage:44.3137%,Test Loss: 3.3978, Test Percentage:40.4293%\n",
            "Epoch [915/1200], Training Loss: 0.9214, Validation Loss: 2.9965, Validation Percentage:44.5098%,Test Loss: 3.2985, Test Percentage:42.0394%\n",
            "Epoch [916/1200], Training Loss: 0.9640, Validation Loss: 2.8187, Validation Percentage:44.8039%,Test Loss: 3.1624, Test Percentage:41.2587%\n",
            "Epoch [917/1200], Training Loss: 0.9317, Validation Loss: 2.8355, Validation Percentage:45.2941%,Test Loss: 3.2374, Test Percentage:41.6978%\n",
            "Epoch [918/1200], Training Loss: 0.9346, Validation Loss: 3.0788, Validation Percentage:42.3529%,Test Loss: 3.3864, Test Percentage:40.4944%\n",
            "Epoch [919/1200], Training Loss: 0.9578, Validation Loss: 3.0583, Validation Percentage:44.1176%,Test Loss: 3.3738, Test Percentage:41.2913%\n",
            "Epoch [920/1200], Training Loss: 0.8917, Validation Loss: 2.9665, Validation Percentage:42.2549%,Test Loss: 3.2377, Test Percentage:41.7141%\n",
            "Epoch [921/1200], Training Loss: 0.9558, Validation Loss: 3.0604, Validation Percentage:43.9216%,Test Loss: 3.2169, Test Percentage:42.2508%\n",
            "Epoch [922/1200], Training Loss: 0.8378, Validation Loss: 3.0196, Validation Percentage:44.8039%,Test Loss: 3.4202, Test Percentage:41.1612%\n",
            "Epoch [923/1200], Training Loss: 0.8656, Validation Loss: 3.1004, Validation Percentage:44.9020%,Test Loss: 3.4285, Test Percentage:42.1695%\n",
            "Epoch [924/1200], Training Loss: 0.9382, Validation Loss: 2.9718, Validation Percentage:44.2157%,Test Loss: 3.2400, Test Percentage:41.4539%\n",
            "Epoch [925/1200], Training Loss: 0.8452, Validation Loss: 2.9709, Validation Percentage:43.0392%,Test Loss: 3.3617, Test Percentage:40.6895%\n",
            "Epoch [926/1200], Training Loss: 0.9303, Validation Loss: 3.0679, Validation Percentage:43.7255%,Test Loss: 3.4744, Test Percentage:41.3888%\n",
            "Epoch [927/1200], Training Loss: 1.0216, Validation Loss: 2.8690, Validation Percentage:42.7451%,Test Loss: 3.2489, Test Percentage:39.4536%\n",
            "Epoch [928/1200], Training Loss: 0.9409, Validation Loss: 2.9661, Validation Percentage:42.8431%,Test Loss: 3.1854, Test Percentage:40.5432%\n",
            "Epoch [929/1200], Training Loss: 0.9186, Validation Loss: 3.0058, Validation Percentage:44.0196%,Test Loss: 3.3368, Test Percentage:41.7954%\n",
            "Epoch [930/1200], Training Loss: 0.8439, Validation Loss: 2.8835, Validation Percentage:44.6078%,Test Loss: 3.3132, Test Percentage:42.0556%\n",
            "Epoch [931/1200], Training Loss: 0.9682, Validation Loss: 2.9214, Validation Percentage:44.5098%,Test Loss: 3.3160, Test Percentage:41.1124%\n",
            "Epoch [932/1200], Training Loss: 0.8622, Validation Loss: 3.1235, Validation Percentage:42.5490%,Test Loss: 3.3386, Test Percentage:40.2179%\n",
            "Epoch [933/1200], Training Loss: 0.8870, Validation Loss: 2.8938, Validation Percentage:44.8039%,Test Loss: 3.1981, Test Percentage:40.8196%\n",
            "Epoch [934/1200], Training Loss: 0.8906, Validation Loss: 3.0612, Validation Percentage:45.4902%,Test Loss: 3.3650, Test Percentage:41.8442%\n",
            "Epoch [935/1200], Training Loss: 0.9156, Validation Loss: 2.8934, Validation Percentage:45.6863%,Test Loss: 3.3977, Test Percentage:41.3075%\n",
            "Epoch [936/1200], Training Loss: 0.7964, Validation Loss: 3.1328, Validation Percentage:44.7059%,Test Loss: 3.5396, Test Percentage:41.4051%\n",
            "Epoch [937/1200], Training Loss: 0.8828, Validation Loss: 3.0188, Validation Percentage:44.9020%,Test Loss: 3.3566, Test Percentage:41.2913%\n",
            "Epoch [938/1200], Training Loss: 0.8232, Validation Loss: 2.9261, Validation Percentage:42.2549%,Test Loss: 3.1537, Test Percentage:40.8034%\n",
            "Epoch [939/1200], Training Loss: 0.9829, Validation Loss: 2.6859, Validation Percentage:45.0980%,Test Loss: 3.1650, Test Percentage:40.6570%\n",
            "Epoch [940/1200], Training Loss: 0.9834, Validation Loss: 2.7233, Validation Percentage:45.1961%,Test Loss: 3.0298, Test Percentage:41.6490%\n",
            "Epoch [941/1200], Training Loss: 0.9769, Validation Loss: 3.0148, Validation Percentage:41.9608%,Test Loss: 3.2088, Test Percentage:40.3968%\n",
            "Epoch [942/1200], Training Loss: 1.0160, Validation Loss: 2.9522, Validation Percentage:44.2157%,Test Loss: 3.2760, Test Percentage:42.0719%\n",
            "Epoch [943/1200], Training Loss: 0.9211, Validation Loss: 2.8674, Validation Percentage:45.3922%,Test Loss: 3.2528, Test Percentage:43.0151%\n",
            "Epoch [944/1200], Training Loss: 0.9029, Validation Loss: 3.0105, Validation Percentage:44.6078%,Test Loss: 3.4727, Test Percentage:41.7629%\n",
            "Epoch [945/1200], Training Loss: 1.0332, Validation Loss: 2.7919, Validation Percentage:42.0588%,Test Loss: 3.1109, Test Percentage:40.4131%\n",
            "Epoch [946/1200], Training Loss: 0.9562, Validation Loss: 3.0213, Validation Percentage:41.0784%,Test Loss: 3.2915, Test Percentage:40.9172%\n",
            "Epoch [947/1200], Training Loss: 0.9543, Validation Loss: 3.2504, Validation Percentage:41.5686%,Test Loss: 3.2979, Test Percentage:41.9093%\n",
            "Epoch [948/1200], Training Loss: 0.8904, Validation Loss: 2.9984, Validation Percentage:45.7843%,Test Loss: 3.3344, Test Percentage:42.0881%\n",
            "Epoch [949/1200], Training Loss: 0.8664, Validation Loss: 3.1236, Validation Percentage:44.2157%,Test Loss: 3.2894, Test Percentage:41.7629%\n",
            "Epoch [950/1200], Training Loss: 0.8958, Validation Loss: 2.9919, Validation Percentage:43.5294%,Test Loss: 3.3627, Test Percentage:41.1124%\n",
            "Epoch [951/1200], Training Loss: 0.8254, Validation Loss: 3.3586, Validation Percentage:42.6471%,Test Loss: 3.6811, Test Percentage:40.1203%\n",
            "Epoch [952/1200], Training Loss: 0.9961, Validation Loss: 2.9608, Validation Percentage:43.6275%,Test Loss: 3.2853, Test Percentage:39.4048%\n",
            "Epoch [953/1200], Training Loss: 0.9280, Validation Loss: 2.9365, Validation Percentage:42.8431%,Test Loss: 3.2856, Test Percentage:40.6245%\n",
            "Epoch [954/1200], Training Loss: 0.9586, Validation Loss: 3.0428, Validation Percentage:43.3333%,Test Loss: 3.3959, Test Percentage:41.1286%\n",
            "Epoch [955/1200], Training Loss: 0.8746, Validation Loss: 3.0610, Validation Percentage:41.8627%,Test Loss: 3.2748, Test Percentage:39.9252%\n",
            "Epoch [956/1200], Training Loss: 0.9332, Validation Loss: 2.9203, Validation Percentage:45.1961%,Test Loss: 3.1653, Test Percentage:41.6978%\n",
            "Epoch [957/1200], Training Loss: 0.9447, Validation Loss: 2.9820, Validation Percentage:44.7059%,Test Loss: 3.3523, Test Percentage:42.8525%\n",
            "Epoch [958/1200], Training Loss: 0.9243, Validation Loss: 2.9066, Validation Percentage:43.6275%,Test Loss: 3.2764, Test Percentage:41.1124%\n",
            "Epoch [959/1200], Training Loss: 0.8239, Validation Loss: 3.0780, Validation Percentage:44.2157%,Test Loss: 3.4348, Test Percentage:41.0636%\n",
            "Epoch [960/1200], Training Loss: 0.8835, Validation Loss: 3.2544, Validation Percentage:42.4510%,Test Loss: 3.3947, Test Percentage:43.2103%\n",
            "Epoch [961/1200], Training Loss: 0.8505, Validation Loss: 2.7958, Validation Percentage:44.8039%,Test Loss: 3.1778, Test Percentage:41.3563%\n",
            "Epoch [962/1200], Training Loss: 0.8967, Validation Loss: 3.1040, Validation Percentage:45.1961%,Test Loss: 3.4004, Test Percentage:41.5515%\n",
            "Epoch [963/1200], Training Loss: 0.9026, Validation Loss: 2.9814, Validation Percentage:42.1569%,Test Loss: 3.2709, Test Percentage:39.4861%\n",
            "Epoch [964/1200], Training Loss: 1.0114, Validation Loss: 3.0940, Validation Percentage:44.9020%,Test Loss: 3.2812, Test Percentage:41.6165%\n",
            "Epoch [965/1200], Training Loss: 0.8409, Validation Loss: 3.2648, Validation Percentage:43.6275%,Test Loss: 3.5559, Test Percentage:41.7466%\n",
            "Epoch [966/1200], Training Loss: 0.9015, Validation Loss: 3.0382, Validation Percentage:43.6275%,Test Loss: 3.2697, Test Percentage:42.0881%\n",
            "Epoch [967/1200], Training Loss: 0.8655, Validation Loss: 2.9434, Validation Percentage:42.4510%,Test Loss: 3.3358, Test Percentage:41.9906%\n",
            "Epoch [968/1200], Training Loss: 0.8762, Validation Loss: 2.9082, Validation Percentage:44.1176%,Test Loss: 3.2561, Test Percentage:41.3726%\n",
            "Epoch [969/1200], Training Loss: 0.8519, Validation Loss: 2.9719, Validation Percentage:41.8627%,Test Loss: 3.2601, Test Percentage:39.3723%\n",
            "Epoch [970/1200], Training Loss: 0.8344, Validation Loss: 3.1990, Validation Percentage:43.5294%,Test Loss: 3.4325, Test Percentage:43.2265%\n",
            "Epoch [971/1200], Training Loss: 0.9103, Validation Loss: 2.8920, Validation Percentage:44.3137%,Test Loss: 3.1146, Test Percentage:41.9743%\n",
            "Epoch [972/1200], Training Loss: 0.9051, Validation Loss: 3.0762, Validation Percentage:43.9216%,Test Loss: 3.2692, Test Percentage:41.1449%\n",
            "Epoch [973/1200], Training Loss: 0.9530, Validation Loss: 3.0105, Validation Percentage:43.8235%,Test Loss: 3.3743, Test Percentage:39.8601%\n",
            "Epoch [974/1200], Training Loss: 0.9619, Validation Loss: 3.3074, Validation Percentage:43.6275%,Test Loss: 3.5109, Test Percentage:40.4456%\n",
            "Epoch [975/1200], Training Loss: 0.8737, Validation Loss: 3.0179, Validation Percentage:45.9804%,Test Loss: 3.3333, Test Percentage:43.2428%\n",
            "Epoch [976/1200], Training Loss: 0.8038, Validation Loss: 3.4735, Validation Percentage:43.1373%,Test Loss: 3.6626, Test Percentage:42.0719%\n",
            "Epoch [977/1200], Training Loss: 0.8244, Validation Loss: 3.1276, Validation Percentage:45.0000%,Test Loss: 3.4602, Test Percentage:41.0311%\n",
            "Epoch [978/1200], Training Loss: 0.9519, Validation Loss: 3.0851, Validation Percentage:41.0784%,Test Loss: 3.2452, Test Percentage:41.9418%\n",
            "Epoch [979/1200], Training Loss: 1.0588, Validation Loss: 2.9706, Validation Percentage:42.6471%,Test Loss: 3.2414, Test Percentage:40.7546%\n",
            "Epoch [980/1200], Training Loss: 0.9775, Validation Loss: 3.1374, Validation Percentage:42.8431%,Test Loss: 3.2792, Test Percentage:41.9580%\n",
            "Epoch [981/1200], Training Loss: 0.8841, Validation Loss: 3.0681, Validation Percentage:45.0000%,Test Loss: 3.2219, Test Percentage:42.1369%\n",
            "Epoch [982/1200], Training Loss: 0.8759, Validation Loss: 3.1308, Validation Percentage:43.7255%,Test Loss: 3.5261, Test Percentage:41.7304%\n",
            "Epoch [983/1200], Training Loss: 0.8886, Validation Loss: 2.8669, Validation Percentage:46.0784%,Test Loss: 3.1992, Test Percentage:43.5681%\n",
            "Epoch [984/1200], Training Loss: 0.8046, Validation Loss: 3.2943, Validation Percentage:44.2157%,Test Loss: 3.4129, Test Percentage:42.3646%\n",
            "Epoch [985/1200], Training Loss: 0.9252, Validation Loss: 3.4377, Validation Percentage:44.7059%,Test Loss: 3.6702, Test Percentage:41.7954%\n",
            "Epoch [986/1200], Training Loss: 0.8886, Validation Loss: 3.0750, Validation Percentage:43.9216%,Test Loss: 3.3106, Test Percentage:42.3483%\n",
            "Epoch [987/1200], Training Loss: 1.0239, Validation Loss: 3.0522, Validation Percentage:43.3333%,Test Loss: 3.2770, Test Percentage:40.5757%\n",
            "Epoch [988/1200], Training Loss: 0.9038, Validation Loss: 2.9561, Validation Percentage:45.0980%,Test Loss: 3.1421, Test Percentage:41.6003%\n",
            "Epoch [989/1200], Training Loss: 0.9678, Validation Loss: 2.7441, Validation Percentage:44.3137%,Test Loss: 3.0938, Test Percentage:41.1124%\n",
            "Epoch [990/1200], Training Loss: 0.9338, Validation Loss: 3.0269, Validation Percentage:45.8824%,Test Loss: 3.3768, Test Percentage:42.3321%\n",
            "Epoch [991/1200], Training Loss: 0.9233, Validation Loss: 3.3093, Validation Percentage:44.9020%,Test Loss: 3.5049, Test Percentage:42.4947%\n",
            "Epoch [992/1200], Training Loss: 0.8933, Validation Loss: 2.9353, Validation Percentage:44.7059%,Test Loss: 3.3320, Test Percentage:41.1612%\n",
            "Epoch [993/1200], Training Loss: 0.9054, Validation Loss: 2.7878, Validation Percentage:44.9020%,Test Loss: 3.1947, Test Percentage:41.1124%\n",
            "Epoch [994/1200], Training Loss: 0.9013, Validation Loss: 3.0181, Validation Percentage:44.9020%,Test Loss: 3.2919, Test Percentage:40.9172%\n",
            "Epoch [995/1200], Training Loss: 0.9257, Validation Loss: 3.1570, Validation Percentage:43.0392%,Test Loss: 3.3061, Test Percentage:42.1207%\n",
            "Epoch [996/1200], Training Loss: 0.9875, Validation Loss: 2.9001, Validation Percentage:42.7451%,Test Loss: 3.1530, Test Percentage:41.5027%\n",
            "Epoch [997/1200], Training Loss: 0.8373, Validation Loss: 3.0813, Validation Percentage:42.6471%,Test Loss: 3.2259, Test Percentage:43.0964%\n",
            "Epoch [998/1200], Training Loss: 0.9656, Validation Loss: 2.8125, Validation Percentage:45.2941%,Test Loss: 3.1018, Test Percentage:41.3726%\n",
            "Epoch [999/1200], Training Loss: 0.8442, Validation Loss: 3.1019, Validation Percentage:43.3333%,Test Loss: 3.1603, Test Percentage:43.1615%\n",
            "Epoch [1000/1200], Training Loss: 0.8097, Validation Loss: 2.9660, Validation Percentage:44.2157%,Test Loss: 3.3577, Test Percentage:41.9418%\n",
            "Epoch [1001/1200], Training Loss: 0.9483, Validation Loss: 3.1657, Validation Percentage:46.6667%,Test Loss: 3.6281, Test Percentage:42.5110%\n",
            "Epoch [1002/1200], Training Loss: 0.8550, Validation Loss: 3.0924, Validation Percentage:45.9804%,Test Loss: 3.3166, Test Percentage:44.1047%\n",
            "Epoch [1003/1200], Training Loss: 0.8651, Validation Loss: 3.1379, Validation Percentage:44.4118%,Test Loss: 3.3959, Test Percentage:42.3158%\n",
            "Epoch [1004/1200], Training Loss: 0.8578, Validation Loss: 2.8622, Validation Percentage:45.8824%,Test Loss: 3.2733, Test Percentage:43.2428%\n",
            "Epoch [1005/1200], Training Loss: 0.8751, Validation Loss: 2.9770, Validation Percentage:46.5686%,Test Loss: 3.3328, Test Percentage:42.8200%\n",
            "Epoch [1006/1200], Training Loss: 0.8668, Validation Loss: 3.3007, Validation Percentage:44.6078%,Test Loss: 3.6337, Test Percentage:42.2508%\n",
            "Epoch [1007/1200], Training Loss: 0.8673, Validation Loss: 3.0549, Validation Percentage:45.0980%,Test Loss: 3.2667, Test Percentage:43.3079%\n",
            "Epoch [1008/1200], Training Loss: 0.9490, Validation Loss: 3.0785, Validation Percentage:44.0196%,Test Loss: 3.2467, Test Percentage:42.4459%\n",
            "Epoch [1009/1200], Training Loss: 0.7719, Validation Loss: 2.8485, Validation Percentage:46.4706%,Test Loss: 3.2654, Test Percentage:42.8362%\n",
            "Epoch [1010/1200], Training Loss: 0.9290, Validation Loss: 2.9156, Validation Percentage:41.8627%,Test Loss: 3.3538, Test Percentage:40.4781%\n",
            "Epoch [1011/1200], Training Loss: 0.8257, Validation Loss: 3.1354, Validation Percentage:44.3137%,Test Loss: 3.3796, Test Percentage:43.1615%\n",
            "Epoch [1012/1200], Training Loss: 0.8140, Validation Loss: 2.9687, Validation Percentage:43.4314%,Test Loss: 3.2752, Test Percentage:41.5189%\n",
            "Epoch [1013/1200], Training Loss: 0.9015, Validation Loss: 3.1923, Validation Percentage:44.1176%,Test Loss: 3.4855, Test Percentage:40.7546%\n",
            "Epoch [1014/1200], Training Loss: 0.8426, Validation Loss: 3.0493, Validation Percentage:48.2353%,Test Loss: 3.4005, Test Percentage:44.5438%\n",
            "Epoch [1015/1200], Training Loss: 0.8828, Validation Loss: 3.2153, Validation Percentage:43.2353%,Test Loss: 3.4212, Test Percentage:41.6328%\n",
            "Epoch [1016/1200], Training Loss: 0.9158, Validation Loss: 2.9330, Validation Percentage:46.1765%,Test Loss: 3.2410, Test Percentage:42.9989%\n",
            "Epoch [1017/1200], Training Loss: 0.8898, Validation Loss: 3.1590, Validation Percentage:45.9804%,Test Loss: 3.4537, Test Percentage:42.2020%\n",
            "Epoch [1018/1200], Training Loss: 0.7876, Validation Loss: 3.3669, Validation Percentage:44.5098%,Test Loss: 3.4765, Test Percentage:42.6248%\n",
            "Epoch [1019/1200], Training Loss: 0.8697, Validation Loss: 3.4678, Validation Percentage:41.8627%,Test Loss: 3.5046, Test Percentage:42.5598%\n",
            "Epoch [1020/1200], Training Loss: 0.9364, Validation Loss: 2.9963, Validation Percentage:43.5294%,Test Loss: 3.2798, Test Percentage:41.3238%\n",
            "Epoch [1021/1200], Training Loss: 0.9253, Validation Loss: 3.0240, Validation Percentage:44.7059%,Test Loss: 3.3015, Test Percentage:41.5515%\n",
            "Epoch [1022/1200], Training Loss: 0.9519, Validation Loss: 3.0430, Validation Percentage:43.4314%,Test Loss: 3.2840, Test Percentage:42.4622%\n",
            "Epoch [1023/1200], Training Loss: 0.8571, Validation Loss: 3.0553, Validation Percentage:46.3725%,Test Loss: 3.4830, Test Percentage:43.5843%\n",
            "Epoch [1024/1200], Training Loss: 0.9725, Validation Loss: 2.9813, Validation Percentage:44.6078%,Test Loss: 3.4698, Test Percentage:41.2587%\n",
            "Epoch [1025/1200], Training Loss: 0.9080, Validation Loss: 3.3213, Validation Percentage:43.0392%,Test Loss: 3.6714, Test Percentage:41.1286%\n",
            "Epoch [1026/1200], Training Loss: 0.8027, Validation Loss: 3.0534, Validation Percentage:43.7255%,Test Loss: 3.2787, Test Percentage:42.1532%\n",
            "Epoch [1027/1200], Training Loss: 0.8735, Validation Loss: 2.9429, Validation Percentage:45.1961%,Test Loss: 3.2930, Test Percentage:43.4867%\n",
            "Epoch [1028/1200], Training Loss: 0.7909, Validation Loss: 3.0717, Validation Percentage:44.9020%,Test Loss: 3.5114, Test Percentage:43.5681%\n",
            "Epoch [1029/1200], Training Loss: 0.7687, Validation Loss: 3.2081, Validation Percentage:43.4314%,Test Loss: 3.5306, Test Percentage:41.3401%\n",
            "Epoch [1030/1200], Training Loss: 0.9405, Validation Loss: 3.2001, Validation Percentage:45.0980%,Test Loss: 3.5835, Test Percentage:41.9093%\n",
            "Epoch [1031/1200], Training Loss: 0.8191, Validation Loss: 2.9957, Validation Percentage:44.1176%,Test Loss: 3.2390, Test Percentage:42.5110%\n",
            "Epoch [1032/1200], Training Loss: 0.8138, Validation Loss: 3.0001, Validation Percentage:45.0980%,Test Loss: 3.3719, Test Percentage:41.9906%\n",
            "Epoch [1033/1200], Training Loss: 0.9409, Validation Loss: 3.1599, Validation Percentage:45.5882%,Test Loss: 3.5344, Test Percentage:42.7549%\n",
            "Epoch [1034/1200], Training Loss: 0.8434, Validation Loss: 3.0875, Validation Percentage:44.9020%,Test Loss: 3.4637, Test Percentage:42.5923%\n",
            "Epoch [1035/1200], Training Loss: 0.9389, Validation Loss: 2.9754, Validation Percentage:43.5294%,Test Loss: 3.3433, Test Percentage:42.0881%\n",
            "Epoch [1036/1200], Training Loss: 0.9454, Validation Loss: 2.9285, Validation Percentage:43.1373%,Test Loss: 3.0889, Test Percentage:42.5923%\n",
            "Epoch [1037/1200], Training Loss: 0.8399, Validation Loss: 2.9198, Validation Percentage:45.4902%,Test Loss: 3.2809, Test Percentage:42.5272%\n",
            "Epoch [1038/1200], Training Loss: 0.9238, Validation Loss: 3.3096, Validation Percentage:41.8627%,Test Loss: 3.5720, Test Percentage:41.3726%\n",
            "Epoch [1039/1200], Training Loss: 1.0260, Validation Loss: 2.8847, Validation Percentage:42.8431%,Test Loss: 3.1063, Test Percentage:40.8684%\n",
            "Epoch [1040/1200], Training Loss: 0.9074, Validation Loss: 2.9336, Validation Percentage:44.4118%,Test Loss: 3.2008, Test Percentage:43.6331%\n",
            "Epoch [1041/1200], Training Loss: 0.8420, Validation Loss: 3.2825, Validation Percentage:42.3529%,Test Loss: 3.5607, Test Percentage:41.1449%\n",
            "Epoch [1042/1200], Training Loss: 0.8963, Validation Loss: 3.0914, Validation Percentage:44.4118%,Test Loss: 3.5027, Test Percentage:41.9093%\n",
            "Epoch [1043/1200], Training Loss: 0.8807, Validation Loss: 3.1402, Validation Percentage:44.2157%,Test Loss: 3.2732, Test Percentage:43.0314%\n",
            "Epoch [1044/1200], Training Loss: 0.9124, Validation Loss: 3.0805, Validation Percentage:44.4118%,Test Loss: 3.4211, Test Percentage:40.0065%\n",
            "Epoch [1045/1200], Training Loss: 0.8535, Validation Loss: 3.3645, Validation Percentage:42.6471%,Test Loss: 3.5420, Test Percentage:43.0964%\n",
            "Epoch [1046/1200], Training Loss: 0.8634, Validation Loss: 2.9615, Validation Percentage:47.4510%,Test Loss: 3.4541, Test Percentage:42.7224%\n",
            "Epoch [1047/1200], Training Loss: 0.7953, Validation Loss: 2.9810, Validation Percentage:48.3333%,Test Loss: 3.3971, Test Percentage:41.9580%\n",
            "Epoch [1048/1200], Training Loss: 0.7641, Validation Loss: 3.3277, Validation Percentage:44.8039%,Test Loss: 3.7040, Test Percentage:41.8279%\n",
            "Epoch [1049/1200], Training Loss: 0.8016, Validation Loss: 3.0544, Validation Percentage:42.1569%,Test Loss: 3.2425, Test Percentage:42.4785%\n",
            "Epoch [1050/1200], Training Loss: 0.8447, Validation Loss: 3.1153, Validation Percentage:42.7451%,Test Loss: 3.4276, Test Percentage:42.0394%\n",
            "Epoch [1051/1200], Training Loss: 0.7192, Validation Loss: 3.2420, Validation Percentage:41.7647%,Test Loss: 3.5243, Test Percentage:40.9497%\n",
            "Epoch [1052/1200], Training Loss: 0.9212, Validation Loss: 3.1795, Validation Percentage:43.7255%,Test Loss: 3.4259, Test Percentage:42.5110%\n",
            "Epoch [1053/1200], Training Loss: 0.8682, Validation Loss: 2.9607, Validation Percentage:45.5882%,Test Loss: 3.2054, Test Percentage:41.1937%\n",
            "Epoch [1054/1200], Training Loss: 0.8274, Validation Loss: 2.8752, Validation Percentage:45.0000%,Test Loss: 3.2322, Test Percentage:41.8117%\n",
            "Epoch [1055/1200], Training Loss: 0.9055, Validation Loss: 2.9016, Validation Percentage:43.3333%,Test Loss: 3.2595, Test Percentage:42.5435%\n",
            "Epoch [1056/1200], Training Loss: 0.8656, Validation Loss: 3.1171, Validation Percentage:46.0784%,Test Loss: 3.3906, Test Percentage:42.4297%\n",
            "Epoch [1057/1200], Training Loss: 0.9020, Validation Loss: 2.9444, Validation Percentage:46.8627%,Test Loss: 3.2253, Test Percentage:42.2833%\n",
            "Epoch [1058/1200], Training Loss: 0.8543, Validation Loss: 3.0623, Validation Percentage:44.4118%,Test Loss: 3.4688, Test Percentage:40.7709%\n",
            "Epoch [1059/1200], Training Loss: 0.9675, Validation Loss: 3.1768, Validation Percentage:42.9412%,Test Loss: 3.2707, Test Percentage:41.9906%\n",
            "Epoch [1060/1200], Training Loss: 0.8766, Validation Loss: 3.1468, Validation Percentage:45.3922%,Test Loss: 3.2689, Test Percentage:43.1452%\n",
            "Epoch [1061/1200], Training Loss: 0.7892, Validation Loss: 3.0715, Validation Percentage:43.4314%,Test Loss: 3.2454, Test Percentage:43.2265%\n",
            "Epoch [1062/1200], Training Loss: 0.8406, Validation Loss: 3.0967, Validation Percentage:43.4314%,Test Loss: 3.3549, Test Percentage:41.7466%\n",
            "Epoch [1063/1200], Training Loss: 0.9566, Validation Loss: 3.0492, Validation Percentage:44.1176%,Test Loss: 3.3969, Test Percentage:41.8767%\n",
            "Epoch [1064/1200], Training Loss: 0.9397, Validation Loss: 2.9775, Validation Percentage:45.1961%,Test Loss: 3.2946, Test Percentage:41.6003%\n",
            "Epoch [1065/1200], Training Loss: 0.7595, Validation Loss: 3.0474, Validation Percentage:43.6275%,Test Loss: 3.3332, Test Percentage:42.1532%\n",
            "Epoch [1066/1200], Training Loss: 0.8576, Validation Loss: 3.1367, Validation Percentage:44.7059%,Test Loss: 3.4186, Test Percentage:41.0473%\n",
            "Epoch [1067/1200], Training Loss: 0.9305, Validation Loss: 2.9961, Validation Percentage:44.5098%,Test Loss: 3.3398, Test Percentage:42.1369%\n",
            "Epoch [1068/1200], Training Loss: 0.8898, Validation Loss: 2.9642, Validation Percentage:41.9608%,Test Loss: 3.2440, Test Percentage:42.8525%\n",
            "Epoch [1069/1200], Training Loss: 0.7501, Validation Loss: 3.5363, Validation Percentage:43.7255%,Test Loss: 3.5088, Test Percentage:43.6656%\n",
            "Epoch [1070/1200], Training Loss: 0.8068, Validation Loss: 3.0897, Validation Percentage:44.4118%,Test Loss: 3.2790, Test Percentage:42.3971%\n",
            "Epoch [1071/1200], Training Loss: 0.9451, Validation Loss: 3.0751, Validation Percentage:44.6078%,Test Loss: 3.5352, Test Percentage:41.2262%\n",
            "Epoch [1072/1200], Training Loss: 0.8993, Validation Loss: 3.3298, Validation Percentage:43.4314%,Test Loss: 3.4335, Test Percentage:42.6736%\n",
            "Epoch [1073/1200], Training Loss: 0.8381, Validation Loss: 3.2740, Validation Percentage:44.8039%,Test Loss: 3.4893, Test Percentage:43.5681%\n",
            "Epoch [1074/1200], Training Loss: 0.9871, Validation Loss: 3.1011, Validation Percentage:42.9412%,Test Loss: 3.1779, Test Percentage:42.3809%\n",
            "Epoch [1075/1200], Training Loss: 0.9417, Validation Loss: 3.0757, Validation Percentage:43.1373%,Test Loss: 3.2795, Test Percentage:41.5027%\n",
            "Epoch [1076/1200], Training Loss: 0.8993, Validation Loss: 2.8622, Validation Percentage:45.0000%,Test Loss: 3.1993, Test Percentage:41.3401%\n",
            "Epoch [1077/1200], Training Loss: 0.9387, Validation Loss: 2.7617, Validation Percentage:46.0784%,Test Loss: 3.0692, Test Percentage:42.8200%\n",
            "Epoch [1078/1200], Training Loss: 0.8639, Validation Loss: 3.0673, Validation Percentage:45.3922%,Test Loss: 3.4459, Test Percentage:42.9175%\n",
            "Epoch [1079/1200], Training Loss: 0.8505, Validation Loss: 2.9381, Validation Percentage:44.9020%,Test Loss: 3.3737, Test Percentage:42.1044%\n",
            "Epoch [1080/1200], Training Loss: 0.8846, Validation Loss: 3.1914, Validation Percentage:46.4706%,Test Loss: 3.3550, Test Percentage:43.4705%\n",
            "Epoch [1081/1200], Training Loss: 0.8890, Validation Loss: 3.1690, Validation Percentage:43.7255%,Test Loss: 3.4289, Test Percentage:42.0231%\n",
            "Epoch [1082/1200], Training Loss: 0.8672, Validation Loss: 3.2778, Validation Percentage:43.4314%,Test Loss: 3.4636, Test Percentage:41.7954%\n",
            "Epoch [1083/1200], Training Loss: 0.9645, Validation Loss: 2.9698, Validation Percentage:42.8431%,Test Loss: 3.3082, Test Percentage:40.9985%\n",
            "Epoch [1084/1200], Training Loss: 0.8972, Validation Loss: 3.0438, Validation Percentage:45.6863%,Test Loss: 3.2757, Test Percentage:41.8117%\n",
            "Epoch [1085/1200], Training Loss: 0.8980, Validation Loss: 2.9609, Validation Percentage:43.5294%,Test Loss: 3.2266, Test Percentage:42.1857%\n",
            "Epoch [1086/1200], Training Loss: 0.8465, Validation Loss: 3.2827, Validation Percentage:45.4902%,Test Loss: 3.5337, Test Percentage:43.6168%\n",
            "Epoch [1087/1200], Training Loss: 0.8669, Validation Loss: 3.2163, Validation Percentage:44.1176%,Test Loss: 3.5599, Test Percentage:41.6165%\n",
            "Epoch [1088/1200], Training Loss: 0.8477, Validation Loss: 3.1361, Validation Percentage:41.1765%,Test Loss: 3.4282, Test Percentage:41.2425%\n",
            "Epoch [1089/1200], Training Loss: 0.9927, Validation Loss: 3.2854, Validation Percentage:43.4314%,Test Loss: 3.4840, Test Percentage:43.1615%\n",
            "Epoch [1090/1200], Training Loss: 0.8721, Validation Loss: 2.9730, Validation Percentage:43.9216%,Test Loss: 3.2959, Test Percentage:42.3158%\n",
            "Epoch [1091/1200], Training Loss: 0.9047, Validation Loss: 2.9110, Validation Percentage:46.6667%,Test Loss: 3.2042, Test Percentage:42.6411%\n",
            "Epoch [1092/1200], Training Loss: 0.9499, Validation Loss: 3.3697, Validation Percentage:45.2941%,Test Loss: 3.8118, Test Percentage:39.8764%\n",
            "Epoch [1093/1200], Training Loss: 0.9463, Validation Loss: 2.8856, Validation Percentage:47.9412%,Test Loss: 3.2663, Test Percentage:43.4867%\n",
            "Epoch [1094/1200], Training Loss: 0.7736, Validation Loss: 3.3781, Validation Percentage:45.4902%,Test Loss: 3.5331, Test Percentage:43.2916%\n",
            "Epoch [1095/1200], Training Loss: 0.8155, Validation Loss: 3.0144, Validation Percentage:44.4118%,Test Loss: 3.3825, Test Percentage:40.9497%\n",
            "Epoch [1096/1200], Training Loss: 0.9255, Validation Loss: 3.0974, Validation Percentage:43.4314%,Test Loss: 3.3551, Test Percentage:42.0719%\n",
            "Epoch [1097/1200], Training Loss: 0.9120, Validation Loss: 3.0744, Validation Percentage:44.7059%,Test Loss: 3.4086, Test Percentage:41.8442%\n",
            "Epoch [1098/1200], Training Loss: 0.8950, Validation Loss: 2.9972, Validation Percentage:43.4314%,Test Loss: 3.2738, Test Percentage:41.6978%\n",
            "Epoch [1099/1200], Training Loss: 0.8533, Validation Loss: 3.0387, Validation Percentage:44.5098%,Test Loss: 3.3643, Test Percentage:42.9826%\n",
            "Epoch [1100/1200], Training Loss: 0.8258, Validation Loss: 3.1187, Validation Percentage:43.1373%,Test Loss: 3.4322, Test Percentage:42.3646%\n",
            "Epoch [1101/1200], Training Loss: 0.7917, Validation Loss: 3.1755, Validation Percentage:44.6078%,Test Loss: 3.5407, Test Percentage:41.5515%\n",
            "Epoch [1102/1200], Training Loss: 0.7580, Validation Loss: 3.2756, Validation Percentage:45.1961%,Test Loss: 3.6883, Test Percentage:43.1127%\n",
            "Epoch [1103/1200], Training Loss: 0.8713, Validation Loss: 3.0938, Validation Percentage:43.4314%,Test Loss: 3.4953, Test Percentage:41.6816%\n",
            "Epoch [1104/1200], Training Loss: 0.8750, Validation Loss: 2.9337, Validation Percentage:42.4510%,Test Loss: 3.0625, Test Percentage:42.7874%\n",
            "Epoch [1105/1200], Training Loss: 0.8436, Validation Loss: 3.0926, Validation Percentage:44.9020%,Test Loss: 3.3471, Test Percentage:42.9175%\n",
            "Epoch [1106/1200], Training Loss: 0.7135, Validation Loss: 3.0468, Validation Percentage:43.2353%,Test Loss: 3.4151, Test Percentage:42.1369%\n",
            "Epoch [1107/1200], Training Loss: 0.7712, Validation Loss: 3.1060, Validation Percentage:44.6078%,Test Loss: 3.4833, Test Percentage:42.9175%\n",
            "Epoch [1108/1200], Training Loss: 0.8281, Validation Loss: 3.2267, Validation Percentage:42.5490%,Test Loss: 3.7073, Test Percentage:39.8276%\n",
            "Epoch [1109/1200], Training Loss: 0.8650, Validation Loss: 2.9307, Validation Percentage:42.0588%,Test Loss: 3.2909, Test Percentage:41.2913%\n",
            "Epoch [1110/1200], Training Loss: 0.8151, Validation Loss: 3.2279, Validation Percentage:41.4706%,Test Loss: 3.5525, Test Percentage:40.6245%\n",
            "Epoch [1111/1200], Training Loss: 0.9396, Validation Loss: 2.9635, Validation Percentage:44.6078%,Test Loss: 3.3114, Test Percentage:41.6165%\n",
            "Epoch [1112/1200], Training Loss: 0.9306, Validation Loss: 3.3067, Validation Percentage:47.7451%,Test Loss: 3.6373, Test Percentage:42.0068%\n",
            "Epoch [1113/1200], Training Loss: 0.9067, Validation Loss: 2.9770, Validation Percentage:43.1373%,Test Loss: 3.2558, Test Percentage:42.4459%\n",
            "Epoch [1114/1200], Training Loss: 0.8674, Validation Loss: 2.8793, Validation Percentage:46.2745%,Test Loss: 3.2449, Test Percentage:42.6899%\n",
            "Epoch [1115/1200], Training Loss: 0.8667, Validation Loss: 2.9474, Validation Percentage:43.2353%,Test Loss: 3.1585, Test Percentage:42.0556%\n",
            "Epoch [1116/1200], Training Loss: 0.8220, Validation Loss: 2.9314, Validation Percentage:44.9020%,Test Loss: 3.2436, Test Percentage:42.7387%\n",
            "Epoch [1117/1200], Training Loss: 0.7966, Validation Loss: 3.4367, Validation Percentage:44.8039%,Test Loss: 3.5821, Test Percentage:42.2670%\n",
            "Epoch [1118/1200], Training Loss: 0.8781, Validation Loss: 2.9561, Validation Percentage:47.9412%,Test Loss: 3.4557, Test Percentage:42.1044%\n",
            "Epoch [1119/1200], Training Loss: 0.7377, Validation Loss: 3.0989, Validation Percentage:44.9020%,Test Loss: 3.3515, Test Percentage:44.1210%\n",
            "Epoch [1120/1200], Training Loss: 0.7563, Validation Loss: 3.2988, Validation Percentage:42.5490%,Test Loss: 3.7366, Test Percentage:41.1286%\n",
            "Epoch [1121/1200], Training Loss: 0.8598, Validation Loss: 3.2427, Validation Percentage:43.0392%,Test Loss: 3.6254, Test Percentage:41.4214%\n",
            "Epoch [1122/1200], Training Loss: 0.8554, Validation Loss: 3.1599, Validation Percentage:41.4706%,Test Loss: 3.3332, Test Percentage:40.8522%\n",
            "Epoch [1123/1200], Training Loss: 0.9520, Validation Loss: 3.2148, Validation Percentage:45.4902%,Test Loss: 3.6881, Test Percentage:42.5760%\n",
            "Epoch [1124/1200], Training Loss: 0.9417, Validation Loss: 3.2079, Validation Percentage:43.2353%,Test Loss: 3.3941, Test Percentage:41.5515%\n",
            "Epoch [1125/1200], Training Loss: 0.7679, Validation Loss: 3.0699, Validation Percentage:45.0980%,Test Loss: 3.4657, Test Percentage:42.0394%\n",
            "Epoch [1126/1200], Training Loss: 0.9443, Validation Loss: 3.2242, Validation Percentage:45.5882%,Test Loss: 3.5822, Test Percentage:42.0394%\n",
            "Epoch [1127/1200], Training Loss: 0.9654, Validation Loss: 3.0507, Validation Percentage:44.0196%,Test Loss: 3.4736, Test Percentage:41.9255%\n",
            "Epoch [1128/1200], Training Loss: 0.8583, Validation Loss: 2.9529, Validation Percentage:43.4314%,Test Loss: 3.4749, Test Percentage:40.5432%\n",
            "Epoch [1129/1200], Training Loss: 0.8705, Validation Loss: 2.9826, Validation Percentage:45.9804%,Test Loss: 3.2900, Test Percentage:41.2750%\n",
            "Epoch [1130/1200], Training Loss: 0.8315, Validation Loss: 3.0569, Validation Percentage:45.0000%,Test Loss: 3.3745, Test Percentage:41.2913%\n",
            "Epoch [1131/1200], Training Loss: 0.8544, Validation Loss: 3.1583, Validation Percentage:41.9608%,Test Loss: 3.3655, Test Percentage:42.1857%\n",
            "Epoch [1132/1200], Training Loss: 0.8061, Validation Loss: 2.9631, Validation Percentage:43.7255%,Test Loss: 3.2109, Test Percentage:42.1369%\n",
            "Epoch [1133/1200], Training Loss: 0.8907, Validation Loss: 2.9534, Validation Percentage:45.9804%,Test Loss: 3.2973, Test Percentage:42.4134%\n",
            "Epoch [1134/1200], Training Loss: 0.8711, Validation Loss: 3.1022, Validation Percentage:45.1961%,Test Loss: 3.4076, Test Percentage:42.9663%\n",
            "Epoch [1135/1200], Training Loss: 0.8266, Validation Loss: 3.1882, Validation Percentage:42.5490%,Test Loss: 3.5395, Test Percentage:42.2670%\n",
            "Epoch [1136/1200], Training Loss: 0.7329, Validation Loss: 3.3083, Validation Percentage:45.7843%,Test Loss: 3.7322, Test Percentage:42.2996%\n",
            "Epoch [1137/1200], Training Loss: 0.9148, Validation Loss: 3.1284, Validation Percentage:44.2157%,Test Loss: 3.3286, Test Percentage:42.6411%\n",
            "Epoch [1138/1200], Training Loss: 0.9512, Validation Loss: 3.1278, Validation Percentage:42.8431%,Test Loss: 3.4184, Test Percentage:40.8847%\n",
            "Epoch [1139/1200], Training Loss: 0.8701, Validation Loss: 3.3451, Validation Percentage:44.5098%,Test Loss: 3.5526, Test Percentage:42.6248%\n",
            "Epoch [1140/1200], Training Loss: 0.7906, Validation Loss: 3.0680, Validation Percentage:44.8039%,Test Loss: 3.3413, Test Percentage:42.2833%\n",
            "Epoch [1141/1200], Training Loss: 0.8731, Validation Loss: 2.9600, Validation Percentage:43.9216%,Test Loss: 3.1444, Test Percentage:42.0719%\n",
            "Epoch [1142/1200], Training Loss: 0.9068, Validation Loss: 2.7738, Validation Percentage:46.7647%,Test Loss: 3.2518, Test Percentage:42.2508%\n",
            "Epoch [1143/1200], Training Loss: 0.8554, Validation Loss: 3.3225, Validation Percentage:41.9608%,Test Loss: 3.4524, Test Percentage:40.8847%\n",
            "Epoch [1144/1200], Training Loss: 0.8972, Validation Loss: 2.9527, Validation Percentage:44.2157%,Test Loss: 3.4049, Test Percentage:41.2100%\n",
            "Epoch [1145/1200], Training Loss: 0.7957, Validation Loss: 3.1766, Validation Percentage:45.6863%,Test Loss: 3.4236, Test Percentage:44.1860%\n",
            "Epoch [1146/1200], Training Loss: 0.8841, Validation Loss: 3.2383, Validation Percentage:42.7451%,Test Loss: 3.5158, Test Percentage:41.9906%\n",
            "Epoch [1147/1200], Training Loss: 0.8488, Validation Loss: 3.0090, Validation Percentage:48.6275%,Test Loss: 3.4009, Test Percentage:43.5030%\n",
            "Epoch [1148/1200], Training Loss: 0.7702, Validation Loss: 2.9187, Validation Percentage:45.4902%,Test Loss: 3.4094, Test Percentage:42.4947%\n",
            "Epoch [1149/1200], Training Loss: 0.8650, Validation Loss: 3.2277, Validation Percentage:44.9020%,Test Loss: 3.4932, Test Percentage:43.3892%\n",
            "Epoch [1150/1200], Training Loss: 0.8794, Validation Loss: 3.0486, Validation Percentage:46.7647%,Test Loss: 3.4721, Test Percentage:42.7224%\n",
            "Epoch [1151/1200], Training Loss: 0.7760, Validation Loss: 2.9674, Validation Percentage:45.5882%,Test Loss: 3.3224, Test Percentage:43.2103%\n",
            "Epoch [1152/1200], Training Loss: 0.6985, Validation Loss: 3.3570, Validation Percentage:41.8627%,Test Loss: 3.4907, Test Percentage:41.6165%\n",
            "Epoch [1153/1200], Training Loss: 0.9675, Validation Loss: 2.8672, Validation Percentage:43.6275%,Test Loss: 3.2166, Test Percentage:41.5840%\n",
            "Epoch [1154/1200], Training Loss: 0.7739, Validation Loss: 3.3523, Validation Percentage:43.8235%,Test Loss: 3.6129, Test Percentage:42.3646%\n",
            "Epoch [1155/1200], Training Loss: 0.8842, Validation Loss: 3.1612, Validation Percentage:44.7059%,Test Loss: 3.4116, Test Percentage:43.3241%\n",
            "Epoch [1156/1200], Training Loss: 0.9492, Validation Loss: 2.7979, Validation Percentage:47.6471%,Test Loss: 3.3647, Test Percentage:40.8359%\n",
            "Epoch [1157/1200], Training Loss: 0.7847, Validation Loss: 3.1024, Validation Percentage:44.5098%,Test Loss: 3.4207, Test Percentage:40.3480%\n",
            "Epoch [1158/1200], Training Loss: 0.8284, Validation Loss: 3.1683, Validation Percentage:43.1373%,Test Loss: 3.6148, Test Percentage:41.5027%\n",
            "Epoch [1159/1200], Training Loss: 0.8752, Validation Loss: 2.9762, Validation Percentage:43.6275%,Test Loss: 3.3287, Test Percentage:40.9985%\n",
            "Epoch [1160/1200], Training Loss: 0.8373, Validation Loss: 2.8956, Validation Percentage:44.3137%,Test Loss: 3.0947, Test Percentage:42.9826%\n",
            "Epoch [1161/1200], Training Loss: 0.8849, Validation Loss: 2.9910, Validation Percentage:44.0196%,Test Loss: 3.2920, Test Percentage:42.7874%\n",
            "Epoch [1162/1200], Training Loss: 0.8068, Validation Loss: 3.0187, Validation Percentage:44.7059%,Test Loss: 3.5285, Test Percentage:41.5677%\n",
            "Epoch [1163/1200], Training Loss: 0.8022, Validation Loss: 3.0030, Validation Percentage:43.8235%,Test Loss: 3.2886, Test Percentage:40.7546%\n",
            "Epoch [1164/1200], Training Loss: 0.8775, Validation Loss: 3.3817, Validation Percentage:45.2941%,Test Loss: 3.5600, Test Percentage:42.4459%\n",
            "Epoch [1165/1200], Training Loss: 0.9011, Validation Loss: 2.9074, Validation Percentage:44.3137%,Test Loss: 3.1565, Test Percentage:41.2425%\n",
            "Epoch [1166/1200], Training Loss: 0.8154, Validation Loss: 3.2236, Validation Percentage:44.9020%,Test Loss: 3.4443, Test Percentage:41.5189%\n",
            "Epoch [1167/1200], Training Loss: 0.7918, Validation Loss: 3.2297, Validation Percentage:44.0196%,Test Loss: 3.5907, Test Percentage:42.0068%\n",
            "Epoch [1168/1200], Training Loss: 0.9193, Validation Loss: 2.9381, Validation Percentage:49.3137%,Test Loss: 3.3974, Test Percentage:44.1047%\n",
            "Epoch [1169/1200], Training Loss: 0.8859, Validation Loss: 3.1603, Validation Percentage:44.9020%,Test Loss: 3.5112, Test Percentage:42.3646%\n",
            "Epoch [1170/1200], Training Loss: 0.8793, Validation Loss: 3.2804, Validation Percentage:46.7647%,Test Loss: 3.6779, Test Percentage:43.5030%\n",
            "Epoch [1171/1200], Training Loss: 0.8077, Validation Loss: 3.1320, Validation Percentage:44.0196%,Test Loss: 3.4790, Test Percentage:42.1532%\n",
            "Epoch [1172/1200], Training Loss: 0.8049, Validation Loss: 3.4279, Validation Percentage:42.5490%,Test Loss: 3.5518, Test Percentage:41.3563%\n",
            "Epoch [1173/1200], Training Loss: 0.8310, Validation Loss: 2.9489, Validation Percentage:44.7059%,Test Loss: 3.2702, Test Percentage:42.4622%\n",
            "Epoch [1174/1200], Training Loss: 0.8650, Validation Loss: 3.1359, Validation Percentage:46.7647%,Test Loss: 3.4539, Test Percentage:43.8283%\n",
            "Epoch [1175/1200], Training Loss: 0.8811, Validation Loss: 3.1285, Validation Percentage:44.8039%,Test Loss: 3.4529, Test Percentage:42.0719%\n",
            "Epoch [1176/1200], Training Loss: 0.8943, Validation Loss: 3.1536, Validation Percentage:45.4902%,Test Loss: 3.4122, Test Percentage:43.2103%\n",
            "Epoch [1177/1200], Training Loss: 0.8135, Validation Loss: 3.5910, Validation Percentage:44.5098%,Test Loss: 3.8455, Test Percentage:42.1532%\n",
            "Epoch [1178/1200], Training Loss: 0.8783, Validation Loss: 2.9657, Validation Percentage:46.5686%,Test Loss: 3.3693, Test Percentage:43.0151%\n",
            "Epoch [1179/1200], Training Loss: 0.8642, Validation Loss: 2.7803, Validation Percentage:47.1569%,Test Loss: 3.2844, Test Percentage:41.3726%\n",
            "Epoch [1180/1200], Training Loss: 0.7672, Validation Loss: 3.0264, Validation Percentage:46.2745%,Test Loss: 3.3759, Test Percentage:43.1290%\n",
            "Epoch [1181/1200], Training Loss: 0.7414, Validation Loss: 3.2346, Validation Percentage:43.8235%,Test Loss: 3.4923, Test Percentage:42.2670%\n",
            "Epoch [1182/1200], Training Loss: 0.7848, Validation Loss: 2.9804, Validation Percentage:43.4314%,Test Loss: 3.2574, Test Percentage:42.2833%\n",
            "Epoch [1183/1200], Training Loss: 0.8570, Validation Loss: 2.9774, Validation Percentage:43.6275%,Test Loss: 3.4240, Test Percentage:40.5920%\n",
            "Epoch [1184/1200], Training Loss: 0.8699, Validation Loss: 3.0087, Validation Percentage:45.5882%,Test Loss: 3.4259, Test Percentage:40.9172%\n",
            "Epoch [1185/1200], Training Loss: 0.7627, Validation Loss: 3.3383, Validation Percentage:44.9020%,Test Loss: 3.6887, Test Percentage:43.6819%\n",
            "Epoch [1186/1200], Training Loss: 0.8580, Validation Loss: 2.8721, Validation Percentage:45.5882%,Test Loss: 3.1769, Test Percentage:42.4947%\n",
            "Epoch [1187/1200], Training Loss: 0.7499, Validation Loss: 3.0791, Validation Percentage:46.6667%,Test Loss: 3.3884, Test Percentage:42.3971%\n",
            "Epoch [1188/1200], Training Loss: 0.7656, Validation Loss: 3.2606, Validation Percentage:44.2157%,Test Loss: 3.5385, Test Percentage:44.7065%\n",
            "Epoch [1189/1200], Training Loss: 0.8617, Validation Loss: 2.9586, Validation Percentage:44.9020%,Test Loss: 3.3431, Test Percentage:42.3646%\n",
            "Epoch [1190/1200], Training Loss: 0.9113, Validation Loss: 3.2064, Validation Percentage:42.6471%,Test Loss: 3.6162, Test Percentage:41.1286%\n",
            "Epoch [1191/1200], Training Loss: 0.8987, Validation Loss: 3.1024, Validation Percentage:44.5098%,Test Loss: 3.5332, Test Percentage:42.4947%\n",
            "Epoch [1192/1200], Training Loss: 0.9094, Validation Loss: 2.9251, Validation Percentage:45.5882%,Test Loss: 3.1798, Test Percentage:42.6411%\n",
            "Epoch [1193/1200], Training Loss: 0.8162, Validation Loss: 3.0910, Validation Percentage:43.9216%,Test Loss: 3.4044, Test Percentage:42.3483%\n",
            "Epoch [1194/1200], Training Loss: 0.9003, Validation Loss: 3.3293, Validation Percentage:45.8824%,Test Loss: 3.2134, Test Percentage:43.1940%\n",
            "Epoch [1195/1200], Training Loss: 0.8327, Validation Loss: 2.9377, Validation Percentage:46.3725%,Test Loss: 3.2798, Test Percentage:41.7954%\n",
            "Epoch [1196/1200], Training Loss: 0.8134, Validation Loss: 3.2210, Validation Percentage:45.0000%,Test Loss: 3.5337, Test Percentage:42.1532%\n",
            "Epoch [1197/1200], Training Loss: 0.8115, Validation Loss: 3.1535, Validation Percentage:45.3922%,Test Loss: 3.2768, Test Percentage:43.2428%\n",
            "Epoch [1198/1200], Training Loss: 0.7918, Validation Loss: 2.8728, Validation Percentage:45.5882%,Test Loss: 3.2497, Test Percentage:40.8034%\n",
            "Epoch [1199/1200], Training Loss: 0.9014, Validation Loss: 3.1983, Validation Percentage:45.1961%,Test Loss: 3.4540, Test Percentage:42.1369%\n",
            "Epoch [1200/1200], Training Loss: 0.8737, Validation Loss: 2.8494, Validation Percentage:45.1961%,Test Loss: 3.2877, Test Percentage:40.3480%\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = 0.0\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "#Test\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate average test loss and accuracy\n",
        "test_loss = test_loss / len(test_loader.dataset)\n",
        "test_accuracy = test_correct / test_total\n",
        "\n",
        "\n",
        "print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "          f'Training Loss: {epoch_loss:.4f}, '\n",
        "          f'Test Loss: {test_loss:.4f}, '\n",
        "          f'Test Percentage:{test_accuracy*100:.4f}%'\n",
        "          )"
      ],
      "metadata": {
        "id": "t4VjBXGTBgfm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4378d51a-4d21-4205-cc0a-a57010e6bdf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1200/1200], Training Loss: 0.8737, Test Loss: 3.3277, Test Percentage:40.1203%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import Flowers102\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define transforms for data augmentation and normalization\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the dataset with train/val/test splits\n",
        "train_dataset = Flowers102(root='./data', split='train', transform=train_transform, download=True)\n",
        "val_dataset = Flowers102(root='./data', split='val', transform=val_transform, download=True)\n",
        "\n",
        "test_dataset = Flowers102(root='./data', split='test', transform=val_transform, download=True)\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "\n",
        "class CNN_NN(nn.Module):\n",
        "    def __init__(self, num_classes=102):\n",
        "        super(CNN_NN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv5= nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv6= nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation =1, ceil_mode=False)\n",
        "\n",
        "        # Initialize the size of the fully connected layer based on the sample input\n",
        "        self._to_linear = None\n",
        "        self.convs = nn.Sequential(\n",
        "            self.conv1,\n",
        "            nn.ReLU(inplace=True),\n",
        "            self.conv2,\n",
        "            nn.ReLU(inplace=True),\n",
        "            self.pool,\n",
        "            self.conv3,\n",
        "            nn.ReLU(inplace=True),\n",
        "            self.conv4,\n",
        "            nn.ReLU(inplace=True),\n",
        "            self.pool,\n",
        "            self.conv5,\n",
        "            nn.ReLU(inplace=True),\n",
        "            self.conv6,\n",
        "            nn.ReLU(inplace=True),\n",
        "            self.pool,\n",
        "        )\n",
        "        self._initialize_linear_layer()\n",
        "\n",
        "        self.fc1 = nn.Linear(self._to_linear, 512)\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def _initialize_linear_layer(self):\n",
        "        # Create a dummy input tensor to calculate the output size after conv layers\n",
        "        x = torch.randn(1, 3, 224, 224)\n",
        "        x = self.convs(x)\n",
        "        self._to_linear = x.view(1, -1).shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.convs(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor while preserving the batch size\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Define the device (GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "num_classes = 102  # Number of output classes\n",
        "learning_rate = 0.001\n",
        "num_epochs = 1200\n",
        "\n",
        "# Initialize the model\n",
        "model = CNN_NN(num_classes=num_classes).to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    # Calculate average loss for the epoch\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Calculate average validation loss and accuracy\n",
        "    val_loss = val_loss / len(val_loader.dataset)\n",
        "    accuracy = correct / total\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "          f'Training Loss: {epoch_loss:.4f}, '\n",
        "          f'Validation Loss: {val_loss:.4f}, '\n",
        "          f'Validation Percentage:{accuracy*100:.4f}%,'\n",
        "          )\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9hUepFfMity",
        "outputId": "b37e2786-f5d9-4c31-969c-88bef3bf0e85"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1200], Training Loss: 4.6457, Validation Loss: 4.6176, Validation Percentage:1.6667%,\n",
            "Epoch [2/1200], Training Loss: 4.6010, Validation Loss: 4.4682, Validation Percentage:1.1765%,\n",
            "Epoch [3/1200], Training Loss: 4.5066, Validation Loss: 4.4327, Validation Percentage:1.5686%,\n",
            "Epoch [4/1200], Training Loss: 4.4606, Validation Loss: 4.4088, Validation Percentage:1.8627%,\n",
            "Epoch [5/1200], Training Loss: 4.4516, Validation Loss: 4.4128, Validation Percentage:2.3529%,\n",
            "Epoch [6/1200], Training Loss: 4.4306, Validation Loss: 4.4106, Validation Percentage:1.7647%,\n",
            "Epoch [7/1200], Training Loss: 4.3816, Validation Loss: 4.3780, Validation Percentage:2.2549%,\n",
            "Epoch [8/1200], Training Loss: 4.3419, Validation Loss: 4.2687, Validation Percentage:2.3529%,\n",
            "Epoch [9/1200], Training Loss: 4.3052, Validation Loss: 4.2243, Validation Percentage:2.8431%,\n",
            "Epoch [10/1200], Training Loss: 4.2819, Validation Loss: 4.1866, Validation Percentage:3.5294%,\n",
            "Epoch [11/1200], Training Loss: 4.2200, Validation Loss: 4.2385, Validation Percentage:2.3529%,\n",
            "Epoch [12/1200], Training Loss: 4.2095, Validation Loss: 4.1295, Validation Percentage:3.6275%,\n",
            "Epoch [13/1200], Training Loss: 4.1947, Validation Loss: 4.1096, Validation Percentage:4.3137%,\n",
            "Epoch [14/1200], Training Loss: 4.1661, Validation Loss: 4.1393, Validation Percentage:5.3922%,\n",
            "Epoch [15/1200], Training Loss: 4.1309, Validation Loss: 4.0944, Validation Percentage:5.5882%,\n",
            "Epoch [16/1200], Training Loss: 4.1279, Validation Loss: 4.1225, Validation Percentage:4.6078%,\n",
            "Epoch [17/1200], Training Loss: 4.0437, Validation Loss: 4.0155, Validation Percentage:5.6863%,\n",
            "Epoch [18/1200], Training Loss: 4.0867, Validation Loss: 4.0411, Validation Percentage:6.8627%,\n",
            "Epoch [19/1200], Training Loss: 4.0217, Validation Loss: 4.0186, Validation Percentage:6.7647%,\n",
            "Epoch [20/1200], Training Loss: 4.0458, Validation Loss: 4.1477, Validation Percentage:5.7843%,\n",
            "Epoch [21/1200], Training Loss: 3.9841, Validation Loss: 3.9636, Validation Percentage:8.1373%,\n",
            "Epoch [22/1200], Training Loss: 3.9210, Validation Loss: 3.9199, Validation Percentage:8.6275%,\n",
            "Epoch [23/1200], Training Loss: 3.9661, Validation Loss: 3.9151, Validation Percentage:7.9412%,\n",
            "Epoch [24/1200], Training Loss: 3.9198, Validation Loss: 3.9392, Validation Percentage:7.7451%,\n",
            "Epoch [25/1200], Training Loss: 3.8808, Validation Loss: 3.9209, Validation Percentage:7.9412%,\n",
            "Epoch [26/1200], Training Loss: 3.7931, Validation Loss: 3.8562, Validation Percentage:9.0196%,\n",
            "Epoch [27/1200], Training Loss: 3.8580, Validation Loss: 3.8569, Validation Percentage:8.8235%,\n",
            "Epoch [28/1200], Training Loss: 3.8177, Validation Loss: 3.8447, Validation Percentage:10.0980%,\n",
            "Epoch [29/1200], Training Loss: 3.7841, Validation Loss: 3.9209, Validation Percentage:8.8235%,\n",
            "Epoch [30/1200], Training Loss: 3.7839, Validation Loss: 3.8624, Validation Percentage:9.1176%,\n",
            "Epoch [31/1200], Training Loss: 3.7098, Validation Loss: 3.7720, Validation Percentage:9.7059%,\n",
            "Epoch [32/1200], Training Loss: 3.6818, Validation Loss: 3.7481, Validation Percentage:10.2941%,\n",
            "Epoch [33/1200], Training Loss: 3.6690, Validation Loss: 3.7352, Validation Percentage:11.0784%,\n",
            "Epoch [34/1200], Training Loss: 3.6567, Validation Loss: 3.7562, Validation Percentage:10.3922%,\n",
            "Epoch [35/1200], Training Loss: 3.6365, Validation Loss: 3.6841, Validation Percentage:10.7843%,\n",
            "Epoch [36/1200], Training Loss: 3.5622, Validation Loss: 3.7562, Validation Percentage:12.5490%,\n",
            "Epoch [37/1200], Training Loss: 3.6564, Validation Loss: 3.6996, Validation Percentage:11.5686%,\n",
            "Epoch [38/1200], Training Loss: 3.5327, Validation Loss: 3.6156, Validation Percentage:13.7255%,\n",
            "Epoch [39/1200], Training Loss: 3.4490, Validation Loss: 3.6911, Validation Percentage:12.6471%,\n",
            "Epoch [40/1200], Training Loss: 3.5479, Validation Loss: 3.6436, Validation Percentage:12.8431%,\n",
            "Epoch [41/1200], Training Loss: 3.4875, Validation Loss: 3.5920, Validation Percentage:13.4314%,\n",
            "Epoch [42/1200], Training Loss: 3.4364, Validation Loss: 3.6275, Validation Percentage:14.0196%,\n",
            "Epoch [43/1200], Training Loss: 3.4610, Validation Loss: 3.6128, Validation Percentage:14.3137%,\n",
            "Epoch [44/1200], Training Loss: 3.4247, Validation Loss: 3.5670, Validation Percentage:15.7843%,\n",
            "Epoch [45/1200], Training Loss: 3.3967, Validation Loss: 3.5451, Validation Percentage:15.0980%,\n",
            "Epoch [46/1200], Training Loss: 3.3677, Validation Loss: 3.6290, Validation Percentage:14.6078%,\n",
            "Epoch [47/1200], Training Loss: 3.3745, Validation Loss: 3.6377, Validation Percentage:13.4314%,\n",
            "Epoch [48/1200], Training Loss: 3.4243, Validation Loss: 3.5747, Validation Percentage:14.4118%,\n",
            "Epoch [49/1200], Training Loss: 3.3437, Validation Loss: 3.5371, Validation Percentage:15.7843%,\n",
            "Epoch [50/1200], Training Loss: 3.3336, Validation Loss: 3.5285, Validation Percentage:15.2941%,\n",
            "Epoch [51/1200], Training Loss: 3.3050, Validation Loss: 3.5852, Validation Percentage:13.8235%,\n",
            "Epoch [52/1200], Training Loss: 3.2150, Validation Loss: 3.6410, Validation Percentage:16.5686%,\n",
            "Epoch [53/1200], Training Loss: 3.3132, Validation Loss: 3.4934, Validation Percentage:16.2745%,\n",
            "Epoch [54/1200], Training Loss: 3.2363, Validation Loss: 3.5649, Validation Percentage:15.7843%,\n",
            "Epoch [55/1200], Training Loss: 3.2324, Validation Loss: 3.4951, Validation Percentage:16.8627%,\n",
            "Epoch [56/1200], Training Loss: 3.1979, Validation Loss: 3.5398, Validation Percentage:15.3922%,\n",
            "Epoch [57/1200], Training Loss: 3.2068, Validation Loss: 3.6297, Validation Percentage:13.8235%,\n",
            "Epoch [58/1200], Training Loss: 3.1225, Validation Loss: 3.4796, Validation Percentage:17.4510%,\n",
            "Epoch [59/1200], Training Loss: 3.1005, Validation Loss: 3.4940, Validation Percentage:17.4510%,\n",
            "Epoch [60/1200], Training Loss: 3.0402, Validation Loss: 3.5413, Validation Percentage:15.0980%,\n",
            "Epoch [61/1200], Training Loss: 3.1569, Validation Loss: 3.4495, Validation Percentage:17.1569%,\n",
            "Epoch [62/1200], Training Loss: 3.1480, Validation Loss: 3.4365, Validation Percentage:18.2353%,\n",
            "Epoch [63/1200], Training Loss: 3.0313, Validation Loss: 3.4897, Validation Percentage:16.2745%,\n",
            "Epoch [64/1200], Training Loss: 3.0879, Validation Loss: 3.4777, Validation Percentage:17.7451%,\n",
            "Epoch [65/1200], Training Loss: 2.9679, Validation Loss: 3.5277, Validation Percentage:18.2353%,\n",
            "Epoch [66/1200], Training Loss: 3.0506, Validation Loss: 3.5118, Validation Percentage:18.0392%,\n",
            "Epoch [67/1200], Training Loss: 3.0368, Validation Loss: 3.5094, Validation Percentage:17.9412%,\n",
            "Epoch [68/1200], Training Loss: 2.9959, Validation Loss: 3.4500, Validation Percentage:18.1373%,\n",
            "Epoch [69/1200], Training Loss: 2.9113, Validation Loss: 3.4412, Validation Percentage:19.4118%,\n",
            "Epoch [70/1200], Training Loss: 2.9314, Validation Loss: 3.4413, Validation Percentage:17.4510%,\n",
            "Epoch [71/1200], Training Loss: 2.9867, Validation Loss: 3.4208, Validation Percentage:19.1176%,\n",
            "Epoch [72/1200], Training Loss: 2.8964, Validation Loss: 3.4057, Validation Percentage:20.4902%,\n",
            "Epoch [73/1200], Training Loss: 2.9783, Validation Loss: 3.4753, Validation Percentage:18.0392%,\n",
            "Epoch [74/1200], Training Loss: 2.8695, Validation Loss: 3.4374, Validation Percentage:19.4118%,\n",
            "Epoch [75/1200], Training Loss: 2.8136, Validation Loss: 3.3415, Validation Percentage:21.8627%,\n",
            "Epoch [76/1200], Training Loss: 2.9126, Validation Loss: 3.3479, Validation Percentage:19.1176%,\n",
            "Epoch [77/1200], Training Loss: 2.7714, Validation Loss: 3.3868, Validation Percentage:20.1961%,\n",
            "Epoch [78/1200], Training Loss: 2.8719, Validation Loss: 3.4724, Validation Percentage:17.7451%,\n",
            "Epoch [79/1200], Training Loss: 2.7514, Validation Loss: 3.4469, Validation Percentage:21.7647%,\n",
            "Epoch [80/1200], Training Loss: 2.8174, Validation Loss: 3.3401, Validation Percentage:22.0588%,\n",
            "Epoch [81/1200], Training Loss: 2.7907, Validation Loss: 3.3259, Validation Percentage:21.2745%,\n",
            "Epoch [82/1200], Training Loss: 2.8488, Validation Loss: 3.3190, Validation Percentage:22.5490%,\n",
            "Epoch [83/1200], Training Loss: 2.7468, Validation Loss: 3.2820, Validation Percentage:23.0392%,\n",
            "Epoch [84/1200], Training Loss: 2.7581, Validation Loss: 3.3512, Validation Percentage:20.2941%,\n",
            "Epoch [85/1200], Training Loss: 2.7505, Validation Loss: 3.3430, Validation Percentage:22.4510%,\n",
            "Epoch [86/1200], Training Loss: 2.7362, Validation Loss: 3.3132, Validation Percentage:23.3333%,\n",
            "Epoch [87/1200], Training Loss: 2.7620, Validation Loss: 3.3605, Validation Percentage:19.3137%,\n",
            "Epoch [88/1200], Training Loss: 2.7306, Validation Loss: 3.3649, Validation Percentage:20.6863%,\n",
            "Epoch [89/1200], Training Loss: 2.6711, Validation Loss: 3.2833, Validation Percentage:23.3333%,\n",
            "Epoch [90/1200], Training Loss: 2.6053, Validation Loss: 3.2681, Validation Percentage:23.9216%,\n",
            "Epoch [91/1200], Training Loss: 2.6280, Validation Loss: 3.3559, Validation Percentage:20.9804%,\n",
            "Epoch [92/1200], Training Loss: 2.6140, Validation Loss: 3.3019, Validation Percentage:22.7451%,\n",
            "Epoch [93/1200], Training Loss: 2.5505, Validation Loss: 3.3720, Validation Percentage:23.2353%,\n",
            "Epoch [94/1200], Training Loss: 2.6472, Validation Loss: 3.2571, Validation Percentage:24.7059%,\n",
            "Epoch [95/1200], Training Loss: 2.6942, Validation Loss: 3.2865, Validation Percentage:22.3529%,\n",
            "Epoch [96/1200], Training Loss: 2.6828, Validation Loss: 3.2124, Validation Percentage:23.4314%,\n",
            "Epoch [97/1200], Training Loss: 2.6343, Validation Loss: 3.2116, Validation Percentage:24.7059%,\n",
            "Epoch [98/1200], Training Loss: 2.5389, Validation Loss: 3.2157, Validation Percentage:23.9216%,\n",
            "Epoch [99/1200], Training Loss: 2.5793, Validation Loss: 3.3380, Validation Percentage:22.2549%,\n",
            "Epoch [100/1200], Training Loss: 2.5227, Validation Loss: 3.2570, Validation Percentage:23.8235%,\n",
            "Epoch [101/1200], Training Loss: 2.5824, Validation Loss: 3.2119, Validation Percentage:25.0000%,\n",
            "Epoch [102/1200], Training Loss: 2.6232, Validation Loss: 3.1505, Validation Percentage:23.7255%,\n",
            "Epoch [103/1200], Training Loss: 2.4907, Validation Loss: 3.2265, Validation Percentage:24.8039%,\n",
            "Epoch [104/1200], Training Loss: 2.4989, Validation Loss: 3.1880, Validation Percentage:26.3725%,\n",
            "Epoch [105/1200], Training Loss: 2.4941, Validation Loss: 3.1813, Validation Percentage:24.3137%,\n",
            "Epoch [106/1200], Training Loss: 2.4701, Validation Loss: 3.2237, Validation Percentage:25.5882%,\n",
            "Epoch [107/1200], Training Loss: 2.4587, Validation Loss: 3.2005, Validation Percentage:26.2745%,\n",
            "Epoch [108/1200], Training Loss: 2.4291, Validation Loss: 3.2220, Validation Percentage:24.0196%,\n",
            "Epoch [109/1200], Training Loss: 2.5778, Validation Loss: 3.1782, Validation Percentage:24.7059%,\n",
            "Epoch [110/1200], Training Loss: 2.4276, Validation Loss: 3.2572, Validation Percentage:25.3922%,\n",
            "Epoch [111/1200], Training Loss: 2.4784, Validation Loss: 3.2653, Validation Percentage:25.4902%,\n",
            "Epoch [112/1200], Training Loss: 2.3001, Validation Loss: 3.2170, Validation Percentage:26.8627%,\n",
            "Epoch [113/1200], Training Loss: 2.4055, Validation Loss: 3.2159, Validation Percentage:25.4902%,\n",
            "Epoch [114/1200], Training Loss: 2.4046, Validation Loss: 3.1901, Validation Percentage:26.9608%,\n",
            "Epoch [115/1200], Training Loss: 2.3784, Validation Loss: 3.2072, Validation Percentage:26.6667%,\n",
            "Epoch [116/1200], Training Loss: 2.3115, Validation Loss: 3.0956, Validation Percentage:28.3333%,\n",
            "Epoch [117/1200], Training Loss: 2.3921, Validation Loss: 3.1366, Validation Percentage:27.9412%,\n",
            "Epoch [118/1200], Training Loss: 2.2769, Validation Loss: 3.1819, Validation Percentage:27.6471%,\n",
            "Epoch [119/1200], Training Loss: 2.4238, Validation Loss: 3.0942, Validation Percentage:26.8627%,\n",
            "Epoch [120/1200], Training Loss: 2.2061, Validation Loss: 3.3662, Validation Percentage:23.3333%,\n",
            "Epoch [121/1200], Training Loss: 2.3490, Validation Loss: 3.1203, Validation Percentage:28.4314%,\n",
            "Epoch [122/1200], Training Loss: 2.3223, Validation Loss: 3.1178, Validation Percentage:27.4510%,\n",
            "Epoch [123/1200], Training Loss: 2.3391, Validation Loss: 3.0617, Validation Percentage:28.7255%,\n",
            "Epoch [124/1200], Training Loss: 2.3531, Validation Loss: 3.1385, Validation Percentage:25.3922%,\n",
            "Epoch [125/1200], Training Loss: 2.2988, Validation Loss: 3.1380, Validation Percentage:27.5490%,\n",
            "Epoch [126/1200], Training Loss: 2.3504, Validation Loss: 3.1247, Validation Percentage:26.3725%,\n",
            "Epoch [127/1200], Training Loss: 2.3464, Validation Loss: 3.1694, Validation Percentage:29.0196%,\n",
            "Epoch [128/1200], Training Loss: 2.1538, Validation Loss: 3.1996, Validation Percentage:27.0588%,\n",
            "Epoch [129/1200], Training Loss: 2.2749, Validation Loss: 3.1691, Validation Percentage:26.7647%,\n",
            "Epoch [130/1200], Training Loss: 2.2526, Validation Loss: 3.0956, Validation Percentage:26.9608%,\n",
            "Epoch [131/1200], Training Loss: 2.1557, Validation Loss: 3.1979, Validation Percentage:28.2353%,\n",
            "Epoch [132/1200], Training Loss: 2.2769, Validation Loss: 3.1702, Validation Percentage:27.2549%,\n",
            "Epoch [133/1200], Training Loss: 2.2357, Validation Loss: 3.1632, Validation Percentage:28.0392%,\n",
            "Epoch [134/1200], Training Loss: 2.2001, Validation Loss: 3.1870, Validation Percentage:28.2353%,\n",
            "Epoch [135/1200], Training Loss: 2.1626, Validation Loss: 3.2161, Validation Percentage:27.5490%,\n",
            "Epoch [136/1200], Training Loss: 2.1390, Validation Loss: 3.0456, Validation Percentage:28.2353%,\n",
            "Epoch [137/1200], Training Loss: 2.1964, Validation Loss: 3.1512, Validation Percentage:27.5490%,\n",
            "Epoch [138/1200], Training Loss: 2.2201, Validation Loss: 3.0378, Validation Percentage:27.9412%,\n",
            "Epoch [139/1200], Training Loss: 2.1375, Validation Loss: 3.1473, Validation Percentage:27.5490%,\n",
            "Epoch [140/1200], Training Loss: 2.1990, Validation Loss: 3.0511, Validation Percentage:29.0196%,\n",
            "Epoch [141/1200], Training Loss: 2.0788, Validation Loss: 3.1766, Validation Percentage:27.3529%,\n",
            "Epoch [142/1200], Training Loss: 2.0705, Validation Loss: 3.1839, Validation Percentage:27.7451%,\n",
            "Epoch [143/1200], Training Loss: 2.1872, Validation Loss: 3.1114, Validation Percentage:28.4314%,\n",
            "Epoch [144/1200], Training Loss: 2.1018, Validation Loss: 3.1189, Validation Percentage:30.3922%,\n",
            "Epoch [145/1200], Training Loss: 2.0959, Validation Loss: 3.0356, Validation Percentage:29.5098%,\n",
            "Epoch [146/1200], Training Loss: 2.0764, Validation Loss: 3.1420, Validation Percentage:27.9412%,\n",
            "Epoch [147/1200], Training Loss: 2.1223, Validation Loss: 3.1614, Validation Percentage:28.1373%,\n",
            "Epoch [148/1200], Training Loss: 2.0847, Validation Loss: 3.0552, Validation Percentage:30.4902%,\n",
            "Epoch [149/1200], Training Loss: 2.1585, Validation Loss: 3.2698, Validation Percentage:27.2549%,\n",
            "Epoch [150/1200], Training Loss: 2.1140, Validation Loss: 3.1001, Validation Percentage:30.8824%,\n",
            "Epoch [151/1200], Training Loss: 2.1184, Validation Loss: 3.1079, Validation Percentage:29.7059%,\n",
            "Epoch [152/1200], Training Loss: 2.0584, Validation Loss: 3.0724, Validation Percentage:28.8235%,\n",
            "Epoch [153/1200], Training Loss: 2.0888, Validation Loss: 2.9987, Validation Percentage:31.8627%,\n",
            "Epoch [154/1200], Training Loss: 1.9592, Validation Loss: 3.1630, Validation Percentage:29.0196%,\n",
            "Epoch [155/1200], Training Loss: 1.9502, Validation Loss: 3.1891, Validation Percentage:28.8235%,\n",
            "Epoch [156/1200], Training Loss: 2.0015, Validation Loss: 3.0625, Validation Percentage:30.9804%,\n",
            "Epoch [157/1200], Training Loss: 2.0905, Validation Loss: 3.0135, Validation Percentage:32.2549%,\n",
            "Epoch [158/1200], Training Loss: 1.9528, Validation Loss: 3.1514, Validation Percentage:28.1373%,\n",
            "Epoch [159/1200], Training Loss: 2.0120, Validation Loss: 2.9936, Validation Percentage:32.0588%,\n",
            "Epoch [160/1200], Training Loss: 2.0348, Validation Loss: 3.1034, Validation Percentage:29.6078%,\n",
            "Epoch [161/1200], Training Loss: 1.9898, Validation Loss: 3.0814, Validation Percentage:30.7843%,\n",
            "Epoch [162/1200], Training Loss: 1.9955, Validation Loss: 3.0865, Validation Percentage:28.4314%,\n",
            "Epoch [163/1200], Training Loss: 2.1306, Validation Loss: 3.0119, Validation Percentage:30.9804%,\n",
            "Epoch [164/1200], Training Loss: 2.0536, Validation Loss: 2.9363, Validation Percentage:30.6863%,\n",
            "Epoch [165/1200], Training Loss: 2.1135, Validation Loss: 3.0385, Validation Percentage:29.3137%,\n",
            "Epoch [166/1200], Training Loss: 1.9579, Validation Loss: 3.1389, Validation Percentage:30.0980%,\n",
            "Epoch [167/1200], Training Loss: 1.9236, Validation Loss: 3.1361, Validation Percentage:30.5882%,\n",
            "Epoch [168/1200], Training Loss: 1.9720, Validation Loss: 3.1745, Validation Percentage:29.3137%,\n",
            "Epoch [169/1200], Training Loss: 2.0942, Validation Loss: 3.1544, Validation Percentage:30.0980%,\n",
            "Epoch [170/1200], Training Loss: 2.0732, Validation Loss: 3.0910, Validation Percentage:28.3333%,\n",
            "Epoch [171/1200], Training Loss: 1.9317, Validation Loss: 3.0188, Validation Percentage:32.1569%,\n",
            "Epoch [172/1200], Training Loss: 1.9502, Validation Loss: 3.1009, Validation Percentage:30.2941%,\n",
            "Epoch [173/1200], Training Loss: 1.9368, Validation Loss: 3.1096, Validation Percentage:30.8824%,\n",
            "Epoch [174/1200], Training Loss: 1.9393, Validation Loss: 2.9998, Validation Percentage:32.0588%,\n",
            "Epoch [175/1200], Training Loss: 1.9110, Validation Loss: 2.9677, Validation Percentage:32.1569%,\n",
            "Epoch [176/1200], Training Loss: 1.8710, Validation Loss: 3.0978, Validation Percentage:29.1176%,\n",
            "Epoch [177/1200], Training Loss: 1.9428, Validation Loss: 3.1458, Validation Percentage:29.1176%,\n",
            "Epoch [178/1200], Training Loss: 1.9360, Validation Loss: 3.0444, Validation Percentage:31.8627%,\n",
            "Epoch [179/1200], Training Loss: 1.8977, Validation Loss: 3.0054, Validation Percentage:32.2549%,\n",
            "Epoch [180/1200], Training Loss: 1.8219, Validation Loss: 3.0365, Validation Percentage:32.2549%,\n",
            "Epoch [181/1200], Training Loss: 1.9879, Validation Loss: 2.9887, Validation Percentage:33.8235%,\n",
            "Epoch [182/1200], Training Loss: 1.9264, Validation Loss: 2.9487, Validation Percentage:32.5490%,\n",
            "Epoch [183/1200], Training Loss: 1.8562, Validation Loss: 3.0737, Validation Percentage:28.8235%,\n",
            "Epoch [184/1200], Training Loss: 1.9032, Validation Loss: 3.1023, Validation Percentage:30.1961%,\n",
            "Epoch [185/1200], Training Loss: 1.8075, Validation Loss: 2.9852, Validation Percentage:32.5490%,\n",
            "Epoch [186/1200], Training Loss: 1.8852, Validation Loss: 3.0725, Validation Percentage:32.5490%,\n",
            "Epoch [187/1200], Training Loss: 1.8729, Validation Loss: 3.1039, Validation Percentage:31.6667%,\n",
            "Epoch [188/1200], Training Loss: 1.9604, Validation Loss: 2.8980, Validation Percentage:33.9216%,\n",
            "Epoch [189/1200], Training Loss: 1.7913, Validation Loss: 2.9396, Validation Percentage:32.7451%,\n",
            "Epoch [190/1200], Training Loss: 1.7613, Validation Loss: 3.0056, Validation Percentage:34.0196%,\n",
            "Epoch [191/1200], Training Loss: 1.8970, Validation Loss: 3.2085, Validation Percentage:29.7059%,\n",
            "Epoch [192/1200], Training Loss: 1.7272, Validation Loss: 3.0867, Validation Percentage:32.3529%,\n",
            "Epoch [193/1200], Training Loss: 1.8436, Validation Loss: 3.1206, Validation Percentage:33.4314%,\n",
            "Epoch [194/1200], Training Loss: 1.9084, Validation Loss: 2.9963, Validation Percentage:31.0784%,\n",
            "Epoch [195/1200], Training Loss: 1.8811, Validation Loss: 2.9970, Validation Percentage:33.0392%,\n",
            "Epoch [196/1200], Training Loss: 1.7932, Validation Loss: 3.0965, Validation Percentage:31.8627%,\n",
            "Epoch [197/1200], Training Loss: 1.8797, Validation Loss: 3.0462, Validation Percentage:32.7451%,\n",
            "Epoch [198/1200], Training Loss: 1.8552, Validation Loss: 2.9793, Validation Percentage:32.8431%,\n",
            "Epoch [199/1200], Training Loss: 1.7575, Validation Loss: 3.0298, Validation Percentage:32.9412%,\n",
            "Epoch [200/1200], Training Loss: 1.8225, Validation Loss: 3.1573, Validation Percentage:33.7255%,\n",
            "Epoch [201/1200], Training Loss: 1.8838, Validation Loss: 3.2418, Validation Percentage:31.1765%,\n",
            "Epoch [202/1200], Training Loss: 1.8261, Validation Loss: 3.1124, Validation Percentage:32.5490%,\n",
            "Epoch [203/1200], Training Loss: 1.8154, Validation Loss: 3.0851, Validation Percentage:33.3333%,\n",
            "Epoch [204/1200], Training Loss: 1.7015, Validation Loss: 3.0157, Validation Percentage:34.2157%,\n",
            "Epoch [205/1200], Training Loss: 1.7977, Validation Loss: 3.0524, Validation Percentage:31.3725%,\n",
            "Epoch [206/1200], Training Loss: 1.7725, Validation Loss: 2.9211, Validation Percentage:34.7059%,\n",
            "Epoch [207/1200], Training Loss: 1.7905, Validation Loss: 3.0652, Validation Percentage:32.6471%,\n",
            "Epoch [208/1200], Training Loss: 1.8131, Validation Loss: 3.0575, Validation Percentage:31.0784%,\n",
            "Epoch [209/1200], Training Loss: 1.7348, Validation Loss: 2.9991, Validation Percentage:33.8235%,\n",
            "Epoch [210/1200], Training Loss: 1.7140, Validation Loss: 3.1267, Validation Percentage:32.2549%,\n",
            "Epoch [211/1200], Training Loss: 1.7043, Validation Loss: 3.1524, Validation Percentage:31.3725%,\n",
            "Epoch [212/1200], Training Loss: 1.7700, Validation Loss: 3.0562, Validation Percentage:29.2157%,\n",
            "Epoch [213/1200], Training Loss: 1.7606, Validation Loss: 3.0672, Validation Percentage:31.4706%,\n",
            "Epoch [214/1200], Training Loss: 1.7678, Validation Loss: 3.0555, Validation Percentage:34.2157%,\n",
            "Epoch [215/1200], Training Loss: 1.7884, Validation Loss: 3.0282, Validation Percentage:33.6275%,\n",
            "Epoch [216/1200], Training Loss: 1.6937, Validation Loss: 3.0019, Validation Percentage:32.6471%,\n",
            "Epoch [217/1200], Training Loss: 1.5622, Validation Loss: 3.0808, Validation Percentage:33.7255%,\n",
            "Epoch [218/1200], Training Loss: 1.6789, Validation Loss: 3.0149, Validation Percentage:31.9608%,\n",
            "Epoch [219/1200], Training Loss: 1.7722, Validation Loss: 3.1073, Validation Percentage:31.9608%,\n",
            "Epoch [220/1200], Training Loss: 1.7443, Validation Loss: 3.0247, Validation Percentage:34.2157%,\n",
            "Epoch [221/1200], Training Loss: 1.7899, Validation Loss: 2.8817, Validation Percentage:33.0392%,\n",
            "Epoch [222/1200], Training Loss: 1.7135, Validation Loss: 3.1462, Validation Percentage:32.2549%,\n",
            "Epoch [223/1200], Training Loss: 1.8055, Validation Loss: 3.0481, Validation Percentage:32.6471%,\n",
            "Epoch [224/1200], Training Loss: 1.7222, Validation Loss: 3.1288, Validation Percentage:32.8431%,\n",
            "Epoch [225/1200], Training Loss: 1.7310, Validation Loss: 3.1068, Validation Percentage:30.1961%,\n",
            "Epoch [226/1200], Training Loss: 1.6706, Validation Loss: 3.1591, Validation Percentage:30.2941%,\n",
            "Epoch [227/1200], Training Loss: 1.7684, Validation Loss: 3.1139, Validation Percentage:31.0784%,\n",
            "Epoch [228/1200], Training Loss: 1.7520, Validation Loss: 3.0221, Validation Percentage:34.7059%,\n",
            "Epoch [229/1200], Training Loss: 1.6107, Validation Loss: 3.1119, Validation Percentage:34.2157%,\n",
            "Epoch [230/1200], Training Loss: 1.6187, Validation Loss: 2.9430, Validation Percentage:35.0980%,\n",
            "Epoch [231/1200], Training Loss: 1.7115, Validation Loss: 3.1621, Validation Percentage:32.2549%,\n",
            "Epoch [232/1200], Training Loss: 1.6228, Validation Loss: 3.0489, Validation Percentage:35.2941%,\n",
            "Epoch [233/1200], Training Loss: 1.6881, Validation Loss: 2.9118, Validation Percentage:34.8039%,\n",
            "Epoch [234/1200], Training Loss: 1.6567, Validation Loss: 3.0197, Validation Percentage:35.0000%,\n",
            "Epoch [235/1200], Training Loss: 1.5676, Validation Loss: 2.9652, Validation Percentage:33.4314%,\n",
            "Epoch [236/1200], Training Loss: 1.6199, Validation Loss: 3.1634, Validation Percentage:30.5882%,\n",
            "Epoch [237/1200], Training Loss: 1.6489, Validation Loss: 3.1825, Validation Percentage:32.9412%,\n",
            "Epoch [238/1200], Training Loss: 1.6007, Validation Loss: 3.1151, Validation Percentage:33.2353%,\n",
            "Epoch [239/1200], Training Loss: 1.6346, Validation Loss: 2.9819, Validation Percentage:34.3137%,\n",
            "Epoch [240/1200], Training Loss: 1.5704, Validation Loss: 3.1313, Validation Percentage:30.6863%,\n",
            "Epoch [241/1200], Training Loss: 1.7770, Validation Loss: 2.9271, Validation Percentage:35.4902%,\n",
            "Epoch [242/1200], Training Loss: 1.7105, Validation Loss: 2.9417, Validation Percentage:33.6275%,\n",
            "Epoch [243/1200], Training Loss: 1.6398, Validation Loss: 3.0459, Validation Percentage:33.7255%,\n",
            "Epoch [244/1200], Training Loss: 1.5901, Validation Loss: 3.0153, Validation Percentage:34.5098%,\n",
            "Epoch [245/1200], Training Loss: 1.5410, Validation Loss: 3.0799, Validation Percentage:32.4510%,\n",
            "Epoch [246/1200], Training Loss: 1.5599, Validation Loss: 3.0978, Validation Percentage:33.9216%,\n",
            "Epoch [247/1200], Training Loss: 1.5760, Validation Loss: 3.0457, Validation Percentage:33.8235%,\n",
            "Epoch [248/1200], Training Loss: 1.5831, Validation Loss: 3.1692, Validation Percentage:32.2549%,\n",
            "Epoch [249/1200], Training Loss: 1.5109, Validation Loss: 3.0603, Validation Percentage:33.0392%,\n",
            "Epoch [250/1200], Training Loss: 1.5773, Validation Loss: 3.2479, Validation Percentage:32.4510%,\n",
            "Epoch [251/1200], Training Loss: 1.6593, Validation Loss: 2.9017, Validation Percentage:35.3922%,\n",
            "Epoch [252/1200], Training Loss: 1.5514, Validation Loss: 2.9772, Validation Percentage:33.3333%,\n",
            "Epoch [253/1200], Training Loss: 1.6079, Validation Loss: 3.0467, Validation Percentage:35.6863%,\n",
            "Epoch [254/1200], Training Loss: 1.7481, Validation Loss: 2.9448, Validation Percentage:35.2941%,\n",
            "Epoch [255/1200], Training Loss: 1.6264, Validation Loss: 3.0621, Validation Percentage:33.4314%,\n",
            "Epoch [256/1200], Training Loss: 1.6324, Validation Loss: 2.9982, Validation Percentage:34.5098%,\n",
            "Epoch [257/1200], Training Loss: 1.6110, Validation Loss: 3.0273, Validation Percentage:34.4118%,\n",
            "Epoch [258/1200], Training Loss: 1.5092, Validation Loss: 2.9504, Validation Percentage:33.5294%,\n",
            "Epoch [259/1200], Training Loss: 1.6458, Validation Loss: 2.8149, Validation Percentage:34.6078%,\n",
            "Epoch [260/1200], Training Loss: 1.4797, Validation Loss: 3.0512, Validation Percentage:33.4314%,\n",
            "Epoch [261/1200], Training Loss: 1.6112, Validation Loss: 3.0120, Validation Percentage:34.7059%,\n",
            "Epoch [262/1200], Training Loss: 1.5333, Validation Loss: 3.0510, Validation Percentage:36.6667%,\n",
            "Epoch [263/1200], Training Loss: 1.5971, Validation Loss: 2.9663, Validation Percentage:36.0784%,\n",
            "Epoch [264/1200], Training Loss: 1.5688, Validation Loss: 3.1455, Validation Percentage:35.6863%,\n",
            "Epoch [265/1200], Training Loss: 1.6575, Validation Loss: 3.0626, Validation Percentage:33.5294%,\n",
            "Epoch [266/1200], Training Loss: 1.6312, Validation Loss: 3.1738, Validation Percentage:33.2353%,\n",
            "Epoch [267/1200], Training Loss: 1.6602, Validation Loss: 3.1156, Validation Percentage:33.2353%,\n",
            "Epoch [268/1200], Training Loss: 1.5492, Validation Loss: 2.8997, Validation Percentage:36.2745%,\n",
            "Epoch [269/1200], Training Loss: 1.5281, Validation Loss: 3.0465, Validation Percentage:34.5098%,\n",
            "Epoch [270/1200], Training Loss: 1.5581, Validation Loss: 3.0054, Validation Percentage:35.0980%,\n",
            "Epoch [271/1200], Training Loss: 1.6198, Validation Loss: 2.9372, Validation Percentage:34.5098%,\n",
            "Epoch [272/1200], Training Loss: 1.5711, Validation Loss: 2.9922, Validation Percentage:34.2157%,\n",
            "Epoch [273/1200], Training Loss: 1.5175, Validation Loss: 3.0877, Validation Percentage:35.6863%,\n",
            "Epoch [274/1200], Training Loss: 1.6180, Validation Loss: 2.9911, Validation Percentage:34.8039%,\n",
            "Epoch [275/1200], Training Loss: 1.4266, Validation Loss: 2.9789, Validation Percentage:37.0588%,\n",
            "Epoch [276/1200], Training Loss: 1.4322, Validation Loss: 3.0231, Validation Percentage:35.5882%,\n",
            "Epoch [277/1200], Training Loss: 1.5312, Validation Loss: 2.9006, Validation Percentage:34.5098%,\n",
            "Epoch [278/1200], Training Loss: 1.5834, Validation Loss: 2.9773, Validation Percentage:36.2745%,\n",
            "Epoch [279/1200], Training Loss: 1.4949, Validation Loss: 3.0040, Validation Percentage:36.1765%,\n",
            "Epoch [280/1200], Training Loss: 1.4164, Validation Loss: 3.0049, Validation Percentage:37.0588%,\n",
            "Epoch [281/1200], Training Loss: 1.4949, Validation Loss: 2.9769, Validation Percentage:37.9412%,\n",
            "Epoch [282/1200], Training Loss: 1.4404, Validation Loss: 2.9051, Validation Percentage:35.5882%,\n",
            "Epoch [283/1200], Training Loss: 1.4707, Validation Loss: 3.0251, Validation Percentage:35.4902%,\n",
            "Epoch [284/1200], Training Loss: 1.5317, Validation Loss: 2.9409, Validation Percentage:37.5490%,\n",
            "Epoch [285/1200], Training Loss: 1.5282, Validation Loss: 2.9897, Validation Percentage:34.1176%,\n",
            "Epoch [286/1200], Training Loss: 1.5408, Validation Loss: 2.8714, Validation Percentage:37.4510%,\n",
            "Epoch [287/1200], Training Loss: 1.4606, Validation Loss: 3.0075, Validation Percentage:36.8627%,\n",
            "Epoch [288/1200], Training Loss: 1.5149, Validation Loss: 2.9355, Validation Percentage:36.5686%,\n",
            "Epoch [289/1200], Training Loss: 1.5999, Validation Loss: 2.9526, Validation Percentage:36.7647%,\n",
            "Epoch [290/1200], Training Loss: 1.5160, Validation Loss: 3.0729, Validation Percentage:32.7451%,\n",
            "Epoch [291/1200], Training Loss: 1.4785, Validation Loss: 2.9045, Validation Percentage:35.0000%,\n",
            "Epoch [292/1200], Training Loss: 1.4721, Validation Loss: 2.9896, Validation Percentage:34.2157%,\n",
            "Epoch [293/1200], Training Loss: 1.5195, Validation Loss: 2.9487, Validation Percentage:36.0784%,\n",
            "Epoch [294/1200], Training Loss: 1.4910, Validation Loss: 2.9549, Validation Percentage:36.7647%,\n",
            "Epoch [295/1200], Training Loss: 1.5283, Validation Loss: 3.0292, Validation Percentage:35.9804%,\n",
            "Epoch [296/1200], Training Loss: 1.5004, Validation Loss: 2.9748, Validation Percentage:35.9804%,\n",
            "Epoch [297/1200], Training Loss: 1.4650, Validation Loss: 3.0596, Validation Percentage:35.8824%,\n",
            "Epoch [298/1200], Training Loss: 1.4108, Validation Loss: 3.0144, Validation Percentage:36.7647%,\n",
            "Epoch [299/1200], Training Loss: 1.4225, Validation Loss: 2.9807, Validation Percentage:33.8235%,\n",
            "Epoch [300/1200], Training Loss: 1.4412, Validation Loss: 2.9890, Validation Percentage:34.6078%,\n",
            "Epoch [301/1200], Training Loss: 1.5261, Validation Loss: 3.0913, Validation Percentage:35.4902%,\n",
            "Epoch [302/1200], Training Loss: 1.5232, Validation Loss: 3.1577, Validation Percentage:33.9216%,\n",
            "Epoch [303/1200], Training Loss: 1.4860, Validation Loss: 2.9432, Validation Percentage:34.3137%,\n",
            "Epoch [304/1200], Training Loss: 1.4414, Validation Loss: 2.9361, Validation Percentage:35.5882%,\n",
            "Epoch [305/1200], Training Loss: 1.4281, Validation Loss: 3.0006, Validation Percentage:36.4706%,\n",
            "Epoch [306/1200], Training Loss: 1.5087, Validation Loss: 3.0025, Validation Percentage:35.1961%,\n",
            "Epoch [307/1200], Training Loss: 1.4014, Validation Loss: 3.0091, Validation Percentage:35.7843%,\n",
            "Epoch [308/1200], Training Loss: 1.5151, Validation Loss: 2.9512, Validation Percentage:36.8627%,\n",
            "Epoch [309/1200], Training Loss: 1.4647, Validation Loss: 2.9904, Validation Percentage:38.0392%,\n",
            "Epoch [310/1200], Training Loss: 1.5591, Validation Loss: 2.9396, Validation Percentage:36.4706%,\n",
            "Epoch [311/1200], Training Loss: 1.4117, Validation Loss: 3.0919, Validation Percentage:36.1765%,\n",
            "Epoch [312/1200], Training Loss: 1.5385, Validation Loss: 2.8710, Validation Percentage:37.4510%,\n",
            "Epoch [313/1200], Training Loss: 1.4537, Validation Loss: 2.9351, Validation Percentage:36.6667%,\n",
            "Epoch [314/1200], Training Loss: 1.4541, Validation Loss: 2.8545, Validation Percentage:35.4902%,\n",
            "Epoch [315/1200], Training Loss: 1.3987, Validation Loss: 2.9883, Validation Percentage:36.7647%,\n",
            "Epoch [316/1200], Training Loss: 1.4077, Validation Loss: 3.0341, Validation Percentage:35.0000%,\n",
            "Epoch [317/1200], Training Loss: 1.4912, Validation Loss: 3.0076, Validation Percentage:35.0980%,\n",
            "Epoch [318/1200], Training Loss: 1.3881, Validation Loss: 3.0664, Validation Percentage:36.2745%,\n",
            "Epoch [319/1200], Training Loss: 1.4944, Validation Loss: 2.9238, Validation Percentage:37.0588%,\n",
            "Epoch [320/1200], Training Loss: 1.3518, Validation Loss: 3.0241, Validation Percentage:37.9412%,\n",
            "Epoch [321/1200], Training Loss: 1.3819, Validation Loss: 3.0528, Validation Percentage:35.8824%,\n",
            "Epoch [322/1200], Training Loss: 1.4178, Validation Loss: 2.9435, Validation Percentage:37.6471%,\n",
            "Epoch [323/1200], Training Loss: 1.3569, Validation Loss: 2.9624, Validation Percentage:38.2353%,\n",
            "Epoch [324/1200], Training Loss: 1.5191, Validation Loss: 2.9964, Validation Percentage:36.0784%,\n",
            "Epoch [325/1200], Training Loss: 1.4196, Validation Loss: 2.9129, Validation Percentage:36.9608%,\n",
            "Epoch [326/1200], Training Loss: 1.4999, Validation Loss: 3.1406, Validation Percentage:34.2157%,\n",
            "Epoch [327/1200], Training Loss: 1.4233, Validation Loss: 2.9861, Validation Percentage:40.0980%,\n",
            "Epoch [328/1200], Training Loss: 1.4015, Validation Loss: 3.0084, Validation Percentage:36.4706%,\n",
            "Epoch [329/1200], Training Loss: 1.5008, Validation Loss: 2.9009, Validation Percentage:34.4118%,\n",
            "Epoch [330/1200], Training Loss: 1.4358, Validation Loss: 2.9273, Validation Percentage:35.6863%,\n",
            "Epoch [331/1200], Training Loss: 1.4041, Validation Loss: 3.1514, Validation Percentage:34.6078%,\n",
            "Epoch [332/1200], Training Loss: 1.4404, Validation Loss: 2.9250, Validation Percentage:35.8824%,\n",
            "Epoch [333/1200], Training Loss: 1.3948, Validation Loss: 2.8742, Validation Percentage:36.5686%,\n",
            "Epoch [334/1200], Training Loss: 1.3295, Validation Loss: 3.0388, Validation Percentage:35.5882%,\n",
            "Epoch [335/1200], Training Loss: 1.4431, Validation Loss: 2.9171, Validation Percentage:38.5294%,\n",
            "Epoch [336/1200], Training Loss: 1.3714, Validation Loss: 3.1667, Validation Percentage:37.1569%,\n",
            "Epoch [337/1200], Training Loss: 1.4045, Validation Loss: 2.9013, Validation Percentage:35.6863%,\n",
            "Epoch [338/1200], Training Loss: 1.4557, Validation Loss: 2.9780, Validation Percentage:36.4706%,\n",
            "Epoch [339/1200], Training Loss: 1.4461, Validation Loss: 3.0082, Validation Percentage:36.8627%,\n",
            "Epoch [340/1200], Training Loss: 1.4166, Validation Loss: 3.0385, Validation Percentage:35.7843%,\n",
            "Epoch [341/1200], Training Loss: 1.3097, Validation Loss: 3.0890, Validation Percentage:36.2745%,\n",
            "Epoch [342/1200], Training Loss: 1.3475, Validation Loss: 3.1356, Validation Percentage:33.8235%,\n",
            "Epoch [343/1200], Training Loss: 1.3391, Validation Loss: 3.0665, Validation Percentage:37.1569%,\n",
            "Epoch [344/1200], Training Loss: 1.4036, Validation Loss: 3.0257, Validation Percentage:36.2745%,\n",
            "Epoch [345/1200], Training Loss: 1.3986, Validation Loss: 2.8920, Validation Percentage:37.3529%,\n",
            "Epoch [346/1200], Training Loss: 1.3361, Validation Loss: 2.9266, Validation Percentage:37.0588%,\n",
            "Epoch [347/1200], Training Loss: 1.3032, Validation Loss: 2.8740, Validation Percentage:40.4902%,\n",
            "Epoch [348/1200], Training Loss: 1.2955, Validation Loss: 3.0246, Validation Percentage:38.3333%,\n",
            "Epoch [349/1200], Training Loss: 1.4565, Validation Loss: 3.1565, Validation Percentage:36.0784%,\n",
            "Epoch [350/1200], Training Loss: 1.2596, Validation Loss: 3.2078, Validation Percentage:36.7647%,\n",
            "Epoch [351/1200], Training Loss: 1.4039, Validation Loss: 2.9154, Validation Percentage:36.1765%,\n",
            "Epoch [352/1200], Training Loss: 1.3394, Validation Loss: 2.8906, Validation Percentage:36.5686%,\n",
            "Epoch [353/1200], Training Loss: 1.4044, Validation Loss: 2.8778, Validation Percentage:34.9020%,\n",
            "Epoch [354/1200], Training Loss: 1.4890, Validation Loss: 2.7583, Validation Percentage:37.4510%,\n",
            "Epoch [355/1200], Training Loss: 1.4090, Validation Loss: 3.1291, Validation Percentage:39.1176%,\n",
            "Epoch [356/1200], Training Loss: 1.2899, Validation Loss: 3.0086, Validation Percentage:37.4510%,\n",
            "Epoch [357/1200], Training Loss: 1.3912, Validation Loss: 3.0200, Validation Percentage:36.8627%,\n",
            "Epoch [358/1200], Training Loss: 1.3847, Validation Loss: 2.8233, Validation Percentage:40.6863%,\n",
            "Epoch [359/1200], Training Loss: 1.2444, Validation Loss: 2.9681, Validation Percentage:37.8431%,\n",
            "Epoch [360/1200], Training Loss: 1.3399, Validation Loss: 2.9394, Validation Percentage:36.8627%,\n",
            "Epoch [361/1200], Training Loss: 1.3802, Validation Loss: 2.9092, Validation Percentage:36.8627%,\n",
            "Epoch [362/1200], Training Loss: 1.3348, Validation Loss: 2.8759, Validation Percentage:36.6667%,\n",
            "Epoch [363/1200], Training Loss: 1.4068, Validation Loss: 3.1061, Validation Percentage:35.9804%,\n",
            "Epoch [364/1200], Training Loss: 1.3321, Validation Loss: 2.9775, Validation Percentage:38.1373%,\n",
            "Epoch [365/1200], Training Loss: 1.3923, Validation Loss: 2.8317, Validation Percentage:39.5098%,\n",
            "Epoch [366/1200], Training Loss: 1.4582, Validation Loss: 2.9211, Validation Percentage:37.2549%,\n",
            "Epoch [367/1200], Training Loss: 1.3852, Validation Loss: 3.0049, Validation Percentage:36.1765%,\n",
            "Epoch [368/1200], Training Loss: 1.3971, Validation Loss: 3.0590, Validation Percentage:34.8039%,\n",
            "Epoch [369/1200], Training Loss: 1.4178, Validation Loss: 2.9108, Validation Percentage:37.3529%,\n",
            "Epoch [370/1200], Training Loss: 1.2671, Validation Loss: 2.8877, Validation Percentage:38.0392%,\n",
            "Epoch [371/1200], Training Loss: 1.3932, Validation Loss: 3.0241, Validation Percentage:35.6863%,\n",
            "Epoch [372/1200], Training Loss: 1.3118, Validation Loss: 2.8018, Validation Percentage:36.4706%,\n",
            "Epoch [373/1200], Training Loss: 1.3574, Validation Loss: 2.9707, Validation Percentage:36.8627%,\n",
            "Epoch [374/1200], Training Loss: 1.2487, Validation Loss: 3.0557, Validation Percentage:37.2549%,\n",
            "Epoch [375/1200], Training Loss: 1.3394, Validation Loss: 2.9807, Validation Percentage:38.9216%,\n",
            "Epoch [376/1200], Training Loss: 1.3520, Validation Loss: 2.8494, Validation Percentage:36.5686%,\n",
            "Epoch [377/1200], Training Loss: 1.2852, Validation Loss: 2.9941, Validation Percentage:39.0196%,\n",
            "Epoch [378/1200], Training Loss: 1.3059, Validation Loss: 3.0167, Validation Percentage:35.3922%,\n",
            "Epoch [379/1200], Training Loss: 1.2496, Validation Loss: 2.9931, Validation Percentage:38.1373%,\n",
            "Epoch [380/1200], Training Loss: 1.2974, Validation Loss: 3.1758, Validation Percentage:36.9608%,\n",
            "Epoch [381/1200], Training Loss: 1.2067, Validation Loss: 3.1535, Validation Percentage:38.2353%,\n",
            "Epoch [382/1200], Training Loss: 1.2970, Validation Loss: 3.0425, Validation Percentage:38.0392%,\n",
            "Epoch [383/1200], Training Loss: 1.2733, Validation Loss: 2.9573, Validation Percentage:38.9216%,\n",
            "Epoch [384/1200], Training Loss: 1.3270, Validation Loss: 2.7626, Validation Percentage:42.7451%,\n",
            "Epoch [385/1200], Training Loss: 1.2258, Validation Loss: 2.9064, Validation Percentage:37.6471%,\n",
            "Epoch [386/1200], Training Loss: 1.3004, Validation Loss: 3.0435, Validation Percentage:36.8627%,\n",
            "Epoch [387/1200], Training Loss: 1.3180, Validation Loss: 3.0959, Validation Percentage:37.5490%,\n",
            "Epoch [388/1200], Training Loss: 1.3359, Validation Loss: 3.0472, Validation Percentage:38.5294%,\n",
            "Epoch [389/1200], Training Loss: 1.4279, Validation Loss: 2.7545, Validation Percentage:39.5098%,\n",
            "Epoch [390/1200], Training Loss: 1.3186, Validation Loss: 3.0043, Validation Percentage:37.8431%,\n",
            "Epoch [391/1200], Training Loss: 1.2838, Validation Loss: 2.9859, Validation Percentage:36.4706%,\n",
            "Epoch [392/1200], Training Loss: 1.2256, Validation Loss: 3.0662, Validation Percentage:37.1569%,\n",
            "Epoch [393/1200], Training Loss: 1.2507, Validation Loss: 3.1297, Validation Percentage:35.0000%,\n",
            "Epoch [394/1200], Training Loss: 1.2776, Validation Loss: 3.0716, Validation Percentage:36.8627%,\n",
            "Epoch [395/1200], Training Loss: 1.3613, Validation Loss: 2.9481, Validation Percentage:36.5686%,\n",
            "Epoch [396/1200], Training Loss: 1.3954, Validation Loss: 2.9618, Validation Percentage:39.1176%,\n",
            "Epoch [397/1200], Training Loss: 1.2340, Validation Loss: 3.0698, Validation Percentage:35.4902%,\n",
            "Epoch [398/1200], Training Loss: 1.2227, Validation Loss: 3.2015, Validation Percentage:37.0588%,\n",
            "Epoch [399/1200], Training Loss: 1.3363, Validation Loss: 3.0371, Validation Percentage:35.2941%,\n",
            "Epoch [400/1200], Training Loss: 1.2636, Validation Loss: 3.0055, Validation Percentage:37.2549%,\n",
            "Epoch [401/1200], Training Loss: 1.2883, Validation Loss: 3.0251, Validation Percentage:37.0588%,\n",
            "Epoch [402/1200], Training Loss: 1.2936, Validation Loss: 2.9809, Validation Percentage:37.3529%,\n",
            "Epoch [403/1200], Training Loss: 1.2797, Validation Loss: 3.0519, Validation Percentage:37.8431%,\n",
            "Epoch [404/1200], Training Loss: 1.2490, Validation Loss: 2.9612, Validation Percentage:39.2157%,\n",
            "Epoch [405/1200], Training Loss: 1.1887, Validation Loss: 3.0949, Validation Percentage:36.6667%,\n",
            "Epoch [406/1200], Training Loss: 1.1805, Validation Loss: 2.9554, Validation Percentage:37.3529%,\n",
            "Epoch [407/1200], Training Loss: 1.3298, Validation Loss: 2.8251, Validation Percentage:37.3529%,\n",
            "Epoch [408/1200], Training Loss: 1.2515, Validation Loss: 2.8376, Validation Percentage:39.1176%,\n",
            "Epoch [409/1200], Training Loss: 1.2666, Validation Loss: 2.8575, Validation Percentage:39.2157%,\n",
            "Epoch [410/1200], Training Loss: 1.2023, Validation Loss: 2.9847, Validation Percentage:36.6667%,\n",
            "Epoch [411/1200], Training Loss: 1.2705, Validation Loss: 3.0362, Validation Percentage:38.2353%,\n",
            "Epoch [412/1200], Training Loss: 1.2776, Validation Loss: 3.0354, Validation Percentage:37.2549%,\n",
            "Epoch [413/1200], Training Loss: 1.1068, Validation Loss: 3.3696, Validation Percentage:35.0000%,\n",
            "Epoch [414/1200], Training Loss: 1.3625, Validation Loss: 3.0018, Validation Percentage:36.6667%,\n",
            "Epoch [415/1200], Training Loss: 1.1837, Validation Loss: 3.1248, Validation Percentage:36.4706%,\n",
            "Epoch [416/1200], Training Loss: 1.2754, Validation Loss: 3.0346, Validation Percentage:37.1569%,\n",
            "Epoch [417/1200], Training Loss: 1.3249, Validation Loss: 3.0121, Validation Percentage:38.3333%,\n",
            "Epoch [418/1200], Training Loss: 1.2285, Validation Loss: 3.0522, Validation Percentage:37.9412%,\n",
            "Epoch [419/1200], Training Loss: 1.2706, Validation Loss: 2.9681, Validation Percentage:37.3529%,\n",
            "Epoch [420/1200], Training Loss: 1.1462, Validation Loss: 2.9221, Validation Percentage:38.6275%,\n",
            "Epoch [421/1200], Training Loss: 1.2600, Validation Loss: 2.8547, Validation Percentage:38.7255%,\n",
            "Epoch [422/1200], Training Loss: 1.3485, Validation Loss: 3.1427, Validation Percentage:38.4314%,\n",
            "Epoch [423/1200], Training Loss: 1.2504, Validation Loss: 3.0850, Validation Percentage:38.2353%,\n",
            "Epoch [424/1200], Training Loss: 1.2101, Validation Loss: 3.1510, Validation Percentage:37.4510%,\n",
            "Epoch [425/1200], Training Loss: 1.3036, Validation Loss: 2.7783, Validation Percentage:41.0784%,\n",
            "Epoch [426/1200], Training Loss: 1.3471, Validation Loss: 2.7966, Validation Percentage:39.8039%,\n",
            "Epoch [427/1200], Training Loss: 1.2459, Validation Loss: 2.9002, Validation Percentage:40.0980%,\n",
            "Epoch [428/1200], Training Loss: 1.2631, Validation Loss: 2.8812, Validation Percentage:39.0196%,\n",
            "Epoch [429/1200], Training Loss: 1.1939, Validation Loss: 3.0158, Validation Percentage:38.6275%,\n",
            "Epoch [430/1200], Training Loss: 1.3233, Validation Loss: 2.8980, Validation Percentage:38.1373%,\n",
            "Epoch [431/1200], Training Loss: 1.1827, Validation Loss: 3.0781, Validation Percentage:39.1176%,\n",
            "Epoch [432/1200], Training Loss: 1.3205, Validation Loss: 2.9898, Validation Percentage:36.7647%,\n",
            "Epoch [433/1200], Training Loss: 1.2857, Validation Loss: 2.8508, Validation Percentage:38.6275%,\n",
            "Epoch [434/1200], Training Loss: 1.2063, Validation Loss: 2.9549, Validation Percentage:37.1569%,\n",
            "Epoch [435/1200], Training Loss: 1.2537, Validation Loss: 3.0326, Validation Percentage:36.2745%,\n",
            "Epoch [436/1200], Training Loss: 1.1598, Validation Loss: 3.1161, Validation Percentage:37.3529%,\n",
            "Epoch [437/1200], Training Loss: 1.1876, Validation Loss: 2.9376, Validation Percentage:36.7647%,\n",
            "Epoch [438/1200], Training Loss: 1.2206, Validation Loss: 2.8828, Validation Percentage:40.0980%,\n",
            "Epoch [439/1200], Training Loss: 1.1457, Validation Loss: 2.9434, Validation Percentage:40.4902%,\n",
            "Epoch [440/1200], Training Loss: 1.2228, Validation Loss: 3.0692, Validation Percentage:39.1176%,\n",
            "Epoch [441/1200], Training Loss: 1.2529, Validation Loss: 3.0501, Validation Percentage:37.8431%,\n",
            "Epoch [442/1200], Training Loss: 1.2443, Validation Loss: 2.9924, Validation Percentage:36.5686%,\n",
            "Epoch [443/1200], Training Loss: 1.2055, Validation Loss: 2.8808, Validation Percentage:38.4314%,\n",
            "Epoch [444/1200], Training Loss: 1.1555, Validation Loss: 2.9325, Validation Percentage:38.1373%,\n",
            "Epoch [445/1200], Training Loss: 1.3126, Validation Loss: 3.0378, Validation Percentage:36.3725%,\n",
            "Epoch [446/1200], Training Loss: 1.1532, Validation Loss: 2.8723, Validation Percentage:40.2941%,\n",
            "Epoch [447/1200], Training Loss: 1.3371, Validation Loss: 2.8477, Validation Percentage:40.2941%,\n",
            "Epoch [448/1200], Training Loss: 1.3257, Validation Loss: 2.9177, Validation Percentage:38.1373%,\n",
            "Epoch [449/1200], Training Loss: 1.2882, Validation Loss: 2.7970, Validation Percentage:40.7843%,\n",
            "Epoch [450/1200], Training Loss: 1.1639, Validation Loss: 2.9358, Validation Percentage:40.8824%,\n",
            "Epoch [451/1200], Training Loss: 1.2138, Validation Loss: 3.2074, Validation Percentage:36.9608%,\n",
            "Epoch [452/1200], Training Loss: 1.2772, Validation Loss: 2.7515, Validation Percentage:40.9804%,\n",
            "Epoch [453/1200], Training Loss: 1.2012, Validation Loss: 2.7980, Validation Percentage:41.5686%,\n",
            "Epoch [454/1200], Training Loss: 1.1157, Validation Loss: 3.0898, Validation Percentage:39.2157%,\n",
            "Epoch [455/1200], Training Loss: 1.0934, Validation Loss: 3.0367, Validation Percentage:40.2941%,\n",
            "Epoch [456/1200], Training Loss: 1.2568, Validation Loss: 3.1581, Validation Percentage:39.4118%,\n",
            "Epoch [457/1200], Training Loss: 1.3199, Validation Loss: 2.8718, Validation Percentage:40.1961%,\n",
            "Epoch [458/1200], Training Loss: 1.1486, Validation Loss: 2.8819, Validation Percentage:41.9608%,\n",
            "Epoch [459/1200], Training Loss: 1.2931, Validation Loss: 2.9413, Validation Percentage:39.4118%,\n",
            "Epoch [460/1200], Training Loss: 1.2608, Validation Loss: 3.0297, Validation Percentage:38.4314%,\n",
            "Epoch [461/1200], Training Loss: 1.1254, Validation Loss: 3.1785, Validation Percentage:39.0196%,\n",
            "Epoch [462/1200], Training Loss: 1.1678, Validation Loss: 3.0802, Validation Percentage:38.4314%,\n",
            "Epoch [463/1200], Training Loss: 1.3046, Validation Loss: 2.8062, Validation Percentage:39.8039%,\n",
            "Epoch [464/1200], Training Loss: 1.2306, Validation Loss: 2.9836, Validation Percentage:39.4118%,\n",
            "Epoch [465/1200], Training Loss: 1.1995, Validation Loss: 2.9293, Validation Percentage:38.7255%,\n",
            "Epoch [466/1200], Training Loss: 1.1970, Validation Loss: 2.9513, Validation Percentage:38.0392%,\n",
            "Epoch [467/1200], Training Loss: 1.1630, Validation Loss: 3.0385, Validation Percentage:40.9804%,\n",
            "Epoch [468/1200], Training Loss: 1.3390, Validation Loss: 2.8111, Validation Percentage:39.6078%,\n",
            "Epoch [469/1200], Training Loss: 1.3068, Validation Loss: 2.9732, Validation Percentage:39.3137%,\n",
            "Epoch [470/1200], Training Loss: 1.2910, Validation Loss: 2.9242, Validation Percentage:40.4902%,\n",
            "Epoch [471/1200], Training Loss: 1.3014, Validation Loss: 2.8018, Validation Percentage:39.5098%,\n",
            "Epoch [472/1200], Training Loss: 1.1864, Validation Loss: 2.9159, Validation Percentage:39.7059%,\n",
            "Epoch [473/1200], Training Loss: 1.2381, Validation Loss: 2.9997, Validation Percentage:37.7451%,\n",
            "Epoch [474/1200], Training Loss: 1.2601, Validation Loss: 3.1498, Validation Percentage:38.8235%,\n",
            "Epoch [475/1200], Training Loss: 1.1781, Validation Loss: 2.8621, Validation Percentage:36.7647%,\n",
            "Epoch [476/1200], Training Loss: 1.1624, Validation Loss: 2.9929, Validation Percentage:39.9020%,\n",
            "Epoch [477/1200], Training Loss: 1.0527, Validation Loss: 2.9468, Validation Percentage:41.1765%,\n",
            "Epoch [478/1200], Training Loss: 1.2265, Validation Loss: 2.9133, Validation Percentage:38.9216%,\n",
            "Epoch [479/1200], Training Loss: 1.2249, Validation Loss: 2.8393, Validation Percentage:39.1176%,\n",
            "Epoch [480/1200], Training Loss: 1.1201, Validation Loss: 2.9121, Validation Percentage:38.5294%,\n",
            "Epoch [481/1200], Training Loss: 1.2290, Validation Loss: 2.9147, Validation Percentage:38.9216%,\n",
            "Epoch [482/1200], Training Loss: 1.2032, Validation Loss: 2.9340, Validation Percentage:39.7059%,\n",
            "Epoch [483/1200], Training Loss: 1.1527, Validation Loss: 2.8778, Validation Percentage:40.2941%,\n",
            "Epoch [484/1200], Training Loss: 1.1526, Validation Loss: 2.8645, Validation Percentage:39.2157%,\n",
            "Epoch [485/1200], Training Loss: 1.1688, Validation Loss: 3.0600, Validation Percentage:37.5490%,\n",
            "Epoch [486/1200], Training Loss: 1.1476, Validation Loss: 2.9499, Validation Percentage:38.4314%,\n",
            "Epoch [487/1200], Training Loss: 1.0897, Validation Loss: 3.0468, Validation Percentage:39.3137%,\n",
            "Epoch [488/1200], Training Loss: 1.1065, Validation Loss: 3.0275, Validation Percentage:38.1373%,\n",
            "Epoch [489/1200], Training Loss: 1.2542, Validation Loss: 2.9084, Validation Percentage:40.0000%,\n",
            "Epoch [490/1200], Training Loss: 1.1233, Validation Loss: 2.9217, Validation Percentage:40.3922%,\n",
            "Epoch [491/1200], Training Loss: 1.1683, Validation Loss: 2.9124, Validation Percentage:40.5882%,\n",
            "Epoch [492/1200], Training Loss: 1.1801, Validation Loss: 3.1224, Validation Percentage:37.7451%,\n",
            "Epoch [493/1200], Training Loss: 1.1216, Validation Loss: 2.9454, Validation Percentage:39.0196%,\n",
            "Epoch [494/1200], Training Loss: 1.1787, Validation Loss: 3.2491, Validation Percentage:37.9412%,\n",
            "Epoch [495/1200], Training Loss: 1.1994, Validation Loss: 3.3036, Validation Percentage:39.1176%,\n",
            "Epoch [496/1200], Training Loss: 1.1270, Validation Loss: 2.9170, Validation Percentage:39.4118%,\n",
            "Epoch [497/1200], Training Loss: 1.1665, Validation Loss: 2.9313, Validation Percentage:38.4314%,\n",
            "Epoch [498/1200], Training Loss: 1.2190, Validation Loss: 2.8063, Validation Percentage:38.8235%,\n",
            "Epoch [499/1200], Training Loss: 1.1380, Validation Loss: 2.9894, Validation Percentage:41.2745%,\n",
            "Epoch [500/1200], Training Loss: 1.1608, Validation Loss: 2.9643, Validation Percentage:39.7059%,\n",
            "Epoch [501/1200], Training Loss: 1.1658, Validation Loss: 2.9522, Validation Percentage:41.5686%,\n",
            "Epoch [502/1200], Training Loss: 1.1209, Validation Loss: 2.9658, Validation Percentage:39.2157%,\n",
            "Epoch [503/1200], Training Loss: 1.1083, Validation Loss: 2.9559, Validation Percentage:39.3137%,\n",
            "Epoch [504/1200], Training Loss: 1.1495, Validation Loss: 2.8129, Validation Percentage:39.2157%,\n",
            "Epoch [505/1200], Training Loss: 1.1636, Validation Loss: 2.9763, Validation Percentage:39.1176%,\n",
            "Epoch [506/1200], Training Loss: 1.0553, Validation Loss: 2.9580, Validation Percentage:42.2549%,\n",
            "Epoch [507/1200], Training Loss: 1.2740, Validation Loss: 2.8533, Validation Percentage:39.5098%,\n",
            "Epoch [508/1200], Training Loss: 1.1514, Validation Loss: 2.9982, Validation Percentage:35.6863%,\n",
            "Epoch [509/1200], Training Loss: 1.1812, Validation Loss: 2.9815, Validation Percentage:39.1176%,\n",
            "Epoch [510/1200], Training Loss: 1.1823, Validation Loss: 2.8585, Validation Percentage:40.6863%,\n",
            "Epoch [511/1200], Training Loss: 1.0882, Validation Loss: 3.0949, Validation Percentage:39.5098%,\n",
            "Epoch [512/1200], Training Loss: 1.0701, Validation Loss: 2.9962, Validation Percentage:38.0392%,\n",
            "Epoch [513/1200], Training Loss: 1.1944, Validation Loss: 3.0633, Validation Percentage:38.4314%,\n",
            "Epoch [514/1200], Training Loss: 1.1036, Validation Loss: 3.0868, Validation Percentage:40.0980%,\n",
            "Epoch [515/1200], Training Loss: 1.1760, Validation Loss: 2.9519, Validation Percentage:37.9412%,\n",
            "Epoch [516/1200], Training Loss: 1.1414, Validation Loss: 2.8335, Validation Percentage:39.3137%,\n",
            "Epoch [517/1200], Training Loss: 1.1023, Validation Loss: 3.0195, Validation Percentage:40.1961%,\n",
            "Epoch [518/1200], Training Loss: 1.1476, Validation Loss: 2.7623, Validation Percentage:42.1569%,\n",
            "Epoch [519/1200], Training Loss: 1.1171, Validation Loss: 3.1738, Validation Percentage:37.1569%,\n",
            "Epoch [520/1200], Training Loss: 1.1406, Validation Loss: 2.9217, Validation Percentage:39.4118%,\n",
            "Epoch [521/1200], Training Loss: 1.1354, Validation Loss: 2.8750, Validation Percentage:39.5098%,\n",
            "Epoch [522/1200], Training Loss: 1.1723, Validation Loss: 2.9650, Validation Percentage:40.1961%,\n",
            "Epoch [523/1200], Training Loss: 1.2134, Validation Loss: 3.0619, Validation Percentage:41.6667%,\n",
            "Epoch [524/1200], Training Loss: 1.0350, Validation Loss: 3.0356, Validation Percentage:39.7059%,\n",
            "Epoch [525/1200], Training Loss: 1.1040, Validation Loss: 3.0928, Validation Percentage:41.1765%,\n",
            "Epoch [526/1200], Training Loss: 1.1202, Validation Loss: 2.9869, Validation Percentage:39.0196%,\n",
            "Epoch [527/1200], Training Loss: 1.0753, Validation Loss: 3.0679, Validation Percentage:39.1176%,\n",
            "Epoch [528/1200], Training Loss: 1.1769, Validation Loss: 3.0887, Validation Percentage:39.0196%,\n",
            "Epoch [529/1200], Training Loss: 1.1536, Validation Loss: 3.1515, Validation Percentage:38.9216%,\n",
            "Epoch [530/1200], Training Loss: 1.0292, Validation Loss: 2.9322, Validation Percentage:38.6275%,\n",
            "Epoch [531/1200], Training Loss: 1.1455, Validation Loss: 2.9695, Validation Percentage:38.7255%,\n",
            "Epoch [532/1200], Training Loss: 1.2050, Validation Loss: 3.0061, Validation Percentage:39.5098%,\n",
            "Epoch [533/1200], Training Loss: 1.1965, Validation Loss: 2.7032, Validation Percentage:40.4902%,\n",
            "Epoch [534/1200], Training Loss: 1.1171, Validation Loss: 3.0516, Validation Percentage:38.6275%,\n",
            "Epoch [535/1200], Training Loss: 1.1231, Validation Loss: 2.7893, Validation Percentage:39.5098%,\n",
            "Epoch [536/1200], Training Loss: 1.2408, Validation Loss: 2.9233, Validation Percentage:39.3137%,\n",
            "Epoch [537/1200], Training Loss: 1.2125, Validation Loss: 3.1360, Validation Percentage:39.5098%,\n",
            "Epoch [538/1200], Training Loss: 1.0352, Validation Loss: 2.9061, Validation Percentage:40.4902%,\n",
            "Epoch [539/1200], Training Loss: 1.1301, Validation Loss: 3.0115, Validation Percentage:39.5098%,\n",
            "Epoch [540/1200], Training Loss: 1.1149, Validation Loss: 2.9389, Validation Percentage:39.3137%,\n",
            "Epoch [541/1200], Training Loss: 1.1158, Validation Loss: 3.0323, Validation Percentage:38.8235%,\n",
            "Epoch [542/1200], Training Loss: 1.1135, Validation Loss: 2.8859, Validation Percentage:40.1961%,\n",
            "Epoch [543/1200], Training Loss: 1.1445, Validation Loss: 2.8074, Validation Percentage:40.8824%,\n",
            "Epoch [544/1200], Training Loss: 1.0904, Validation Loss: 3.1664, Validation Percentage:39.2157%,\n",
            "Epoch [545/1200], Training Loss: 1.1631, Validation Loss: 3.0770, Validation Percentage:38.0392%,\n",
            "Epoch [546/1200], Training Loss: 1.2080, Validation Loss: 3.0686, Validation Percentage:38.0392%,\n",
            "Epoch [547/1200], Training Loss: 1.1220, Validation Loss: 3.0293, Validation Percentage:38.3333%,\n",
            "Epoch [548/1200], Training Loss: 1.0663, Validation Loss: 2.9699, Validation Percentage:40.0980%,\n",
            "Epoch [549/1200], Training Loss: 1.1811, Validation Loss: 2.9311, Validation Percentage:40.9804%,\n",
            "Epoch [550/1200], Training Loss: 1.1050, Validation Loss: 3.3014, Validation Percentage:39.2157%,\n",
            "Epoch [551/1200], Training Loss: 1.1412, Validation Loss: 2.9031, Validation Percentage:38.2353%,\n",
            "Epoch [552/1200], Training Loss: 1.1419, Validation Loss: 2.9889, Validation Percentage:39.0196%,\n",
            "Epoch [553/1200], Training Loss: 1.1485, Validation Loss: 3.0068, Validation Percentage:37.8431%,\n",
            "Epoch [554/1200], Training Loss: 1.0568, Validation Loss: 2.9759, Validation Percentage:41.6667%,\n",
            "Epoch [555/1200], Training Loss: 1.0973, Validation Loss: 3.1063, Validation Percentage:42.1569%,\n",
            "Epoch [556/1200], Training Loss: 1.1781, Validation Loss: 3.1883, Validation Percentage:37.8431%,\n",
            "Epoch [557/1200], Training Loss: 1.3737, Validation Loss: 3.1723, Validation Percentage:38.0392%,\n",
            "Epoch [558/1200], Training Loss: 1.1299, Validation Loss: 2.9942, Validation Percentage:37.6471%,\n",
            "Epoch [559/1200], Training Loss: 1.1293, Validation Loss: 3.0979, Validation Percentage:39.4118%,\n",
            "Epoch [560/1200], Training Loss: 1.0657, Validation Loss: 3.0603, Validation Percentage:41.1765%,\n",
            "Epoch [561/1200], Training Loss: 1.1010, Validation Loss: 2.9268, Validation Percentage:40.8824%,\n",
            "Epoch [562/1200], Training Loss: 1.1566, Validation Loss: 3.0795, Validation Percentage:38.8235%,\n",
            "Epoch [563/1200], Training Loss: 1.0972, Validation Loss: 2.9697, Validation Percentage:39.9020%,\n",
            "Epoch [564/1200], Training Loss: 1.1491, Validation Loss: 3.1998, Validation Percentage:40.0000%,\n",
            "Epoch [565/1200], Training Loss: 1.1662, Validation Loss: 3.0554, Validation Percentage:39.4118%,\n",
            "Epoch [566/1200], Training Loss: 1.1892, Validation Loss: 3.0770, Validation Percentage:38.5294%,\n",
            "Epoch [567/1200], Training Loss: 1.1552, Validation Loss: 3.0770, Validation Percentage:40.3922%,\n",
            "Epoch [568/1200], Training Loss: 1.1636, Validation Loss: 3.0377, Validation Percentage:39.7059%,\n",
            "Epoch [569/1200], Training Loss: 1.0221, Validation Loss: 2.8032, Validation Percentage:40.3922%,\n",
            "Epoch [570/1200], Training Loss: 1.0299, Validation Loss: 3.0610, Validation Percentage:39.7059%,\n",
            "Epoch [571/1200], Training Loss: 1.0891, Validation Loss: 2.7313, Validation Percentage:42.1569%,\n",
            "Epoch [572/1200], Training Loss: 1.1106, Validation Loss: 2.9969, Validation Percentage:40.8824%,\n",
            "Epoch [573/1200], Training Loss: 1.0585, Validation Loss: 2.9829, Validation Percentage:42.5490%,\n",
            "Epoch [574/1200], Training Loss: 1.0806, Validation Loss: 3.1210, Validation Percentage:39.0196%,\n",
            "Epoch [575/1200], Training Loss: 1.0818, Validation Loss: 2.9431, Validation Percentage:41.5686%,\n",
            "Epoch [576/1200], Training Loss: 1.1123, Validation Loss: 3.0903, Validation Percentage:39.9020%,\n",
            "Epoch [577/1200], Training Loss: 1.1141, Validation Loss: 2.9393, Validation Percentage:40.4902%,\n",
            "Epoch [578/1200], Training Loss: 1.1467, Validation Loss: 3.0701, Validation Percentage:40.0000%,\n",
            "Epoch [579/1200], Training Loss: 1.1734, Validation Loss: 2.8211, Validation Percentage:40.8824%,\n",
            "Epoch [580/1200], Training Loss: 0.9734, Validation Loss: 3.0744, Validation Percentage:41.2745%,\n",
            "Epoch [581/1200], Training Loss: 1.0289, Validation Loss: 3.1476, Validation Percentage:40.2941%,\n",
            "Epoch [582/1200], Training Loss: 1.0202, Validation Loss: 3.0423, Validation Percentage:38.8235%,\n",
            "Epoch [583/1200], Training Loss: 1.0089, Validation Loss: 3.0825, Validation Percentage:39.5098%,\n",
            "Epoch [584/1200], Training Loss: 1.0358, Validation Loss: 2.9123, Validation Percentage:41.2745%,\n",
            "Epoch [585/1200], Training Loss: 1.0870, Validation Loss: 3.1385, Validation Percentage:38.9216%,\n",
            "Epoch [586/1200], Training Loss: 1.0919, Validation Loss: 3.0139, Validation Percentage:42.0588%,\n",
            "Epoch [587/1200], Training Loss: 1.0768, Validation Loss: 2.9763, Validation Percentage:41.8627%,\n",
            "Epoch [588/1200], Training Loss: 1.1712, Validation Loss: 3.2044, Validation Percentage:41.3725%,\n",
            "Epoch [589/1200], Training Loss: 1.1984, Validation Loss: 2.9158, Validation Percentage:41.3725%,\n",
            "Epoch [590/1200], Training Loss: 1.1224, Validation Loss: 3.0855, Validation Percentage:38.5294%,\n",
            "Epoch [591/1200], Training Loss: 1.1979, Validation Loss: 2.9418, Validation Percentage:39.2157%,\n",
            "Epoch [592/1200], Training Loss: 1.1089, Validation Loss: 2.8984, Validation Percentage:39.8039%,\n",
            "Epoch [593/1200], Training Loss: 1.1471, Validation Loss: 3.0251, Validation Percentage:40.0980%,\n",
            "Epoch [594/1200], Training Loss: 1.0518, Validation Loss: 2.9547, Validation Percentage:40.8824%,\n",
            "Epoch [595/1200], Training Loss: 1.1179, Validation Loss: 3.1263, Validation Percentage:41.1765%,\n",
            "Epoch [596/1200], Training Loss: 1.0086, Validation Loss: 3.0193, Validation Percentage:41.1765%,\n",
            "Epoch [597/1200], Training Loss: 1.1110, Validation Loss: 2.8540, Validation Percentage:38.7255%,\n",
            "Epoch [598/1200], Training Loss: 1.1309, Validation Loss: 3.0764, Validation Percentage:41.7647%,\n",
            "Epoch [599/1200], Training Loss: 1.1053, Validation Loss: 2.9905, Validation Percentage:41.4706%,\n",
            "Epoch [600/1200], Training Loss: 1.0172, Validation Loss: 2.9883, Validation Percentage:42.5490%,\n",
            "Epoch [601/1200], Training Loss: 1.0702, Validation Loss: 3.0372, Validation Percentage:37.8431%,\n",
            "Epoch [602/1200], Training Loss: 1.0733, Validation Loss: 3.0599, Validation Percentage:38.4314%,\n",
            "Epoch [603/1200], Training Loss: 1.1272, Validation Loss: 2.9458, Validation Percentage:40.1961%,\n",
            "Epoch [604/1200], Training Loss: 1.0880, Validation Loss: 2.9243, Validation Percentage:42.1569%,\n",
            "Epoch [605/1200], Training Loss: 1.0454, Validation Loss: 2.6987, Validation Percentage:42.8431%,\n",
            "Epoch [606/1200], Training Loss: 1.0961, Validation Loss: 2.8942, Validation Percentage:40.1961%,\n",
            "Epoch [607/1200], Training Loss: 1.0916, Validation Loss: 3.1901, Validation Percentage:40.9804%,\n",
            "Epoch [608/1200], Training Loss: 1.0029, Validation Loss: 3.0868, Validation Percentage:41.2745%,\n",
            "Epoch [609/1200], Training Loss: 1.0394, Validation Loss: 2.8783, Validation Percentage:42.4510%,\n",
            "Epoch [610/1200], Training Loss: 1.0181, Validation Loss: 3.1372, Validation Percentage:40.8824%,\n",
            "Epoch [611/1200], Training Loss: 1.0661, Validation Loss: 2.9044, Validation Percentage:42.4510%,\n",
            "Epoch [612/1200], Training Loss: 1.0712, Validation Loss: 2.8602, Validation Percentage:43.3333%,\n",
            "Epoch [613/1200], Training Loss: 1.0578, Validation Loss: 3.1128, Validation Percentage:41.2745%,\n",
            "Epoch [614/1200], Training Loss: 1.0412, Validation Loss: 3.0397, Validation Percentage:39.8039%,\n",
            "Epoch [615/1200], Training Loss: 1.1215, Validation Loss: 2.9837, Validation Percentage:41.7647%,\n",
            "Epoch [616/1200], Training Loss: 1.0752, Validation Loss: 2.9993, Validation Percentage:42.4510%,\n",
            "Epoch [617/1200], Training Loss: 1.0257, Validation Loss: 2.9018, Validation Percentage:45.0000%,\n",
            "Epoch [618/1200], Training Loss: 1.0504, Validation Loss: 2.8935, Validation Percentage:39.8039%,\n",
            "Epoch [619/1200], Training Loss: 1.1151, Validation Loss: 2.7839, Validation Percentage:42.1569%,\n",
            "Epoch [620/1200], Training Loss: 1.1540, Validation Loss: 3.1277, Validation Percentage:38.9216%,\n",
            "Epoch [621/1200], Training Loss: 1.0371, Validation Loss: 2.9789, Validation Percentage:40.9804%,\n",
            "Epoch [622/1200], Training Loss: 1.1146, Validation Loss: 3.0356, Validation Percentage:39.7059%,\n",
            "Epoch [623/1200], Training Loss: 1.0565, Validation Loss: 3.0393, Validation Percentage:40.9804%,\n",
            "Epoch [624/1200], Training Loss: 1.0323, Validation Loss: 2.9565, Validation Percentage:40.0980%,\n",
            "Epoch [625/1200], Training Loss: 1.0779, Validation Loss: 2.8752, Validation Percentage:40.0000%,\n",
            "Epoch [626/1200], Training Loss: 1.1365, Validation Loss: 2.9019, Validation Percentage:40.9804%,\n",
            "Epoch [627/1200], Training Loss: 1.0567, Validation Loss: 2.9237, Validation Percentage:39.6078%,\n",
            "Epoch [628/1200], Training Loss: 0.9967, Validation Loss: 3.1211, Validation Percentage:39.2157%,\n",
            "Epoch [629/1200], Training Loss: 1.0007, Validation Loss: 2.9054, Validation Percentage:40.4902%,\n",
            "Epoch [630/1200], Training Loss: 1.0349, Validation Loss: 2.9543, Validation Percentage:38.2353%,\n",
            "Epoch [631/1200], Training Loss: 1.0503, Validation Loss: 3.0525, Validation Percentage:39.1176%,\n",
            "Epoch [632/1200], Training Loss: 1.0356, Validation Loss: 3.2046, Validation Percentage:40.4902%,\n",
            "Epoch [633/1200], Training Loss: 1.2376, Validation Loss: 3.0993, Validation Percentage:40.3922%,\n",
            "Epoch [634/1200], Training Loss: 1.1007, Validation Loss: 3.0983, Validation Percentage:42.1569%,\n",
            "Epoch [635/1200], Training Loss: 0.9454, Validation Loss: 3.0032, Validation Percentage:43.4314%,\n",
            "Epoch [636/1200], Training Loss: 0.9796, Validation Loss: 2.9804, Validation Percentage:41.6667%,\n",
            "Epoch [637/1200], Training Loss: 1.2183, Validation Loss: 2.7787, Validation Percentage:40.6863%,\n",
            "Epoch [638/1200], Training Loss: 1.1321, Validation Loss: 3.0553, Validation Percentage:40.7843%,\n",
            "Epoch [639/1200], Training Loss: 0.9321, Validation Loss: 3.0435, Validation Percentage:40.9804%,\n",
            "Epoch [640/1200], Training Loss: 1.0506, Validation Loss: 3.2239, Validation Percentage:40.4902%,\n",
            "Epoch [641/1200], Training Loss: 0.9952, Validation Loss: 3.0661, Validation Percentage:40.0000%,\n",
            "Epoch [642/1200], Training Loss: 1.1302, Validation Loss: 2.9012, Validation Percentage:41.5686%,\n",
            "Epoch [643/1200], Training Loss: 1.0533, Validation Loss: 3.1372, Validation Percentage:37.9412%,\n",
            "Epoch [644/1200], Training Loss: 1.0367, Validation Loss: 3.0646, Validation Percentage:39.9020%,\n",
            "Epoch [645/1200], Training Loss: 1.0868, Validation Loss: 2.8826, Validation Percentage:43.2353%,\n",
            "Epoch [646/1200], Training Loss: 1.0586, Validation Loss: 2.9520, Validation Percentage:40.0980%,\n",
            "Epoch [647/1200], Training Loss: 1.0317, Validation Loss: 3.0270, Validation Percentage:40.6863%,\n",
            "Epoch [648/1200], Training Loss: 1.0879, Validation Loss: 2.9638, Validation Percentage:39.3137%,\n",
            "Epoch [649/1200], Training Loss: 0.9955, Validation Loss: 3.2339, Validation Percentage:39.8039%,\n",
            "Epoch [650/1200], Training Loss: 1.0235, Validation Loss: 3.1354, Validation Percentage:40.0000%,\n",
            "Epoch [651/1200], Training Loss: 1.0727, Validation Loss: 3.2105, Validation Percentage:40.0000%,\n",
            "Epoch [652/1200], Training Loss: 1.1123, Validation Loss: 3.1287, Validation Percentage:41.3725%,\n",
            "Epoch [653/1200], Training Loss: 0.9832, Validation Loss: 2.9134, Validation Percentage:38.1373%,\n",
            "Epoch [654/1200], Training Loss: 1.0158, Validation Loss: 3.0621, Validation Percentage:39.7059%,\n",
            "Epoch [655/1200], Training Loss: 1.1128, Validation Loss: 3.2200, Validation Percentage:40.5882%,\n",
            "Epoch [656/1200], Training Loss: 1.0937, Validation Loss: 2.9439, Validation Percentage:40.5882%,\n",
            "Epoch [657/1200], Training Loss: 0.9876, Validation Loss: 3.1237, Validation Percentage:41.5686%,\n",
            "Epoch [658/1200], Training Loss: 1.1267, Validation Loss: 2.9663, Validation Percentage:39.2157%,\n",
            "Epoch [659/1200], Training Loss: 1.0486, Validation Loss: 2.9029, Validation Percentage:41.4706%,\n",
            "Epoch [660/1200], Training Loss: 0.9503, Validation Loss: 3.1903, Validation Percentage:39.1176%,\n",
            "Epoch [661/1200], Training Loss: 0.9741, Validation Loss: 3.1455, Validation Percentage:39.9020%,\n",
            "Epoch [662/1200], Training Loss: 1.0035, Validation Loss: 3.2648, Validation Percentage:40.5882%,\n",
            "Epoch [663/1200], Training Loss: 1.0171, Validation Loss: 2.7978, Validation Percentage:41.6667%,\n",
            "Epoch [664/1200], Training Loss: 1.0552, Validation Loss: 2.7871, Validation Percentage:41.6667%,\n",
            "Epoch [665/1200], Training Loss: 1.0263, Validation Loss: 3.0736, Validation Percentage:41.8627%,\n",
            "Epoch [666/1200], Training Loss: 0.9864, Validation Loss: 2.8454, Validation Percentage:40.5882%,\n",
            "Epoch [667/1200], Training Loss: 1.0678, Validation Loss: 3.1273, Validation Percentage:41.5686%,\n",
            "Epoch [668/1200], Training Loss: 1.0430, Validation Loss: 3.1279, Validation Percentage:38.2353%,\n",
            "Epoch [669/1200], Training Loss: 1.1327, Validation Loss: 2.9809, Validation Percentage:40.2941%,\n",
            "Epoch [670/1200], Training Loss: 1.0126, Validation Loss: 3.3259, Validation Percentage:43.6275%,\n",
            "Epoch [671/1200], Training Loss: 0.9915, Validation Loss: 3.1291, Validation Percentage:39.8039%,\n",
            "Epoch [672/1200], Training Loss: 1.0106, Validation Loss: 3.0621, Validation Percentage:39.5098%,\n",
            "Epoch [673/1200], Training Loss: 1.0342, Validation Loss: 3.1190, Validation Percentage:39.8039%,\n",
            "Epoch [674/1200], Training Loss: 1.0275, Validation Loss: 2.9048, Validation Percentage:43.2353%,\n",
            "Epoch [675/1200], Training Loss: 1.0245, Validation Loss: 3.0143, Validation Percentage:41.3725%,\n",
            "Epoch [676/1200], Training Loss: 1.0293, Validation Loss: 3.0712, Validation Percentage:40.8824%,\n",
            "Epoch [677/1200], Training Loss: 1.0409, Validation Loss: 3.0302, Validation Percentage:40.4902%,\n",
            "Epoch [678/1200], Training Loss: 1.0396, Validation Loss: 3.1998, Validation Percentage:39.8039%,\n",
            "Epoch [679/1200], Training Loss: 1.0047, Validation Loss: 3.0401, Validation Percentage:40.5882%,\n",
            "Epoch [680/1200], Training Loss: 1.0290, Validation Loss: 3.1497, Validation Percentage:40.6863%,\n",
            "Epoch [681/1200], Training Loss: 0.9881, Validation Loss: 3.0719, Validation Percentage:42.3529%,\n",
            "Epoch [682/1200], Training Loss: 1.1111, Validation Loss: 3.1285, Validation Percentage:39.0196%,\n",
            "Epoch [683/1200], Training Loss: 1.0153, Validation Loss: 3.0790, Validation Percentage:39.5098%,\n",
            "Epoch [684/1200], Training Loss: 1.0541, Validation Loss: 3.1760, Validation Percentage:42.4510%,\n",
            "Epoch [685/1200], Training Loss: 1.1049, Validation Loss: 2.8803, Validation Percentage:42.6471%,\n",
            "Epoch [686/1200], Training Loss: 0.9772, Validation Loss: 2.9715, Validation Percentage:39.9020%,\n",
            "Epoch [687/1200], Training Loss: 0.9377, Validation Loss: 3.2038, Validation Percentage:40.9804%,\n",
            "Epoch [688/1200], Training Loss: 1.0560, Validation Loss: 3.0717, Validation Percentage:41.6667%,\n",
            "Epoch [689/1200], Training Loss: 0.9822, Validation Loss: 2.9315, Validation Percentage:43.8235%,\n",
            "Epoch [690/1200], Training Loss: 0.9386, Validation Loss: 3.1050, Validation Percentage:40.6863%,\n",
            "Epoch [691/1200], Training Loss: 1.0017, Validation Loss: 3.1235, Validation Percentage:43.3333%,\n",
            "Epoch [692/1200], Training Loss: 1.0070, Validation Loss: 3.4161, Validation Percentage:37.9412%,\n",
            "Epoch [693/1200], Training Loss: 1.0191, Validation Loss: 3.4222, Validation Percentage:39.2157%,\n",
            "Epoch [694/1200], Training Loss: 0.9844, Validation Loss: 3.0032, Validation Percentage:40.6863%,\n",
            "Epoch [695/1200], Training Loss: 1.0039, Validation Loss: 3.3040, Validation Percentage:41.4706%,\n",
            "Epoch [696/1200], Training Loss: 0.9771, Validation Loss: 2.9386, Validation Percentage:41.8627%,\n",
            "Epoch [697/1200], Training Loss: 0.8993, Validation Loss: 3.0482, Validation Percentage:42.4510%,\n",
            "Epoch [698/1200], Training Loss: 0.9403, Validation Loss: 3.2924, Validation Percentage:38.8235%,\n",
            "Epoch [699/1200], Training Loss: 1.1094, Validation Loss: 2.9942, Validation Percentage:40.1961%,\n",
            "Epoch [700/1200], Training Loss: 1.0676, Validation Loss: 2.9751, Validation Percentage:40.9804%,\n",
            "Epoch [701/1200], Training Loss: 1.0727, Validation Loss: 3.1271, Validation Percentage:41.3725%,\n",
            "Epoch [702/1200], Training Loss: 1.0534, Validation Loss: 3.0455, Validation Percentage:41.8627%,\n",
            "Epoch [703/1200], Training Loss: 0.9729, Validation Loss: 2.9617, Validation Percentage:39.9020%,\n",
            "Epoch [704/1200], Training Loss: 1.0682, Validation Loss: 2.9897, Validation Percentage:40.0000%,\n",
            "Epoch [705/1200], Training Loss: 0.9288, Validation Loss: 3.1231, Validation Percentage:40.4902%,\n",
            "Epoch [706/1200], Training Loss: 0.9611, Validation Loss: 3.1108, Validation Percentage:42.2549%,\n",
            "Epoch [707/1200], Training Loss: 1.0732, Validation Loss: 2.9638, Validation Percentage:40.7843%,\n",
            "Epoch [708/1200], Training Loss: 1.0462, Validation Loss: 2.8547, Validation Percentage:40.6863%,\n",
            "Epoch [709/1200], Training Loss: 1.0406, Validation Loss: 3.0339, Validation Percentage:39.4118%,\n",
            "Epoch [710/1200], Training Loss: 0.9037, Validation Loss: 2.9688, Validation Percentage:42.0588%,\n",
            "Epoch [711/1200], Training Loss: 0.9153, Validation Loss: 3.1299, Validation Percentage:40.2941%,\n",
            "Epoch [712/1200], Training Loss: 1.0582, Validation Loss: 2.9858, Validation Percentage:42.8431%,\n",
            "Epoch [713/1200], Training Loss: 0.9589, Validation Loss: 3.0492, Validation Percentage:40.6863%,\n",
            "Epoch [714/1200], Training Loss: 1.0877, Validation Loss: 2.8078, Validation Percentage:41.3725%,\n",
            "Epoch [715/1200], Training Loss: 0.9176, Validation Loss: 3.0244, Validation Percentage:42.1569%,\n",
            "Epoch [716/1200], Training Loss: 1.1273, Validation Loss: 2.8067, Validation Percentage:42.7451%,\n",
            "Epoch [717/1200], Training Loss: 0.8875, Validation Loss: 3.0357, Validation Percentage:43.6275%,\n",
            "Epoch [718/1200], Training Loss: 1.0961, Validation Loss: 2.7930, Validation Percentage:42.9412%,\n",
            "Epoch [719/1200], Training Loss: 1.0241, Validation Loss: 2.9020, Validation Percentage:42.7451%,\n",
            "Epoch [720/1200], Training Loss: 1.0535, Validation Loss: 2.8863, Validation Percentage:45.1961%,\n",
            "Epoch [721/1200], Training Loss: 0.9459, Validation Loss: 3.0363, Validation Percentage:41.9608%,\n",
            "Epoch [722/1200], Training Loss: 0.9706, Validation Loss: 3.1076, Validation Percentage:42.6471%,\n",
            "Epoch [723/1200], Training Loss: 0.8446, Validation Loss: 3.1089, Validation Percentage:40.3922%,\n",
            "Epoch [724/1200], Training Loss: 0.9997, Validation Loss: 3.0659, Validation Percentage:41.7647%,\n",
            "Epoch [725/1200], Training Loss: 0.9844, Validation Loss: 3.2230, Validation Percentage:42.4510%,\n",
            "Epoch [726/1200], Training Loss: 0.9472, Validation Loss: 3.0803, Validation Percentage:41.5686%,\n",
            "Epoch [727/1200], Training Loss: 0.9046, Validation Loss: 3.2945, Validation Percentage:40.9804%,\n",
            "Epoch [728/1200], Training Loss: 1.0635, Validation Loss: 2.8960, Validation Percentage:42.1569%,\n",
            "Epoch [729/1200], Training Loss: 1.0153, Validation Loss: 2.9302, Validation Percentage:41.1765%,\n",
            "Epoch [730/1200], Training Loss: 1.0190, Validation Loss: 3.0726, Validation Percentage:43.0392%,\n",
            "Epoch [731/1200], Training Loss: 1.0049, Validation Loss: 3.1108, Validation Percentage:39.6078%,\n",
            "Epoch [732/1200], Training Loss: 1.0262, Validation Loss: 3.0859, Validation Percentage:41.9608%,\n",
            "Epoch [733/1200], Training Loss: 0.9668, Validation Loss: 3.0166, Validation Percentage:42.0588%,\n",
            "Epoch [734/1200], Training Loss: 1.0144, Validation Loss: 3.1609, Validation Percentage:39.9020%,\n",
            "Epoch [735/1200], Training Loss: 1.0565, Validation Loss: 2.8502, Validation Percentage:44.0196%,\n",
            "Epoch [736/1200], Training Loss: 1.0150, Validation Loss: 2.9080, Validation Percentage:40.8824%,\n",
            "Epoch [737/1200], Training Loss: 1.0530, Validation Loss: 2.9274, Validation Percentage:41.5686%,\n",
            "Epoch [738/1200], Training Loss: 0.9779, Validation Loss: 2.9656, Validation Percentage:40.2941%,\n",
            "Epoch [739/1200], Training Loss: 0.9350, Validation Loss: 3.0044, Validation Percentage:41.7647%,\n",
            "Epoch [740/1200], Training Loss: 0.9863, Validation Loss: 3.0871, Validation Percentage:41.0784%,\n",
            "Epoch [741/1200], Training Loss: 0.9835, Validation Loss: 3.3247, Validation Percentage:40.6863%,\n",
            "Epoch [742/1200], Training Loss: 1.0550, Validation Loss: 2.8628, Validation Percentage:40.9804%,\n",
            "Epoch [743/1200], Training Loss: 0.9745, Validation Loss: 2.7331, Validation Percentage:43.7255%,\n",
            "Epoch [744/1200], Training Loss: 0.8264, Validation Loss: 2.9152, Validation Percentage:43.4314%,\n",
            "Epoch [745/1200], Training Loss: 0.9043, Validation Loss: 3.1341, Validation Percentage:39.5098%,\n",
            "Epoch [746/1200], Training Loss: 0.9690, Validation Loss: 3.4049, Validation Percentage:41.2745%,\n",
            "Epoch [747/1200], Training Loss: 0.9288, Validation Loss: 2.9339, Validation Percentage:41.1765%,\n",
            "Epoch [748/1200], Training Loss: 0.9538, Validation Loss: 3.0414, Validation Percentage:40.5882%,\n",
            "Epoch [749/1200], Training Loss: 1.0470, Validation Loss: 3.1128, Validation Percentage:42.6471%,\n",
            "Epoch [750/1200], Training Loss: 1.0080, Validation Loss: 3.0069, Validation Percentage:45.0000%,\n",
            "Epoch [751/1200], Training Loss: 0.9738, Validation Loss: 3.2041, Validation Percentage:40.5882%,\n",
            "Epoch [752/1200], Training Loss: 1.0029, Validation Loss: 3.2746, Validation Percentage:42.3529%,\n",
            "Epoch [753/1200], Training Loss: 0.9405, Validation Loss: 2.8891, Validation Percentage:44.8039%,\n",
            "Epoch [754/1200], Training Loss: 0.9732, Validation Loss: 2.9242, Validation Percentage:40.9804%,\n",
            "Epoch [755/1200], Training Loss: 1.0227, Validation Loss: 2.9777, Validation Percentage:42.9412%,\n",
            "Epoch [756/1200], Training Loss: 0.9506, Validation Loss: 3.0861, Validation Percentage:42.9412%,\n",
            "Epoch [757/1200], Training Loss: 1.0836, Validation Loss: 3.0318, Validation Percentage:41.2745%,\n",
            "Epoch [758/1200], Training Loss: 1.0128, Validation Loss: 3.0460, Validation Percentage:42.8431%,\n",
            "Epoch [759/1200], Training Loss: 0.9966, Validation Loss: 2.9985, Validation Percentage:41.3725%,\n",
            "Epoch [760/1200], Training Loss: 1.0380, Validation Loss: 2.7983, Validation Percentage:41.3725%,\n",
            "Epoch [761/1200], Training Loss: 0.9725, Validation Loss: 2.9020, Validation Percentage:41.9608%,\n",
            "Epoch [762/1200], Training Loss: 0.9451, Validation Loss: 3.1359, Validation Percentage:43.4314%,\n",
            "Epoch [763/1200], Training Loss: 0.9896, Validation Loss: 3.0170, Validation Percentage:42.9412%,\n",
            "Epoch [764/1200], Training Loss: 0.9774, Validation Loss: 3.2324, Validation Percentage:38.9216%,\n",
            "Epoch [765/1200], Training Loss: 0.9874, Validation Loss: 3.1244, Validation Percentage:43.1373%,\n",
            "Epoch [766/1200], Training Loss: 1.0512, Validation Loss: 3.0839, Validation Percentage:40.9804%,\n",
            "Epoch [767/1200], Training Loss: 1.0279, Validation Loss: 2.8692, Validation Percentage:41.0784%,\n",
            "Epoch [768/1200], Training Loss: 0.9781, Validation Loss: 3.0645, Validation Percentage:40.8824%,\n",
            "Epoch [769/1200], Training Loss: 0.9205, Validation Loss: 3.1076, Validation Percentage:41.1765%,\n",
            "Epoch [770/1200], Training Loss: 0.9466, Validation Loss: 3.0501, Validation Percentage:42.5490%,\n",
            "Epoch [771/1200], Training Loss: 0.9573, Validation Loss: 2.9728, Validation Percentage:43.2353%,\n",
            "Epoch [772/1200], Training Loss: 0.9610, Validation Loss: 3.0813, Validation Percentage:44.4118%,\n",
            "Epoch [773/1200], Training Loss: 1.0177, Validation Loss: 3.2099, Validation Percentage:39.9020%,\n",
            "Epoch [774/1200], Training Loss: 1.0528, Validation Loss: 3.1243, Validation Percentage:39.0196%,\n",
            "Epoch [775/1200], Training Loss: 0.9868, Validation Loss: 3.1690, Validation Percentage:40.2941%,\n",
            "Epoch [776/1200], Training Loss: 0.9173, Validation Loss: 3.0882, Validation Percentage:40.2941%,\n",
            "Epoch [777/1200], Training Loss: 0.9884, Validation Loss: 3.0348, Validation Percentage:41.1765%,\n",
            "Epoch [778/1200], Training Loss: 1.0043, Validation Loss: 3.1294, Validation Percentage:39.9020%,\n",
            "Epoch [779/1200], Training Loss: 0.9053, Validation Loss: 3.1339, Validation Percentage:39.6078%,\n",
            "Epoch [780/1200], Training Loss: 0.9383, Validation Loss: 3.2469, Validation Percentage:40.9804%,\n",
            "Epoch [781/1200], Training Loss: 0.9832, Validation Loss: 3.1919, Validation Percentage:40.8824%,\n",
            "Epoch [782/1200], Training Loss: 1.0463, Validation Loss: 3.2318, Validation Percentage:42.6471%,\n",
            "Epoch [783/1200], Training Loss: 0.9770, Validation Loss: 3.3723, Validation Percentage:39.3137%,\n",
            "Epoch [784/1200], Training Loss: 0.9243, Validation Loss: 3.0991, Validation Percentage:40.0980%,\n",
            "Epoch [785/1200], Training Loss: 1.0577, Validation Loss: 2.7614, Validation Percentage:42.6471%,\n",
            "Epoch [786/1200], Training Loss: 1.0546, Validation Loss: 2.8930, Validation Percentage:43.6275%,\n",
            "Epoch [787/1200], Training Loss: 1.0407, Validation Loss: 3.0179, Validation Percentage:39.3137%,\n",
            "Epoch [788/1200], Training Loss: 1.0162, Validation Loss: 3.0882, Validation Percentage:39.9020%,\n",
            "Epoch [789/1200], Training Loss: 0.9329, Validation Loss: 3.2339, Validation Percentage:42.0588%,\n",
            "Epoch [790/1200], Training Loss: 0.9346, Validation Loss: 3.0999, Validation Percentage:42.6471%,\n",
            "Epoch [791/1200], Training Loss: 0.8594, Validation Loss: 3.0609, Validation Percentage:40.1961%,\n",
            "Epoch [792/1200], Training Loss: 1.0146, Validation Loss: 3.1002, Validation Percentage:42.1569%,\n",
            "Epoch [793/1200], Training Loss: 0.9946, Validation Loss: 3.0341, Validation Percentage:43.6275%,\n",
            "Epoch [794/1200], Training Loss: 0.9051, Validation Loss: 3.2168, Validation Percentage:40.2941%,\n",
            "Epoch [795/1200], Training Loss: 0.9006, Validation Loss: 3.1187, Validation Percentage:41.2745%,\n",
            "Epoch [796/1200], Training Loss: 1.0490, Validation Loss: 2.9963, Validation Percentage:40.9804%,\n",
            "Epoch [797/1200], Training Loss: 0.8738, Validation Loss: 3.0858, Validation Percentage:41.8627%,\n",
            "Epoch [798/1200], Training Loss: 0.9677, Validation Loss: 2.9934, Validation Percentage:40.6863%,\n",
            "Epoch [799/1200], Training Loss: 1.1118, Validation Loss: 2.9430, Validation Percentage:43.2353%,\n",
            "Epoch [800/1200], Training Loss: 0.8672, Validation Loss: 3.0651, Validation Percentage:43.6275%,\n",
            "Epoch [801/1200], Training Loss: 1.0965, Validation Loss: 2.9790, Validation Percentage:42.4510%,\n",
            "Epoch [802/1200], Training Loss: 1.0061, Validation Loss: 2.9469, Validation Percentage:42.0588%,\n",
            "Epoch [803/1200], Training Loss: 1.0206, Validation Loss: 3.1179, Validation Percentage:43.7255%,\n",
            "Epoch [804/1200], Training Loss: 0.9402, Validation Loss: 2.9695, Validation Percentage:43.8235%,\n",
            "Epoch [805/1200], Training Loss: 1.0526, Validation Loss: 3.2224, Validation Percentage:39.8039%,\n",
            "Epoch [806/1200], Training Loss: 1.0047, Validation Loss: 3.0608, Validation Percentage:43.7255%,\n",
            "Epoch [807/1200], Training Loss: 0.9785, Validation Loss: 3.0627, Validation Percentage:41.8627%,\n",
            "Epoch [808/1200], Training Loss: 0.9171, Validation Loss: 3.0101, Validation Percentage:41.6667%,\n",
            "Epoch [809/1200], Training Loss: 0.9733, Validation Loss: 3.0908, Validation Percentage:43.4314%,\n",
            "Epoch [810/1200], Training Loss: 1.0031, Validation Loss: 3.0153, Validation Percentage:42.9412%,\n",
            "Epoch [811/1200], Training Loss: 0.9131, Validation Loss: 2.9288, Validation Percentage:45.2941%,\n",
            "Epoch [812/1200], Training Loss: 0.9814, Validation Loss: 2.9329, Validation Percentage:41.0784%,\n",
            "Epoch [813/1200], Training Loss: 1.0168, Validation Loss: 2.8059, Validation Percentage:42.6471%,\n",
            "Epoch [814/1200], Training Loss: 1.0235, Validation Loss: 3.1679, Validation Percentage:40.7843%,\n",
            "Epoch [815/1200], Training Loss: 1.0279, Validation Loss: 3.2437, Validation Percentage:41.6667%,\n",
            "Epoch [816/1200], Training Loss: 1.0113, Validation Loss: 2.8952, Validation Percentage:44.1176%,\n",
            "Epoch [817/1200], Training Loss: 0.9719, Validation Loss: 3.1981, Validation Percentage:40.1961%,\n",
            "Epoch [818/1200], Training Loss: 0.9793, Validation Loss: 2.9807, Validation Percentage:42.9412%,\n",
            "Epoch [819/1200], Training Loss: 0.9922, Validation Loss: 3.0131, Validation Percentage:43.4314%,\n",
            "Epoch [820/1200], Training Loss: 1.0136, Validation Loss: 2.8642, Validation Percentage:39.8039%,\n",
            "Epoch [821/1200], Training Loss: 1.0573, Validation Loss: 3.0247, Validation Percentage:40.2941%,\n",
            "Epoch [822/1200], Training Loss: 0.9579, Validation Loss: 3.2090, Validation Percentage:43.0392%,\n",
            "Epoch [823/1200], Training Loss: 0.8694, Validation Loss: 3.1222, Validation Percentage:41.8627%,\n",
            "Epoch [824/1200], Training Loss: 0.9137, Validation Loss: 3.2030, Validation Percentage:42.1569%,\n",
            "Epoch [825/1200], Training Loss: 0.8939, Validation Loss: 3.0538, Validation Percentage:40.7843%,\n",
            "Epoch [826/1200], Training Loss: 0.9634, Validation Loss: 3.0420, Validation Percentage:42.8431%,\n",
            "Epoch [827/1200], Training Loss: 0.9053, Validation Loss: 3.1568, Validation Percentage:40.4902%,\n",
            "Epoch [828/1200], Training Loss: 1.0346, Validation Loss: 2.8549, Validation Percentage:44.2157%,\n",
            "Epoch [829/1200], Training Loss: 0.9229, Validation Loss: 3.2773, Validation Percentage:43.4314%,\n",
            "Epoch [830/1200], Training Loss: 1.0183, Validation Loss: 2.9881, Validation Percentage:40.9804%,\n",
            "Epoch [831/1200], Training Loss: 0.8718, Validation Loss: 3.0919, Validation Percentage:40.7843%,\n",
            "Epoch [832/1200], Training Loss: 0.8575, Validation Loss: 2.9579, Validation Percentage:42.0588%,\n",
            "Epoch [833/1200], Training Loss: 0.9085, Validation Loss: 2.9322, Validation Percentage:42.6471%,\n",
            "Epoch [834/1200], Training Loss: 0.9223, Validation Loss: 3.1015, Validation Percentage:43.3333%,\n",
            "Epoch [835/1200], Training Loss: 1.0702, Validation Loss: 2.8439, Validation Percentage:42.6471%,\n",
            "Epoch [836/1200], Training Loss: 0.9883, Validation Loss: 3.0764, Validation Percentage:39.6078%,\n",
            "Epoch [837/1200], Training Loss: 0.9576, Validation Loss: 3.0625, Validation Percentage:43.1373%,\n",
            "Epoch [838/1200], Training Loss: 0.8754, Validation Loss: 2.9485, Validation Percentage:41.6667%,\n",
            "Epoch [839/1200], Training Loss: 0.9847, Validation Loss: 3.1484, Validation Percentage:43.2353%,\n",
            "Epoch [840/1200], Training Loss: 0.9397, Validation Loss: 3.1624, Validation Percentage:44.0196%,\n",
            "Epoch [841/1200], Training Loss: 0.9188, Validation Loss: 3.2673, Validation Percentage:42.2549%,\n",
            "Epoch [842/1200], Training Loss: 0.9482, Validation Loss: 3.1524, Validation Percentage:41.2745%,\n",
            "Epoch [843/1200], Training Loss: 0.9293, Validation Loss: 3.3175, Validation Percentage:41.8627%,\n",
            "Epoch [844/1200], Training Loss: 0.9584, Validation Loss: 3.2065, Validation Percentage:40.3922%,\n",
            "Epoch [845/1200], Training Loss: 1.0461, Validation Loss: 3.0664, Validation Percentage:41.3725%,\n",
            "Epoch [846/1200], Training Loss: 1.0196, Validation Loss: 2.8682, Validation Percentage:41.5686%,\n",
            "Epoch [847/1200], Training Loss: 0.8343, Validation Loss: 3.0697, Validation Percentage:43.6275%,\n",
            "Epoch [848/1200], Training Loss: 1.0265, Validation Loss: 3.0558, Validation Percentage:42.6471%,\n",
            "Epoch [849/1200], Training Loss: 1.0059, Validation Loss: 3.1621, Validation Percentage:43.0392%,\n",
            "Epoch [850/1200], Training Loss: 0.9865, Validation Loss: 3.1621, Validation Percentage:45.2941%,\n",
            "Epoch [851/1200], Training Loss: 1.0095, Validation Loss: 3.0057, Validation Percentage:43.9216%,\n",
            "Epoch [852/1200], Training Loss: 0.9769, Validation Loss: 3.1535, Validation Percentage:39.9020%,\n",
            "Epoch [853/1200], Training Loss: 0.9075, Validation Loss: 2.8659, Validation Percentage:42.2549%,\n",
            "Epoch [854/1200], Training Loss: 0.9576, Validation Loss: 3.0651, Validation Percentage:41.4706%,\n",
            "Epoch [855/1200], Training Loss: 0.9290, Validation Loss: 2.9717, Validation Percentage:42.0588%,\n",
            "Epoch [856/1200], Training Loss: 1.0080, Validation Loss: 3.4112, Validation Percentage:43.0392%,\n",
            "Epoch [857/1200], Training Loss: 0.9169, Validation Loss: 3.1042, Validation Percentage:41.9608%,\n",
            "Epoch [858/1200], Training Loss: 0.9658, Validation Loss: 3.0003, Validation Percentage:40.4902%,\n",
            "Epoch [859/1200], Training Loss: 1.0293, Validation Loss: 2.9787, Validation Percentage:41.8627%,\n",
            "Epoch [860/1200], Training Loss: 0.9963, Validation Loss: 3.1391, Validation Percentage:44.2157%,\n",
            "Epoch [861/1200], Training Loss: 0.8891, Validation Loss: 3.0888, Validation Percentage:42.6471%,\n",
            "Epoch [862/1200], Training Loss: 1.0025, Validation Loss: 3.0981, Validation Percentage:41.4706%,\n",
            "Epoch [863/1200], Training Loss: 0.9249, Validation Loss: 3.1081, Validation Percentage:43.2353%,\n",
            "Epoch [864/1200], Training Loss: 0.8093, Validation Loss: 3.0362, Validation Percentage:42.8431%,\n",
            "Epoch [865/1200], Training Loss: 0.8460, Validation Loss: 3.1630, Validation Percentage:43.4314%,\n",
            "Epoch [866/1200], Training Loss: 0.9267, Validation Loss: 3.3104, Validation Percentage:42.5490%,\n",
            "Epoch [867/1200], Training Loss: 0.9360, Validation Loss: 3.0850, Validation Percentage:42.8431%,\n",
            "Epoch [868/1200], Training Loss: 0.9187, Validation Loss: 2.9920, Validation Percentage:41.6667%,\n",
            "Epoch [869/1200], Training Loss: 0.9084, Validation Loss: 3.0625, Validation Percentage:41.2745%,\n",
            "Epoch [870/1200], Training Loss: 0.9200, Validation Loss: 3.1929, Validation Percentage:43.2353%,\n",
            "Epoch [871/1200], Training Loss: 0.9715, Validation Loss: 2.9621, Validation Percentage:42.1569%,\n",
            "Epoch [872/1200], Training Loss: 0.9293, Validation Loss: 3.0377, Validation Percentage:40.8824%,\n",
            "Epoch [873/1200], Training Loss: 0.9296, Validation Loss: 3.0831, Validation Percentage:43.2353%,\n",
            "Epoch [874/1200], Training Loss: 0.9746, Validation Loss: 3.1358, Validation Percentage:43.4314%,\n",
            "Epoch [875/1200], Training Loss: 0.8557, Validation Loss: 3.1853, Validation Percentage:41.9608%,\n",
            "Epoch [876/1200], Training Loss: 0.8860, Validation Loss: 3.2187, Validation Percentage:39.8039%,\n",
            "Epoch [877/1200], Training Loss: 0.9653, Validation Loss: 3.1061, Validation Percentage:42.3529%,\n",
            "Epoch [878/1200], Training Loss: 0.9275, Validation Loss: 3.7390, Validation Percentage:37.9412%,\n",
            "Epoch [879/1200], Training Loss: 0.9690, Validation Loss: 2.9273, Validation Percentage:42.9412%,\n",
            "Epoch [880/1200], Training Loss: 0.9198, Validation Loss: 3.1013, Validation Percentage:43.9216%,\n",
            "Epoch [881/1200], Training Loss: 0.9657, Validation Loss: 2.8427, Validation Percentage:45.7843%,\n",
            "Epoch [882/1200], Training Loss: 1.0425, Validation Loss: 2.9193, Validation Percentage:43.0392%,\n",
            "Epoch [883/1200], Training Loss: 0.9177, Validation Loss: 2.8785, Validation Percentage:42.7451%,\n",
            "Epoch [884/1200], Training Loss: 0.9715, Validation Loss: 3.1918, Validation Percentage:41.4706%,\n",
            "Epoch [885/1200], Training Loss: 0.9607, Validation Loss: 2.9159, Validation Percentage:42.5490%,\n",
            "Epoch [886/1200], Training Loss: 0.8729, Validation Loss: 2.9943, Validation Percentage:43.9216%,\n",
            "Epoch [887/1200], Training Loss: 0.8145, Validation Loss: 2.9584, Validation Percentage:42.1569%,\n",
            "Epoch [888/1200], Training Loss: 0.8978, Validation Loss: 3.0510, Validation Percentage:41.2745%,\n",
            "Epoch [889/1200], Training Loss: 1.0092, Validation Loss: 2.9898, Validation Percentage:43.7255%,\n",
            "Epoch [890/1200], Training Loss: 0.8856, Validation Loss: 3.3464, Validation Percentage:40.3922%,\n",
            "Epoch [891/1200], Training Loss: 0.8411, Validation Loss: 2.9098, Validation Percentage:44.6078%,\n",
            "Epoch [892/1200], Training Loss: 0.9338, Validation Loss: 3.2690, Validation Percentage:42.3529%,\n",
            "Epoch [893/1200], Training Loss: 0.8834, Validation Loss: 3.0545, Validation Percentage:43.8235%,\n",
            "Epoch [894/1200], Training Loss: 0.9037, Validation Loss: 3.1801, Validation Percentage:43.9216%,\n",
            "Epoch [895/1200], Training Loss: 0.8795, Validation Loss: 3.0337, Validation Percentage:44.2157%,\n",
            "Epoch [896/1200], Training Loss: 0.8941, Validation Loss: 3.0541, Validation Percentage:43.2353%,\n",
            "Epoch [897/1200], Training Loss: 0.9216, Validation Loss: 3.1166, Validation Percentage:41.7647%,\n",
            "Epoch [898/1200], Training Loss: 0.8610, Validation Loss: 3.1776, Validation Percentage:42.9412%,\n",
            "Epoch [899/1200], Training Loss: 0.9641, Validation Loss: 3.0750, Validation Percentage:43.1373%,\n",
            "Epoch [900/1200], Training Loss: 0.9116, Validation Loss: 3.1158, Validation Percentage:42.7451%,\n",
            "Epoch [901/1200], Training Loss: 0.9231, Validation Loss: 3.1241, Validation Percentage:42.8431%,\n",
            "Epoch [902/1200], Training Loss: 0.9198, Validation Loss: 3.2560, Validation Percentage:41.9608%,\n",
            "Epoch [903/1200], Training Loss: 0.9324, Validation Loss: 3.2809, Validation Percentage:41.0784%,\n",
            "Epoch [904/1200], Training Loss: 0.9316, Validation Loss: 3.0301, Validation Percentage:43.8235%,\n",
            "Epoch [905/1200], Training Loss: 0.9749, Validation Loss: 3.1139, Validation Percentage:40.6863%,\n",
            "Epoch [906/1200], Training Loss: 0.9064, Validation Loss: 2.8358, Validation Percentage:44.0196%,\n",
            "Epoch [907/1200], Training Loss: 0.9296, Validation Loss: 3.1434, Validation Percentage:42.5490%,\n",
            "Epoch [908/1200], Training Loss: 0.9848, Validation Loss: 3.0857, Validation Percentage:42.4510%,\n",
            "Epoch [909/1200], Training Loss: 0.9456, Validation Loss: 3.2231, Validation Percentage:42.3529%,\n",
            "Epoch [910/1200], Training Loss: 0.9995, Validation Loss: 3.1942, Validation Percentage:41.6667%,\n",
            "Epoch [911/1200], Training Loss: 0.9195, Validation Loss: 3.1832, Validation Percentage:44.7059%,\n",
            "Epoch [912/1200], Training Loss: 0.9260, Validation Loss: 3.0138, Validation Percentage:42.4510%,\n",
            "Epoch [913/1200], Training Loss: 0.8808, Validation Loss: 3.1434, Validation Percentage:42.8431%,\n",
            "Epoch [914/1200], Training Loss: 0.8599, Validation Loss: 3.1175, Validation Percentage:42.1569%,\n",
            "Epoch [915/1200], Training Loss: 0.8664, Validation Loss: 3.4072, Validation Percentage:39.6078%,\n",
            "Epoch [916/1200], Training Loss: 0.8573, Validation Loss: 3.3104, Validation Percentage:42.6471%,\n",
            "Epoch [917/1200], Training Loss: 0.9420, Validation Loss: 3.1508, Validation Percentage:40.0980%,\n",
            "Epoch [918/1200], Training Loss: 0.9718, Validation Loss: 3.1105, Validation Percentage:42.9412%,\n",
            "Epoch [919/1200], Training Loss: 0.9682, Validation Loss: 3.0833, Validation Percentage:43.2353%,\n",
            "Epoch [920/1200], Training Loss: 0.8627, Validation Loss: 3.2200, Validation Percentage:41.9608%,\n",
            "Epoch [921/1200], Training Loss: 0.9395, Validation Loss: 3.2111, Validation Percentage:42.3529%,\n",
            "Epoch [922/1200], Training Loss: 0.8535, Validation Loss: 3.1381, Validation Percentage:42.5490%,\n",
            "Epoch [923/1200], Training Loss: 0.9634, Validation Loss: 3.3587, Validation Percentage:40.6863%,\n",
            "Epoch [924/1200], Training Loss: 1.0071, Validation Loss: 3.0017, Validation Percentage:44.8039%,\n",
            "Epoch [925/1200], Training Loss: 0.9156, Validation Loss: 2.9903, Validation Percentage:42.8431%,\n",
            "Epoch [926/1200], Training Loss: 0.8784, Validation Loss: 3.2046, Validation Percentage:43.8235%,\n",
            "Epoch [927/1200], Training Loss: 0.9268, Validation Loss: 2.8745, Validation Percentage:42.1569%,\n",
            "Epoch [928/1200], Training Loss: 0.8149, Validation Loss: 3.3233, Validation Percentage:42.7451%,\n",
            "Epoch [929/1200], Training Loss: 0.9577, Validation Loss: 3.1810, Validation Percentage:41.7647%,\n",
            "Epoch [930/1200], Training Loss: 0.9254, Validation Loss: 3.1925, Validation Percentage:42.9412%,\n",
            "Epoch [931/1200], Training Loss: 0.9151, Validation Loss: 3.1077, Validation Percentage:42.7451%,\n",
            "Epoch [932/1200], Training Loss: 0.9172, Validation Loss: 2.9388, Validation Percentage:40.1961%,\n",
            "Epoch [933/1200], Training Loss: 0.8717, Validation Loss: 3.1080, Validation Percentage:42.5490%,\n",
            "Epoch [934/1200], Training Loss: 0.9810, Validation Loss: 3.2673, Validation Percentage:43.3333%,\n",
            "Epoch [935/1200], Training Loss: 0.9555, Validation Loss: 3.0037, Validation Percentage:43.3333%,\n",
            "Epoch [936/1200], Training Loss: 0.8740, Validation Loss: 3.1408, Validation Percentage:44.1176%,\n",
            "Epoch [937/1200], Training Loss: 0.9266, Validation Loss: 2.9455, Validation Percentage:44.4118%,\n",
            "Epoch [938/1200], Training Loss: 1.0470, Validation Loss: 3.0441, Validation Percentage:42.8431%,\n",
            "Epoch [939/1200], Training Loss: 0.8839, Validation Loss: 2.9130, Validation Percentage:45.8824%,\n",
            "Epoch [940/1200], Training Loss: 0.9095, Validation Loss: 3.2494, Validation Percentage:41.3725%,\n",
            "Epoch [941/1200], Training Loss: 0.9615, Validation Loss: 2.9768, Validation Percentage:43.9216%,\n",
            "Epoch [942/1200], Training Loss: 0.8925, Validation Loss: 2.9661, Validation Percentage:44.3137%,\n",
            "Epoch [943/1200], Training Loss: 1.0317, Validation Loss: 3.0221, Validation Percentage:41.1765%,\n",
            "Epoch [944/1200], Training Loss: 0.9563, Validation Loss: 3.0760, Validation Percentage:44.6078%,\n",
            "Epoch [945/1200], Training Loss: 0.8780, Validation Loss: 3.4038, Validation Percentage:44.5098%,\n",
            "Epoch [946/1200], Training Loss: 0.9448, Validation Loss: 2.8050, Validation Percentage:42.5490%,\n",
            "Epoch [947/1200], Training Loss: 0.9390, Validation Loss: 2.7747, Validation Percentage:45.4902%,\n",
            "Epoch [948/1200], Training Loss: 0.9003, Validation Loss: 3.0968, Validation Percentage:43.8235%,\n",
            "Epoch [949/1200], Training Loss: 0.9686, Validation Loss: 2.9830, Validation Percentage:44.3137%,\n",
            "Epoch [950/1200], Training Loss: 0.9102, Validation Loss: 2.9427, Validation Percentage:41.9608%,\n",
            "Epoch [951/1200], Training Loss: 0.9048, Validation Loss: 2.8800, Validation Percentage:43.4314%,\n",
            "Epoch [952/1200], Training Loss: 0.8669, Validation Loss: 3.1657, Validation Percentage:42.2549%,\n",
            "Epoch [953/1200], Training Loss: 0.7925, Validation Loss: 3.4764, Validation Percentage:42.7451%,\n",
            "Epoch [954/1200], Training Loss: 1.0315, Validation Loss: 2.9570, Validation Percentage:41.5686%,\n",
            "Epoch [955/1200], Training Loss: 0.9880, Validation Loss: 3.0446, Validation Percentage:40.9804%,\n",
            "Epoch [956/1200], Training Loss: 0.9479, Validation Loss: 2.9240, Validation Percentage:42.5490%,\n",
            "Epoch [957/1200], Training Loss: 0.9116, Validation Loss: 2.8747, Validation Percentage:44.2157%,\n",
            "Epoch [958/1200], Training Loss: 0.9694, Validation Loss: 2.8507, Validation Percentage:44.1176%,\n",
            "Epoch [959/1200], Training Loss: 0.9230, Validation Loss: 2.9563, Validation Percentage:43.6275%,\n",
            "Epoch [960/1200], Training Loss: 0.7842, Validation Loss: 3.3420, Validation Percentage:43.7255%,\n",
            "Epoch [961/1200], Training Loss: 0.9059, Validation Loss: 3.2315, Validation Percentage:41.4706%,\n",
            "Epoch [962/1200], Training Loss: 0.8843, Validation Loss: 3.3445, Validation Percentage:43.8235%,\n",
            "Epoch [963/1200], Training Loss: 0.9852, Validation Loss: 3.0013, Validation Percentage:44.1176%,\n",
            "Epoch [964/1200], Training Loss: 0.8441, Validation Loss: 2.8076, Validation Percentage:44.3137%,\n",
            "Epoch [965/1200], Training Loss: 0.9011, Validation Loss: 3.2292, Validation Percentage:43.1373%,\n",
            "Epoch [966/1200], Training Loss: 0.9180, Validation Loss: 3.1259, Validation Percentage:44.1176%,\n",
            "Epoch [967/1200], Training Loss: 0.8580, Validation Loss: 3.2513, Validation Percentage:41.7647%,\n",
            "Epoch [968/1200], Training Loss: 0.9248, Validation Loss: 2.8695, Validation Percentage:44.1176%,\n",
            "Epoch [969/1200], Training Loss: 0.9611, Validation Loss: 3.0444, Validation Percentage:42.6471%,\n",
            "Epoch [970/1200], Training Loss: 0.9890, Validation Loss: 2.8956, Validation Percentage:42.8431%,\n",
            "Epoch [971/1200], Training Loss: 0.8403, Validation Loss: 3.2059, Validation Percentage:43.2353%,\n",
            "Epoch [972/1200], Training Loss: 0.8913, Validation Loss: 3.2707, Validation Percentage:42.2549%,\n",
            "Epoch [973/1200], Training Loss: 1.0068, Validation Loss: 3.0762, Validation Percentage:45.1961%,\n",
            "Epoch [974/1200], Training Loss: 0.9570, Validation Loss: 3.0840, Validation Percentage:42.9412%,\n",
            "Epoch [975/1200], Training Loss: 0.9157, Validation Loss: 3.2131, Validation Percentage:41.5686%,\n",
            "Epoch [976/1200], Training Loss: 1.0017, Validation Loss: 2.8441, Validation Percentage:43.5294%,\n",
            "Epoch [977/1200], Training Loss: 0.8924, Validation Loss: 3.1465, Validation Percentage:43.2353%,\n",
            "Epoch [978/1200], Training Loss: 0.8710, Validation Loss: 3.1153, Validation Percentage:41.9608%,\n",
            "Epoch [979/1200], Training Loss: 0.8930, Validation Loss: 3.1377, Validation Percentage:41.1765%,\n",
            "Epoch [980/1200], Training Loss: 0.8974, Validation Loss: 3.2362, Validation Percentage:40.8824%,\n",
            "Epoch [981/1200], Training Loss: 1.0236, Validation Loss: 2.9976, Validation Percentage:42.6471%,\n",
            "Epoch [982/1200], Training Loss: 0.9321, Validation Loss: 2.8156, Validation Percentage:42.7451%,\n",
            "Epoch [983/1200], Training Loss: 0.9020, Validation Loss: 3.0877, Validation Percentage:41.5686%,\n",
            "Epoch [984/1200], Training Loss: 0.9335, Validation Loss: 2.9687, Validation Percentage:42.2549%,\n",
            "Epoch [985/1200], Training Loss: 0.9616, Validation Loss: 3.0847, Validation Percentage:41.7647%,\n",
            "Epoch [986/1200], Training Loss: 0.8373, Validation Loss: 3.1604, Validation Percentage:43.7255%,\n",
            "Epoch [987/1200], Training Loss: 0.9805, Validation Loss: 3.0580, Validation Percentage:42.7451%,\n",
            "Epoch [988/1200], Training Loss: 0.9386, Validation Loss: 3.2439, Validation Percentage:41.5686%,\n",
            "Epoch [989/1200], Training Loss: 0.8636, Validation Loss: 3.0773, Validation Percentage:43.3333%,\n",
            "Epoch [990/1200], Training Loss: 0.9576, Validation Loss: 2.9031, Validation Percentage:44.4118%,\n",
            "Epoch [991/1200], Training Loss: 0.8918, Validation Loss: 3.0594, Validation Percentage:43.5294%,\n",
            "Epoch [992/1200], Training Loss: 0.7831, Validation Loss: 3.2226, Validation Percentage:43.8235%,\n",
            "Epoch [993/1200], Training Loss: 0.9387, Validation Loss: 2.7801, Validation Percentage:45.0980%,\n",
            "Epoch [994/1200], Training Loss: 0.8608, Validation Loss: 2.8910, Validation Percentage:44.1176%,\n",
            "Epoch [995/1200], Training Loss: 0.9011, Validation Loss: 3.3143, Validation Percentage:42.0588%,\n",
            "Epoch [996/1200], Training Loss: 0.8846, Validation Loss: 3.1510, Validation Percentage:40.1961%,\n",
            "Epoch [997/1200], Training Loss: 1.0385, Validation Loss: 3.0907, Validation Percentage:41.4706%,\n",
            "Epoch [998/1200], Training Loss: 0.9490, Validation Loss: 3.2707, Validation Percentage:43.1373%,\n",
            "Epoch [999/1200], Training Loss: 1.0000, Validation Loss: 3.0943, Validation Percentage:44.4118%,\n",
            "Epoch [1000/1200], Training Loss: 0.8880, Validation Loss: 3.0693, Validation Percentage:41.8627%,\n",
            "Epoch [1001/1200], Training Loss: 0.8819, Validation Loss: 3.0870, Validation Percentage:42.4510%,\n",
            "Epoch [1002/1200], Training Loss: 0.8866, Validation Loss: 2.8135, Validation Percentage:44.0196%,\n",
            "Epoch [1003/1200], Training Loss: 0.9158, Validation Loss: 2.8924, Validation Percentage:43.4314%,\n",
            "Epoch [1004/1200], Training Loss: 0.9743, Validation Loss: 2.9632, Validation Percentage:44.2157%,\n",
            "Epoch [1005/1200], Training Loss: 0.7895, Validation Loss: 3.2565, Validation Percentage:45.0000%,\n",
            "Epoch [1006/1200], Training Loss: 0.8471, Validation Loss: 3.4293, Validation Percentage:42.4510%,\n",
            "Epoch [1007/1200], Training Loss: 0.9425, Validation Loss: 3.3832, Validation Percentage:42.1569%,\n",
            "Epoch [1008/1200], Training Loss: 1.0153, Validation Loss: 3.1318, Validation Percentage:43.0392%,\n",
            "Epoch [1009/1200], Training Loss: 0.9514, Validation Loss: 3.1455, Validation Percentage:42.3529%,\n",
            "Epoch [1010/1200], Training Loss: 0.9930, Validation Loss: 3.7115, Validation Percentage:41.5686%,\n",
            "Epoch [1011/1200], Training Loss: 0.9389, Validation Loss: 2.9529, Validation Percentage:43.3333%,\n",
            "Epoch [1012/1200], Training Loss: 0.9624, Validation Loss: 3.1324, Validation Percentage:40.3922%,\n",
            "Epoch [1013/1200], Training Loss: 0.9188, Validation Loss: 2.9644, Validation Percentage:44.0196%,\n",
            "Epoch [1014/1200], Training Loss: 0.8158, Validation Loss: 3.5006, Validation Percentage:41.5686%,\n",
            "Epoch [1015/1200], Training Loss: 1.0058, Validation Loss: 3.0500, Validation Percentage:44.1176%,\n",
            "Epoch [1016/1200], Training Loss: 0.8994, Validation Loss: 3.2755, Validation Percentage:43.3333%,\n",
            "Epoch [1017/1200], Training Loss: 0.9442, Validation Loss: 3.0269, Validation Percentage:43.8235%,\n",
            "Epoch [1018/1200], Training Loss: 0.9192, Validation Loss: 2.9794, Validation Percentage:44.1176%,\n",
            "Epoch [1019/1200], Training Loss: 0.9543, Validation Loss: 3.0877, Validation Percentage:45.0000%,\n",
            "Epoch [1020/1200], Training Loss: 0.8973, Validation Loss: 2.8928, Validation Percentage:45.0000%,\n",
            "Epoch [1021/1200], Training Loss: 0.8850, Validation Loss: 3.1532, Validation Percentage:41.8627%,\n",
            "Epoch [1022/1200], Training Loss: 0.8258, Validation Loss: 3.1244, Validation Percentage:42.6471%,\n",
            "Epoch [1023/1200], Training Loss: 0.9743, Validation Loss: 3.1972, Validation Percentage:42.2549%,\n",
            "Epoch [1024/1200], Training Loss: 0.9618, Validation Loss: 3.0699, Validation Percentage:43.9216%,\n",
            "Epoch [1025/1200], Training Loss: 0.9017, Validation Loss: 3.3338, Validation Percentage:42.7451%,\n",
            "Epoch [1026/1200], Training Loss: 0.8835, Validation Loss: 3.3171, Validation Percentage:43.1373%,\n",
            "Epoch [1027/1200], Training Loss: 0.8792, Validation Loss: 3.0549, Validation Percentage:42.7451%,\n",
            "Epoch [1028/1200], Training Loss: 0.8441, Validation Loss: 3.2130, Validation Percentage:43.7255%,\n",
            "Epoch [1029/1200], Training Loss: 0.7696, Validation Loss: 3.4936, Validation Percentage:44.7059%,\n",
            "Epoch [1030/1200], Training Loss: 0.9007, Validation Loss: 3.6759, Validation Percentage:42.4510%,\n",
            "Epoch [1031/1200], Training Loss: 0.8698, Validation Loss: 3.0067, Validation Percentage:43.8235%,\n",
            "Epoch [1032/1200], Training Loss: 0.8844, Validation Loss: 2.8320, Validation Percentage:44.6078%,\n",
            "Epoch [1033/1200], Training Loss: 0.8537, Validation Loss: 3.0121, Validation Percentage:43.6275%,\n",
            "Epoch [1034/1200], Training Loss: 0.7645, Validation Loss: 3.1403, Validation Percentage:44.6078%,\n",
            "Epoch [1035/1200], Training Loss: 0.8500, Validation Loss: 3.0007, Validation Percentage:44.4118%,\n",
            "Epoch [1036/1200], Training Loss: 0.9015, Validation Loss: 2.9939, Validation Percentage:44.2157%,\n",
            "Epoch [1037/1200], Training Loss: 0.8705, Validation Loss: 3.2007, Validation Percentage:45.0980%,\n",
            "Epoch [1038/1200], Training Loss: 0.8932, Validation Loss: 3.2757, Validation Percentage:43.2353%,\n",
            "Epoch [1039/1200], Training Loss: 0.9140, Validation Loss: 3.4776, Validation Percentage:41.3725%,\n",
            "Epoch [1040/1200], Training Loss: 0.8796, Validation Loss: 3.0856, Validation Percentage:43.3333%,\n",
            "Epoch [1041/1200], Training Loss: 0.8370, Validation Loss: 3.2292, Validation Percentage:41.6667%,\n",
            "Epoch [1042/1200], Training Loss: 0.9261, Validation Loss: 2.9713, Validation Percentage:42.8431%,\n",
            "Epoch [1043/1200], Training Loss: 0.8125, Validation Loss: 3.0918, Validation Percentage:42.5490%,\n",
            "Epoch [1044/1200], Training Loss: 0.8274, Validation Loss: 3.0268, Validation Percentage:43.4314%,\n",
            "Epoch [1045/1200], Training Loss: 0.9501, Validation Loss: 3.0061, Validation Percentage:42.5490%,\n",
            "Epoch [1046/1200], Training Loss: 0.8291, Validation Loss: 2.9771, Validation Percentage:43.4314%,\n",
            "Epoch [1047/1200], Training Loss: 0.8502, Validation Loss: 2.9840, Validation Percentage:43.0392%,\n",
            "Epoch [1048/1200], Training Loss: 0.8312, Validation Loss: 3.2048, Validation Percentage:43.6275%,\n",
            "Epoch [1049/1200], Training Loss: 1.0305, Validation Loss: 2.7875, Validation Percentage:44.1176%,\n",
            "Epoch [1050/1200], Training Loss: 0.8878, Validation Loss: 2.9540, Validation Percentage:43.4314%,\n",
            "Epoch [1051/1200], Training Loss: 0.8662, Validation Loss: 2.9368, Validation Percentage:42.0588%,\n",
            "Epoch [1052/1200], Training Loss: 0.8300, Validation Loss: 3.2441, Validation Percentage:44.8039%,\n",
            "Epoch [1053/1200], Training Loss: 0.9528, Validation Loss: 3.1657, Validation Percentage:44.1176%,\n",
            "Epoch [1054/1200], Training Loss: 0.8822, Validation Loss: 3.1917, Validation Percentage:41.8627%,\n",
            "Epoch [1055/1200], Training Loss: 0.8629, Validation Loss: 2.8774, Validation Percentage:44.5098%,\n",
            "Epoch [1056/1200], Training Loss: 0.7999, Validation Loss: 3.0371, Validation Percentage:46.8627%,\n",
            "Epoch [1057/1200], Training Loss: 0.8684, Validation Loss: 3.2942, Validation Percentage:43.5294%,\n",
            "Epoch [1058/1200], Training Loss: 0.8908, Validation Loss: 3.0253, Validation Percentage:46.0784%,\n",
            "Epoch [1059/1200], Training Loss: 0.9370, Validation Loss: 3.0140, Validation Percentage:43.6275%,\n",
            "Epoch [1060/1200], Training Loss: 0.8593, Validation Loss: 3.1449, Validation Percentage:42.2549%,\n",
            "Epoch [1061/1200], Training Loss: 0.8808, Validation Loss: 3.1985, Validation Percentage:43.0392%,\n",
            "Epoch [1062/1200], Training Loss: 0.9648, Validation Loss: 3.3016, Validation Percentage:42.0588%,\n",
            "Epoch [1063/1200], Training Loss: 0.9037, Validation Loss: 3.4583, Validation Percentage:43.9216%,\n",
            "Epoch [1064/1200], Training Loss: 0.9411, Validation Loss: 3.0985, Validation Percentage:45.1961%,\n",
            "Epoch [1065/1200], Training Loss: 0.8719, Validation Loss: 3.2825, Validation Percentage:44.7059%,\n",
            "Epoch [1066/1200], Training Loss: 0.8318, Validation Loss: 3.2384, Validation Percentage:43.8235%,\n",
            "Epoch [1067/1200], Training Loss: 0.8991, Validation Loss: 2.8626, Validation Percentage:41.7647%,\n",
            "Epoch [1068/1200], Training Loss: 0.9219, Validation Loss: 2.9888, Validation Percentage:45.0980%,\n",
            "Epoch [1069/1200], Training Loss: 0.9435, Validation Loss: 3.2782, Validation Percentage:42.7451%,\n",
            "Epoch [1070/1200], Training Loss: 0.8040, Validation Loss: 3.1205, Validation Percentage:43.5294%,\n",
            "Epoch [1071/1200], Training Loss: 0.9293, Validation Loss: 3.1700, Validation Percentage:41.7647%,\n",
            "Epoch [1072/1200], Training Loss: 0.8896, Validation Loss: 2.8118, Validation Percentage:45.4902%,\n",
            "Epoch [1073/1200], Training Loss: 0.8804, Validation Loss: 3.0803, Validation Percentage:42.8431%,\n",
            "Epoch [1074/1200], Training Loss: 0.9298, Validation Loss: 3.0179, Validation Percentage:44.3137%,\n",
            "Epoch [1075/1200], Training Loss: 0.9006, Validation Loss: 2.9994, Validation Percentage:43.9216%,\n",
            "Epoch [1076/1200], Training Loss: 0.7804, Validation Loss: 3.1965, Validation Percentage:44.9020%,\n",
            "Epoch [1077/1200], Training Loss: 0.9911, Validation Loss: 3.3216, Validation Percentage:44.4118%,\n",
            "Epoch [1078/1200], Training Loss: 0.8154, Validation Loss: 3.2646, Validation Percentage:43.4314%,\n",
            "Epoch [1079/1200], Training Loss: 0.8487, Validation Loss: 2.9656, Validation Percentage:45.5882%,\n",
            "Epoch [1080/1200], Training Loss: 0.8652, Validation Loss: 3.0547, Validation Percentage:43.7255%,\n",
            "Epoch [1081/1200], Training Loss: 0.9512, Validation Loss: 3.2039, Validation Percentage:44.4118%,\n",
            "Epoch [1082/1200], Training Loss: 1.0139, Validation Loss: 3.2987, Validation Percentage:44.7059%,\n",
            "Epoch [1083/1200], Training Loss: 0.8679, Validation Loss: 3.1575, Validation Percentage:43.5294%,\n",
            "Epoch [1084/1200], Training Loss: 0.8383, Validation Loss: 3.0763, Validation Percentage:45.2941%,\n",
            "Epoch [1085/1200], Training Loss: 0.7802, Validation Loss: 3.3800, Validation Percentage:42.9412%,\n",
            "Epoch [1086/1200], Training Loss: 0.9244, Validation Loss: 3.2338, Validation Percentage:43.2353%,\n",
            "Epoch [1087/1200], Training Loss: 0.8511, Validation Loss: 3.1382, Validation Percentage:45.1961%,\n",
            "Epoch [1088/1200], Training Loss: 0.8617, Validation Loss: 3.1752, Validation Percentage:41.0784%,\n",
            "Epoch [1089/1200], Training Loss: 0.9678, Validation Loss: 3.0411, Validation Percentage:43.9216%,\n",
            "Epoch [1090/1200], Training Loss: 0.8209, Validation Loss: 3.5880, Validation Percentage:43.2353%,\n",
            "Epoch [1091/1200], Training Loss: 0.8937, Validation Loss: 3.2018, Validation Percentage:43.1373%,\n",
            "Epoch [1092/1200], Training Loss: 0.9357, Validation Loss: 3.2451, Validation Percentage:43.3333%,\n",
            "Epoch [1093/1200], Training Loss: 0.9298, Validation Loss: 3.2839, Validation Percentage:43.5294%,\n",
            "Epoch [1094/1200], Training Loss: 0.9102, Validation Loss: 3.2257, Validation Percentage:43.1373%,\n",
            "Epoch [1095/1200], Training Loss: 0.8455, Validation Loss: 3.1916, Validation Percentage:45.4902%,\n",
            "Epoch [1096/1200], Training Loss: 0.9116, Validation Loss: 3.2752, Validation Percentage:45.0980%,\n",
            "Epoch [1097/1200], Training Loss: 0.8725, Validation Loss: 3.3944, Validation Percentage:45.3922%,\n",
            "Epoch [1098/1200], Training Loss: 0.8956, Validation Loss: 3.0882, Validation Percentage:44.6078%,\n",
            "Epoch [1099/1200], Training Loss: 0.8805, Validation Loss: 3.0657, Validation Percentage:46.2745%,\n",
            "Epoch [1100/1200], Training Loss: 0.8651, Validation Loss: 3.0934, Validation Percentage:41.8627%,\n",
            "Epoch [1101/1200], Training Loss: 0.9546, Validation Loss: 2.9963, Validation Percentage:42.6471%,\n",
            "Epoch [1102/1200], Training Loss: 0.9023, Validation Loss: 3.2959, Validation Percentage:42.0588%,\n",
            "Epoch [1103/1200], Training Loss: 0.7584, Validation Loss: 3.0644, Validation Percentage:45.0000%,\n",
            "Epoch [1104/1200], Training Loss: 0.8954, Validation Loss: 3.1735, Validation Percentage:42.4510%,\n",
            "Epoch [1105/1200], Training Loss: 0.8962, Validation Loss: 3.2838, Validation Percentage:42.3529%,\n",
            "Epoch [1106/1200], Training Loss: 0.8340, Validation Loss: 3.3711, Validation Percentage:41.8627%,\n",
            "Epoch [1107/1200], Training Loss: 0.9288, Validation Loss: 3.1369, Validation Percentage:44.1176%,\n",
            "Epoch [1108/1200], Training Loss: 0.8658, Validation Loss: 3.4143, Validation Percentage:44.7059%,\n",
            "Epoch [1109/1200], Training Loss: 0.8182, Validation Loss: 3.1028, Validation Percentage:41.8627%,\n",
            "Epoch [1110/1200], Training Loss: 0.9021, Validation Loss: 3.1695, Validation Percentage:43.5294%,\n",
            "Epoch [1111/1200], Training Loss: 0.9271, Validation Loss: 3.1205, Validation Percentage:42.3529%,\n",
            "Epoch [1112/1200], Training Loss: 0.8221, Validation Loss: 3.5730, Validation Percentage:43.9216%,\n",
            "Epoch [1113/1200], Training Loss: 0.8548, Validation Loss: 3.4384, Validation Percentage:45.1961%,\n",
            "Epoch [1114/1200], Training Loss: 0.8871, Validation Loss: 3.2116, Validation Percentage:42.5490%,\n",
            "Epoch [1115/1200], Training Loss: 0.9921, Validation Loss: 3.0469, Validation Percentage:43.9216%,\n",
            "Epoch [1116/1200], Training Loss: 0.9431, Validation Loss: 3.0421, Validation Percentage:41.3725%,\n",
            "Epoch [1117/1200], Training Loss: 0.9155, Validation Loss: 3.1204, Validation Percentage:43.1373%,\n",
            "Epoch [1118/1200], Training Loss: 0.9313, Validation Loss: 3.1365, Validation Percentage:42.5490%,\n",
            "Epoch [1119/1200], Training Loss: 0.7827, Validation Loss: 3.1072, Validation Percentage:43.9216%,\n",
            "Epoch [1120/1200], Training Loss: 1.0058, Validation Loss: 2.9729, Validation Percentage:42.7451%,\n",
            "Epoch [1121/1200], Training Loss: 0.8585, Validation Loss: 3.1190, Validation Percentage:43.3333%,\n",
            "Epoch [1122/1200], Training Loss: 0.9555, Validation Loss: 3.2097, Validation Percentage:42.6471%,\n",
            "Epoch [1123/1200], Training Loss: 0.8474, Validation Loss: 2.9987, Validation Percentage:42.7451%,\n",
            "Epoch [1124/1200], Training Loss: 0.9505, Validation Loss: 3.1701, Validation Percentage:43.0392%,\n",
            "Epoch [1125/1200], Training Loss: 0.8918, Validation Loss: 3.2016, Validation Percentage:43.2353%,\n",
            "Epoch [1126/1200], Training Loss: 0.8831, Validation Loss: 3.0063, Validation Percentage:44.6078%,\n",
            "Epoch [1127/1200], Training Loss: 0.8385, Validation Loss: 3.1701, Validation Percentage:42.9412%,\n",
            "Epoch [1128/1200], Training Loss: 0.9236, Validation Loss: 2.8956, Validation Percentage:44.1176%,\n",
            "Epoch [1129/1200], Training Loss: 0.9452, Validation Loss: 2.9764, Validation Percentage:45.9804%,\n",
            "Epoch [1130/1200], Training Loss: 0.9001, Validation Loss: 3.0112, Validation Percentage:42.7451%,\n",
            "Epoch [1131/1200], Training Loss: 0.8586, Validation Loss: 3.1053, Validation Percentage:42.0588%,\n",
            "Epoch [1132/1200], Training Loss: 0.9262, Validation Loss: 2.9722, Validation Percentage:43.4314%,\n",
            "Epoch [1133/1200], Training Loss: 0.8677, Validation Loss: 3.3425, Validation Percentage:43.7255%,\n",
            "Epoch [1134/1200], Training Loss: 0.8708, Validation Loss: 3.2557, Validation Percentage:46.6667%,\n",
            "Epoch [1135/1200], Training Loss: 0.8419, Validation Loss: 3.1551, Validation Percentage:44.3137%,\n",
            "Epoch [1136/1200], Training Loss: 0.8838, Validation Loss: 3.3960, Validation Percentage:43.1373%,\n",
            "Epoch [1137/1200], Training Loss: 0.9205, Validation Loss: 3.0461, Validation Percentage:44.6078%,\n",
            "Epoch [1138/1200], Training Loss: 0.8356, Validation Loss: 3.4209, Validation Percentage:41.7647%,\n",
            "Epoch [1139/1200], Training Loss: 0.9274, Validation Loss: 3.3678, Validation Percentage:42.5490%,\n",
            "Epoch [1140/1200], Training Loss: 0.9550, Validation Loss: 3.0818, Validation Percentage:41.1765%,\n",
            "Epoch [1141/1200], Training Loss: 0.8626, Validation Loss: 3.1116, Validation Percentage:42.1569%,\n",
            "Epoch [1142/1200], Training Loss: 0.7847, Validation Loss: 3.2204, Validation Percentage:44.0196%,\n",
            "Epoch [1143/1200], Training Loss: 0.9775, Validation Loss: 2.9987, Validation Percentage:43.7255%,\n",
            "Epoch [1144/1200], Training Loss: 0.9196, Validation Loss: 2.9500, Validation Percentage:44.0196%,\n",
            "Epoch [1145/1200], Training Loss: 0.7915, Validation Loss: 3.1913, Validation Percentage:42.8431%,\n",
            "Epoch [1146/1200], Training Loss: 0.8811, Validation Loss: 3.2857, Validation Percentage:41.6667%,\n",
            "Epoch [1147/1200], Training Loss: 0.7911, Validation Loss: 3.2607, Validation Percentage:40.4902%,\n",
            "Epoch [1148/1200], Training Loss: 0.8607, Validation Loss: 3.0622, Validation Percentage:43.6275%,\n",
            "Epoch [1149/1200], Training Loss: 0.8759, Validation Loss: 3.2510, Validation Percentage:44.8039%,\n",
            "Epoch [1150/1200], Training Loss: 0.8614, Validation Loss: 3.1766, Validation Percentage:43.9216%,\n",
            "Epoch [1151/1200], Training Loss: 0.9138, Validation Loss: 3.3676, Validation Percentage:40.5882%,\n",
            "Epoch [1152/1200], Training Loss: 0.9397, Validation Loss: 3.1496, Validation Percentage:42.5490%,\n",
            "Epoch [1153/1200], Training Loss: 0.9177, Validation Loss: 3.3904, Validation Percentage:40.7843%,\n",
            "Epoch [1154/1200], Training Loss: 0.9773, Validation Loss: 3.0303, Validation Percentage:44.6078%,\n",
            "Epoch [1155/1200], Training Loss: 0.8134, Validation Loss: 3.1710, Validation Percentage:44.2157%,\n",
            "Epoch [1156/1200], Training Loss: 0.7732, Validation Loss: 3.4124, Validation Percentage:43.2353%,\n",
            "Epoch [1157/1200], Training Loss: 0.8412, Validation Loss: 3.3272, Validation Percentage:43.6275%,\n",
            "Epoch [1158/1200], Training Loss: 0.8896, Validation Loss: 3.1635, Validation Percentage:44.5098%,\n",
            "Epoch [1159/1200], Training Loss: 0.9306, Validation Loss: 3.1224, Validation Percentage:42.4510%,\n",
            "Epoch [1160/1200], Training Loss: 0.8586, Validation Loss: 3.1386, Validation Percentage:42.9412%,\n",
            "Epoch [1161/1200], Training Loss: 0.9014, Validation Loss: 3.3918, Validation Percentage:41.0784%,\n",
            "Epoch [1162/1200], Training Loss: 0.8766, Validation Loss: 3.4266, Validation Percentage:40.7843%,\n",
            "Epoch [1163/1200], Training Loss: 0.8511, Validation Loss: 3.5879, Validation Percentage:44.6078%,\n",
            "Epoch [1164/1200], Training Loss: 0.9179, Validation Loss: 3.4619, Validation Percentage:41.9608%,\n",
            "Epoch [1165/1200], Training Loss: 0.9621, Validation Loss: 2.9948, Validation Percentage:42.7451%,\n",
            "Epoch [1166/1200], Training Loss: 0.8536, Validation Loss: 3.0169, Validation Percentage:42.9412%,\n",
            "Epoch [1167/1200], Training Loss: 0.8915, Validation Loss: 3.3559, Validation Percentage:40.1961%,\n",
            "Epoch [1168/1200], Training Loss: 0.8950, Validation Loss: 3.0783, Validation Percentage:44.2157%,\n",
            "Epoch [1169/1200], Training Loss: 0.8875, Validation Loss: 3.2083, Validation Percentage:42.7451%,\n",
            "Epoch [1170/1200], Training Loss: 0.9377, Validation Loss: 3.3439, Validation Percentage:44.3137%,\n",
            "Epoch [1171/1200], Training Loss: 0.8293, Validation Loss: 3.3718, Validation Percentage:42.0588%,\n",
            "Epoch [1172/1200], Training Loss: 0.9548, Validation Loss: 3.1340, Validation Percentage:43.9216%,\n",
            "Epoch [1173/1200], Training Loss: 0.8906, Validation Loss: 3.2181, Validation Percentage:43.6275%,\n",
            "Epoch [1174/1200], Training Loss: 0.9361, Validation Loss: 3.0367, Validation Percentage:43.2353%,\n",
            "Epoch [1175/1200], Training Loss: 1.0154, Validation Loss: 3.1843, Validation Percentage:42.2549%,\n",
            "Epoch [1176/1200], Training Loss: 0.8455, Validation Loss: 3.2677, Validation Percentage:45.5882%,\n",
            "Epoch [1177/1200], Training Loss: 0.7834, Validation Loss: 3.2417, Validation Percentage:44.7059%,\n",
            "Epoch [1178/1200], Training Loss: 0.8083, Validation Loss: 3.2051, Validation Percentage:43.3333%,\n",
            "Epoch [1179/1200], Training Loss: 0.8744, Validation Loss: 3.3396, Validation Percentage:43.3333%,\n",
            "Epoch [1180/1200], Training Loss: 0.9147, Validation Loss: 3.1800, Validation Percentage:42.6471%,\n",
            "Epoch [1181/1200], Training Loss: 0.9438, Validation Loss: 3.2776, Validation Percentage:44.0196%,\n",
            "Epoch [1182/1200], Training Loss: 0.8221, Validation Loss: 3.2666, Validation Percentage:45.4902%,\n",
            "Epoch [1183/1200], Training Loss: 0.8163, Validation Loss: 3.0081, Validation Percentage:42.4510%,\n",
            "Epoch [1184/1200], Training Loss: 0.8721, Validation Loss: 3.2711, Validation Percentage:45.0980%,\n",
            "Epoch [1185/1200], Training Loss: 0.8979, Validation Loss: 3.1262, Validation Percentage:42.8431%,\n",
            "Epoch [1186/1200], Training Loss: 0.8864, Validation Loss: 3.1038, Validation Percentage:43.0392%,\n",
            "Epoch [1187/1200], Training Loss: 0.8717, Validation Loss: 3.3754, Validation Percentage:44.8039%,\n",
            "Epoch [1188/1200], Training Loss: 0.8002, Validation Loss: 3.3246, Validation Percentage:46.3725%,\n",
            "Epoch [1189/1200], Training Loss: 1.0249, Validation Loss: 3.4386, Validation Percentage:43.8235%,\n",
            "Epoch [1190/1200], Training Loss: 0.9624, Validation Loss: 3.0205, Validation Percentage:42.8431%,\n",
            "Epoch [1191/1200], Training Loss: 0.8885, Validation Loss: 3.2238, Validation Percentage:43.8235%,\n",
            "Epoch [1192/1200], Training Loss: 0.8100, Validation Loss: 3.2337, Validation Percentage:45.0000%,\n",
            "Epoch [1193/1200], Training Loss: 0.9524, Validation Loss: 3.2673, Validation Percentage:43.4314%,\n",
            "Epoch [1194/1200], Training Loss: 0.8428, Validation Loss: 3.2190, Validation Percentage:42.5490%,\n",
            "Epoch [1195/1200], Training Loss: 0.9037, Validation Loss: 3.0011, Validation Percentage:42.3529%,\n",
            "Epoch [1196/1200], Training Loss: 0.8878, Validation Loss: 3.3108, Validation Percentage:43.6275%,\n",
            "Epoch [1197/1200], Training Loss: 0.8376, Validation Loss: 3.3084, Validation Percentage:44.6078%,\n",
            "Epoch [1198/1200], Training Loss: 1.0208, Validation Loss: 3.0695, Validation Percentage:43.2353%,\n",
            "Epoch [1199/1200], Training Loss: 0.9395, Validation Loss: 3.2751, Validation Percentage:43.9216%,\n",
            "Epoch [1200/1200], Training Loss: 0.8494, Validation Loss: 2.9993, Validation Percentage:45.0980%,\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = 0.0\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "#Test\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item() * images.size(0)\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        test_total += labels.size(0)\n",
        "        test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate average test loss and accuracy\n",
        "test_loss = test_loss / len(test_loader.dataset)\n",
        "test_accuracy = test_correct / test_total\n",
        "\n",
        "\n",
        "print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
        "          f'Training Loss: {epoch_loss:.4f}, '\n",
        "          f'Test Loss: {test_loss:.4f}, '\n",
        "          f'Test Percentage:{test_accuracy*100:.4f}%'\n",
        "          )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1M65rspaULj",
        "outputId": "11a54e48-c346-447f-d118-26e13accb8b3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1200/1200], Training Loss: 0.8494, Test Loss: 3.2561, Test Percentage:43.1127%\n"
          ]
        }
      ]
    }
  ]
}